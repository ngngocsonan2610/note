{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.FE&FS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/1.FE%26FS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2COeSc5ml1_",
        "colab_type": "text"
      },
      "source": [
        "# 1.Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcX1fpPrrg-_",
        "colab_type": "text"
      },
      "source": [
        "## Transformation Scaling/Standardize\n",
        "News:\n",
        "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
        "\n",
        "\n",
        "Căn bản:\n",
        "- [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n",
        "![alt text](https://miro.medium.com/max/4800/1*z-C9ANBC4rjsk-ZK4wzijg.png)\n",
        "- [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
        "\n",
        "\n",
        "- Đối với từng model nên xét thêm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWrridnMkIwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HULldo6Zm6l1",
        "colab_type": "text"
      },
      "source": [
        "# Onehot Encode/ Label Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfS1ArWkI61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFvPys_F9KB",
        "colab_type": "text"
      },
      "source": [
        "## Reduce memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUyadfjFF_kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgZr6swygYbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvTRFClqgYjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_a8z7m7uec_",
        "colab_type": "text"
      },
      "source": [
        "## Reduce dimensional checking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDc3N94Mj8K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scaling only on tranform data\n",
        "df_V = df_V[c_V_trans] \n",
        "df_V_test = df_V_test[c_V_trans]\n",
        "\n",
        "#standardize scaled\n",
        "scaler = StandardScaler().fit(df_V)\n",
        "df_V_scaled = scaler.transform(df_V)\n",
        "print('Scaled mean df_V',df_V_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_scaled[:,0].std()) \n",
        "scaler = StandardScaler().fit(df_V_test)\n",
        "df_V_test_scaled = scaler.transform(df_V_test)\n",
        "print('Scaled mean df_V',df_V_test_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_test_scaled[:,0].std()) \n",
        "\n",
        "# plot cumulative explained variance\n",
        "# pca = PCA().fit(df_V_scaled)\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# #plt.xlim(0,7,1)\n",
        "# plt.xlabel('Number of components')\n",
        "# plt.ylabel('Cumulative explained variance')\n",
        "\n",
        "\n",
        "# Setup Principal component analysis\n",
        "pca = PCA(n_components=125) \n",
        "#pca = PCA(n_components=0.96)  #v2: n_components=0.95\n",
        "df_V_pca = pca.fit_transform(df_V_scaled)\n",
        "np.save('df_V_pca_v3.npy',df_V_pca)\n",
        "#df_V_pca.to_csv('df_V_pca', sep='\\t')\n",
        "df_V_test_pca = pca.fit_transform(df_V_test_scaled)\n",
        "np.save('df_V_test_pca_v3.npy',df_V_test_pca)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljSftUKgvtyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import io\n",
        "import plotly.offline as py#visualization\n",
        "py.init_notebook_mode(connected=True)#visualization\n",
        "import plotly.graph_objs as go#visualization\n",
        "import plotly.tools as tls#visualization\n",
        "import plotly.figure_factory as ff#visualization\n",
        "\n",
        "'''\n",
        "Scatter plot giữa 3 cột bất kỳ trong dữ liệu, được hue = TARGET\n",
        "cho cái nhìn về dữ liệu, xem liệu nó có khả năng phân tách không\n",
        "'''\n",
        "\n",
        "trace1 = go.Scatter3d(x = churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = churn[\"ANNUAL_FEES\"],\n",
        "                      z = churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      mode = \"markers\",\n",
        "                      name = \"Churn customers\",\n",
        "                      text = \"Id : \" + churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      marker = dict(size = 1,color = \"red\")\n",
        "                     )\n",
        "trace2 = go.Scatter3d(x = not_churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = not_churn[\"ANNUAL_FEES\"],\n",
        "                      z = not_churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      name = \"Non churn customers\",\n",
        "                      text = \"Id : \" + not_churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      mode = \"markers\",\n",
        "                      marker = dict(size = 1,color= \"green\")\n",
        "                     )\n",
        "\n",
        "layout = go.Layout(dict(title = \"Monthly charges,total charges & tenure in customer attrition\",\n",
        "                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n",
        "                                                   center=dict(x=0, y=0, z=0),\n",
        "                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n",
        "                                     xaxis  = dict(title = \"annual incomes\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'),\n",
        "                                     yaxis  = dict(title = \"annual fees\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  ),\n",
        "                                     zaxis  = dict(title = \"term years\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  )\n",
        "                                    ),\n",
        "                        height = 700,\n",
        "                       )\n",
        "                  )\n",
        "                  \n",
        "\n",
        "data = [trace1,trace2]\n",
        "fig  = go.Figure(data = data,layout = layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQI7G83cDx2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "X = tmp[[i for i in tmp.columns if i not in Id_col + target_col]]\n",
        "Y = tmp[target_col + Id_col]\n",
        "\n",
        "principal_components = pca.fit_transform(X)\n",
        "pca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\n",
        "pca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\n",
        "pca_data[\"CHURN\"] = pca_data[\"CHURN\"].replace({1:\"CANCELLED\",0:\"INFORCE\"})\n",
        "\n",
        "def pca_scatter(target,color) :\n",
        "    tracer = go.Scatter(x = pca_data[pca_data[\"CHURN\"] == target][\"PC1\"] ,\n",
        "                        y = pca_data[pca_data[\"CHURN\"] == target][\"PC2\"],\n",
        "                        name = target,mode = \"markers\",\n",
        "                        marker = dict(color = color,\n",
        "                                      line = dict(width = .5),\n",
        "                                      symbol =  \"diamond-open\"),\n",
        "                        text = (\"Customer Id : \" + \n",
        "                                pca_data[pca_data[\"CHURN\"] == target]['MEMBERSHIP_NUMBER'])\n",
        "                       )\n",
        "    return tracer\n",
        "\n",
        "layout = go.Layout(dict(title = \"Visualising data with principal components\",\n",
        "                        plot_bgcolor  = \"rgb(243,243,243)\",\n",
        "                        paper_bgcolor = \"rgb(243,243,243)\",\n",
        "                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 1\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 2\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        height = 600\n",
        "                       )\n",
        "                  )\n",
        "trace1 = pca_scatter(\"CANCELLED\",'red')\n",
        "trace2 = pca_scatter(\"INFORCE\",'royalblue')\n",
        "data = [trace2,trace1]\n",
        "fig = go.Figure(data=data,layout=layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZwNeiLgSMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsgrj8nRC3na",
        "colab_type": "text"
      },
      "source": [
        "# 4.FE\n",
        "Gồm có 2 bước chính\n",
        "- Feature Generation: ở bước này tập trung các kĩ thuật để tạo ra feature: xử lí nlp, image, binning, scaling, grouping, aggregate.\n",
        "  - Kết hợp với Modelling để tìm ra nhóm feature tốt và tập trung mạnh vào đó\n",
        "  \n",
        "- Feature Selection: ở bước này tập trung các kĩ thuật chọn feature để tốt ưu mô hình\n",
        "  - Chọn được feature tốt và tối ưu được bộ nhớ\n",
        "  - Chọn được model parameters tốt nhất phù hợp với bộ features tốt nhất\n",
        "  - Kết hợp với các kĩ thuật chia dataset (CV, leave-one)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffkEEAsbC4EW",
        "colab_type": "text"
      },
      "source": [
        "## 4.1.Feature Generation\n",
        "\n",
        "[Simple FE](https://machinelearningcoban.com/general/2017/02/06/featureengineering/)\n",
        "-    Trực tiếp lấy raw data\n",
        "-    Bag-of-words\n",
        "  -    Bag-of-Words trong Computer Vision\n",
        "-    Feature Scaling and Normalization\n",
        "  -        Rescaling\n",
        "  -        Standardization\n",
        "  -        Scaling to unit length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfdUhklbDAQa",
        "colab_type": "text"
      },
      "source": [
        "## 4.2.Feature Selection\n",
        "\n",
        "- Tiêu chí đánh giá metric of feature\n",
        "  - AUC: thể hiện khả năng dự đoán\n",
        "  - Correlation: kiểm tra độ tương quan giữa feature với target hoặc với các feature quan trọng khác\n",
        "  - Converage: kiểm tra null, null nhiều thì ít thông tin\n",
        "  - Weighted of evidence, and information value:\n",
        "    - $\\ln \\left(\\frac{P(\\text {Good})}{P(\\text {Bad})}\\right)$\n",
        "    - \\begin{array}{l}{\\text { Information value }} \\\\ {\\qquad \\sum(P(G o o d)-P(B a d)) * \\ln \\left(\\frac{P(G o d)}{P(B a d)}\\right)}\\end{array}\n",
        "\n",
        "\n",
        "\n",
        "[Brute Force Approach](https://www.kdnuggets.com/2017/11/rapidminer-basic-concepts-feature-selection.html)\n",
        "Cách tiếp cận bằng các chạy thử thất cả các feature combination và so sánh trên các metrics of feature\n",
        "\n",
        "[Feature Selection](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) \n",
        "- Forwarding: ta bắt đầu với tập feature rỗng, sau đó lần lượt add thêm feature vào tập này. Nếu thấy performance của model tăng ta sẽ tiếp tục quá trình này, ngược lại sẽ dừng lại.\n",
        "- Backwarding: ta bắt đầu với toàn bộ tập feature, sau đó lần lượt remove từng feature khỏi tập này. Nếu thấy performance của model tăng hoặc giảm không quá nhiều, ta sẽ tiếp tục quá trình này, ngược lại nếu performance bị drop quá mạnh sẽ dừng lại.\n",
        "- Hybridge: kết hợp cả 2 hướng trên\n",
        "\n",
        "\n",
        "Research:\n",
        "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
        "- [2](https://www.kaggle.com/sz8416/6-ways-for-feature-selection) example selectkbes-RFE in kaggle\n",
        "- [3](https://www.kaggle.com/dkim1992/feature-selection-ranking) chua doc\n",
        "- [4](https://machinelearningmastery.com/feature-selection-machine-learning-python/) tong quat\n",
        "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "- [5](https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/) chua doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmnU0NRff_ck",
        "colab_type": "text"
      },
      "source": [
        "**RFE**\n",
        "\n",
        "[theory](https://www.kaggle.com/nroman/recursive-feature-elimination)\n",
        "\n",
        "Trong mini course này, tôi sẽ áp dụng hướng “backwarding”. Các bước thực hiện như sau:\n",
        "\n",
        "- Đặt: n là số lần lặp feature selection, k là số feature sẽ drop ở mỗi lần lặp, p là AUC sau mỗi lần train\n",
        "Train model với XGboost\n",
        "- Lấy kết quả feature important sắp xếp giảm dần và loại ra k feature có giá trị thấp nhất\n",
        "- Lưu lại performance hiện tại để so sánh với performance tiếp theo.\n",
        "- Nếu thấp hơn ngưỡng p sẽ dừng\n",
        "- Tiếp tục quá trình selection\n",
        "\n",
        "Tuỳ theo số lượng feature và cài đặt hyper-parameter của model thì thời gian sẽ nhanh chậm khác nhau.\n",
        "\n",
        "Additional:\n",
        "- Genetic algorithm for feature selection\n",
        "\n",
        "Source:\n",
        "- [1](https://ongxuanhong.wordpress.com/2019/04/17/data-science-mini-course/#more-15645)\n",
        "- [2](https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15) doc them\n",
        "\n",
        "---\n",
        "**Boruta**\n",
        "[1](http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlR2VrJocLM1",
        "colab_type": "text"
      },
      "source": [
        "## Genetic algorithm\n",
        "- [Explain 1](https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection)\n",
        "- [Explain 2](https://towardsdatascience.com/feature-reduction-using-genetic-algorithm-with-python-403a5f4ef0c1)\n",
        "- [Explain 3](http://dkopczyk.quantee.co.uk/genetic-algorithm/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0vuN-k5cNip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# load train and valid\n",
        "data_folder = 'competitionData'\n",
        "train = pd.read_csv(os.path.join(data_folder, 'trainingDataDeflated.csv'))\n",
        "valid = pd.read_csv(os.path.join(data_folder, 'validationDataDeflated.csv'))\n",
        "\n",
        "# get training columns\n",
        "training_columns = list(train.columns.values)\n",
        "for column in ['ind', 'deck', 'nofGames', 'nOfPlayers', 'winRate']:\n",
        "    training_columns.remove(column)\n",
        "print(\"Total number of training columns: {}\".format(len(training_columns)))\n",
        "\n",
        "# set index for loc\n",
        "train.set_index('ind', inplace=True)\n",
        "valid.set_index('ind', inplace=True)\n",
        "\n",
        "def load_dna_from_file(filename):\n",
        "    dna = []\n",
        "    f = open(filename)\n",
        "    while True:\n",
        "        line = f.readline().strip()\n",
        "        if line == '':\n",
        "            break\n",
        "        # parse params\n",
        "        params = line.split(\";\")\n",
        "        indices = [int(item) for item in params[3].split(',') if item != '']\n",
        "        # add information\n",
        "        gene = dict()\n",
        "        gene[\"epsilon\"] = float(params[0])\n",
        "        gene[\"c\"] = float(params[1])\n",
        "        gene[\"gamma\"] = round(float(params[2]), 5)\n",
        "        gene[\"indices\"] = indices\n",
        "        dna.append(gene)\n",
        "    f.close()\n",
        "    return dna\n",
        "\n",
        "'''\n",
        "save dna to file\n",
        "'''\n",
        "def save_dna_to_file(item):\n",
        "    dna = item[0]\n",
        "    fitness_score = item[1]\n",
        "    filename = os.path.join(\"finalPopulation\", \"{}.txt\".format(fitness_score))\n",
        "\n",
        "    fout = open(filename, \"w\")\n",
        "    for gene in dna:\n",
        "        ind_text = ','.join(str(index) for index in gene[\"indices\"])\n",
        "        text = ';'.join([str(gene[\"epsilon\"]), str(gene[\"c\"]), str(gene[\"gamma\"]), ind_text])\n",
        "        fout.write(text + '\\n')\n",
        "\n",
        "    fout.close()\n",
        "\n",
        "'''\n",
        "initialize the first population, either randomly or from a file\n",
        "'''\n",
        "def init_population(population_size, from_files=False):\n",
        "    pop = []\n",
        "\n",
        "    # add good dna to the init population from previously tested good solutions\n",
        "    if from_files == True:\n",
        "        files = os.listdir(\"goodSolutions\")\n",
        "        for file in files:\n",
        "            fitness_score = float(file[:-4])\n",
        "            filename = os.path.join(\"goodSolutions\", file)\n",
        "            dna = load_dna_from_file(filename)\n",
        "            pop.append([dna, fitness_score])\n",
        "\n",
        "    num_of_random_dna = population_size - len(pop)\n",
        "    for i in range(num_of_random_dna):\n",
        "        dna = []\n",
        "        for num_of_data in range(600, 1501, 100):\n",
        "            gene = dict()\n",
        "            gene[\"c\"] = 5.0\n",
        "            gene[\"gamma\"] = 1.0/90\n",
        "            gene[\"epsilon\"] = 0.02\n",
        "            gene[\"indices\"] = sorted(random.sample(range(1, 100001), num_of_data))\n",
        "            dna.append(gene)\n",
        "        pop.append([dna, 0.0])\n",
        "\n",
        "    min_score = min([item[1] for item in pop])\n",
        "    return pop, min_score\n",
        "\n",
        "'''\n",
        "R2 predict evaluation\n",
        "'''\n",
        "def R2(x, y):\n",
        "    return 1 - np.sum(np.square(x - y)) / np.sum(np.square(y - np.mean(y)))\n",
        "\n",
        "'''\n",
        "evaluate scores of a dna, sequential processing\n",
        "'''\n",
        "def fitness_sequential(dna, weight_train = 0.5):\n",
        "\n",
        "    gene_valid_r2 = []\n",
        "    gene_train_r2 = []\n",
        "\n",
        "    for gene in dna:\n",
        "        c = gene[\"c\"]\n",
        "        gamma = gene[\"gamma\"]\n",
        "        epsilon = gene[\"epsilon\"]\n",
        "        indices = gene[\"indices\"]\n",
        "\n",
        "        svr = SVR(kernel='rbf', gamma=gamma, C=c, epsilon=epsilon, shrinking=False)\n",
        "        svr.fit(train.loc[indices][training_columns], train.loc[indices][\"winRate\"])\n",
        "        valid_pred = svr.predict(valid[training_columns])\n",
        "        valid_r2 = R2(valid_pred, valid[\"winRate\"])\n",
        "        gene_valid_r2.append(valid_r2)\n",
        "\n",
        "        unselected_indices = list(set(range(1, 100001)) - set(indices))\n",
        "        train_pred = svr.predict(train.loc[unselected_indices][training_columns])\n",
        "        train_r2 = R2(train_pred, train.loc[unselected_indices][\"winRate\"])\n",
        "        gene_train_r2.append(train_r2)\n",
        "\n",
        "    fitness_valid = np.mean(gene_valid_r2)\n",
        "    fitness_train = np.mean(gene_train_r2)\n",
        "    fitness_score = (fitness_valid + weight_train * fitness_train) / (weight_train + 1.0)\n",
        "    print(\"{},{},{}:{}\".format(fitness_score, fitness_valid, fitness_train, gene_valid_r2 + gene_train_r2))\n",
        "    return fitness_score\n",
        "\n",
        "def train_and_valid(gene, using_train = False):\n",
        "    c = gene[\"c\"]\n",
        "    gamma = gene[\"gamma\"]\n",
        "    epsilon = gene[\"epsilon\"]\n",
        "    indices = gene[\"indices\"]\n",
        "\n",
        "    svr = SVR(kernel='rbf', gamma=gamma, C=c, epsilon=epsilon, shrinking=False)\n",
        "    svr.fit(train.loc[indices][training_columns], train.loc[indices][\"winRate\"])\n",
        "    valid_pred = svr.predict(valid[training_columns])\n",
        "    valid_r2 = R2(valid_pred, valid['winRate'])\n",
        "\n",
        "    if using_train:\n",
        "        unselected_indices = list(set(range(1, 100001)) - set(indices))\n",
        "        train_pred = svr.predict(train.loc[unselected_indices][training_columns])\n",
        "        train_r2 = R2(train_pred, train.loc[unselected_indices][\"winRate\"])\n",
        "        return [valid_r2, train_r2]\n",
        "\n",
        "    else:\n",
        "        return valid_r2\n",
        "\n",
        "'''\n",
        "evaluate scores of a dna, parallel processing\n",
        "'''\n",
        "def fitness_parallel(dna, using_train = False, weight_train = 1.0 / 16, valid_threshold = 21.0):\n",
        "    gene_r2 = Parallel(n_jobs=5, verbose=1)(delayed(train_and_valid)(gene, using_train) for gene in dna)\n",
        "    if using_train:\n",
        "        gene_valid_r2 = [item[0] for item in gene_r2]\n",
        "        gene_train_r2 = [item[1] for item in gene_r2]\n",
        "        fitness_valid = np.mean(gene_valid_r2)\n",
        "        fitness_train = np.mean(gene_train_r2)\n",
        "        fitness_score = (fitness_valid + fitness_train * weight_train) / (weight_train + 1.0)\n",
        "        print(\"{},{},{}:{}\".format(fitness_score, fitness_valid, fitness_train, gene_valid_r2 + gene_train_r2))\n",
        "    else:\n",
        "        fitness_score = np.mean(gene_r2)\n",
        "        print(\"{}:{}\".format(fitness_score, gene_r2))\n",
        "    return fitness_score\n",
        "\n",
        "'''\n",
        "randomly replace a number of indices\n",
        "no action is taken on epsilon, gamma, or c\n",
        "'''\n",
        "def mutate_gene(gene, percent_indices = 10):\n",
        "    num_of_indices = len(gene[\"indices\"])\n",
        "    selected_mutation_part = random.randint(0, num_of_indices * 4)\n",
        "    if selected_mutation_part < num_of_indices:\n",
        "        # c varies from 0.00001 to 10.0\n",
        "        gene[\"c\"] = random.randint(1, 1000000) / 100000.0\n",
        "    elif selected_mutation_part < num_of_indices * 2:\n",
        "        # gamma varies from 0.01 to 10.0\n",
        "        gene[\"gamma\"] = round(random.randint(1, 9000000) / 900000.0, 5)\n",
        "    elif selected_mutation_part <= num_of_indices * 3:\n",
        "        # epsilon varies from 0.00001 to 1.0\n",
        "        gene[\"epsilon\"] = random.randint(1, 100000) / 100000.0\n",
        "    else:\n",
        "        indices = gene[\"indices\"]\n",
        "        num_of_removed_indices = int(percent_indices * num_of_indices / 100)\n",
        "        removed_indices = random.sample(indices, num_of_removed_indices)\n",
        "\n",
        "        remaining_indices = set(indices) - set(removed_indices)\n",
        "        pool_indices = list(set(range(1, 100001)) - remaining_indices)\n",
        "        new_indices = random.sample(pool_indices, num_of_removed_indices)\n",
        "\n",
        "        gene[\"indices\"] = list(remaining_indices) + new_indices\n",
        "        assert len(gene[\"indices\"]) == num_of_indices, \"Missing indices in mutation\"\n",
        "\n",
        "'''\n",
        "randomly select a gene for mutation\n",
        "'''\n",
        "def mutate_dna(dna, single_gene=True):\n",
        "    if single_gene:\n",
        "        selected_gene = random.randint(0, 9)\n",
        "        mutate_gene(dna[selected_gene])\n",
        "    else:\n",
        "        for selected_gene in range(10):\n",
        "            mutate_gene(dna[selected_gene])\n",
        "    return dna\n",
        "\n",
        "'''\n",
        "perform crossover between 2 genes\n",
        "'''\n",
        "def crossover_gene(gene_x, gene_y):\n",
        "    new_gene = dict()\n",
        "    new_gene[\"c\"] = random.sample([gene_x[\"c\"], gene_y[\"c\"]], 1)[0]\n",
        "    new_gene[\"epsilon\"] = random.sample([gene_x[\"epsilon\"], gene_y[\"epsilon\"]], 1)[0]\n",
        "    new_gene[\"gamma\"] = random.sample([gene_x[\"gamma\"], gene_y[\"gamma\"]], 1)[0]\n",
        "\n",
        "    x_indices = set(gene_x[\"indices\"])\n",
        "    y_indices = set(gene_y[\"indices\"])\n",
        "    overlapping_indices = x_indices.intersection(y_indices)\n",
        "\n",
        "    num_of_indices = len(x_indices)\n",
        "    num_of_remaining_x_indices = int((num_of_indices - len(overlapping_indices)) / 2)\n",
        "    num_of_remaining_y_indices = num_of_indices - len(overlapping_indices) - num_of_remaining_x_indices\n",
        "\n",
        "    remaining_x_indices = list(x_indices - overlapping_indices)\n",
        "    remaining_y_indices = list(y_indices - overlapping_indices)\n",
        "\n",
        "    selected_x_indices = random.sample(remaining_x_indices, num_of_remaining_x_indices)\n",
        "    selected_y_indices = random.sample(remaining_y_indices, num_of_remaining_y_indices)\n",
        "\n",
        "    new_gene[\"indices\"] = list(overlapping_indices) + selected_x_indices + selected_y_indices\n",
        "    assert len(new_gene[\"indices\"]) == num_of_indices, \"Missing indices in crossover\"\n",
        "\n",
        "    return new_gene\n",
        "\n",
        "'''\n",
        "randomly select a gene and perform crossover on that gene\n",
        "'''\n",
        "def crossover_dna(dna_x, dna_y):\n",
        "\n",
        "    # perform crossover on all genes\n",
        "    new_dna = []\n",
        "    for i in range(10):\n",
        "        gene_x = dna_x[i]\n",
        "        gene_y = dna_y[i]\n",
        "        new_dna.append(crossover_gene(gene_x, gene_y))\n",
        "\n",
        "    return new_dna\n",
        "\n",
        "'''\n",
        "select dna for new generation\n",
        "'''\n",
        "def select(population, population_size):\n",
        "    if len(population) < population_size:\n",
        "        return population\n",
        "\n",
        "    prob_distribution = [item[1] for item in population]\n",
        "    selected_indices = np.random.choice(range(len(population)), population_size, prob_distribution)\n",
        "    removed_indices = [index for index in range(len(population)) if index not in selected_indices]\n",
        "\n",
        "    # set new population based on selected indices\n",
        "    new_population = []\n",
        "    for index in selected_indices:\n",
        "        new_population.append(population[index])\n",
        "\n",
        "    # remove unselected_indices\n",
        "    for index in sorted(removed_indices, reverse=True):\n",
        "        del population[index]\n",
        "    del population\n",
        "\n",
        "    return new_population\n",
        "\n",
        "'''\n",
        "clone a gene\n",
        "'''\n",
        "def copy_gene(gene):\n",
        "    new_gene = dict()\n",
        "    new_gene[\"c\"] = gene[\"c\"]\n",
        "    new_gene[\"gamma\"] = gene[\"gamma\"]\n",
        "    new_gene[\"epsilon\"] = gene[\"epsilon\"]\n",
        "    new_gene[\"indices\"] = list(gene[\"indices\"])\n",
        "    return new_gene\n",
        "\n",
        "'''\n",
        "Clone a dna\n",
        "'''\n",
        "def copy_dna(dna):\n",
        "    new_dna = []\n",
        "    for gene in dna:\n",
        "        new_gene = copy_gene(gene)\n",
        "        new_dna.append(new_gene)\n",
        "    return new_dna\n",
        "\n",
        "'''\n",
        "Select and update files for initial generation\n",
        "'''\n",
        "def update_files(population_size):\n",
        "    new_solutions = os.listdir(\"finalPopulation\")\n",
        "    if len(new_solutions) == 0:\n",
        "        return\n",
        "\n",
        "    for file in new_solutions:\n",
        "        if not os.path.exists(os.path.join(\"goodSolutions\", file)):\n",
        "            shutil.copy2(os.path.join(\"finalPopulation\", file), \"goodSolutions\")\n",
        "        os.remove(os.path.join(\"finalPopulation\", file))\n",
        "\n",
        "    all_solutions = os.listdir(\"goodSolutions\")\n",
        "    all_scores = [float(item[:-4]) for item in all_solutions]\n",
        "    top_scores = sorted(all_scores, reverse=True)[:population_size]\n",
        "\n",
        "    for item in all_solutions:\n",
        "        if float(item[:-4]) not in top_scores:\n",
        "            os.remove(os.path.join(\"goodSolutions\", item))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    counter = 1\n",
        "    while True:\n",
        "        POPULATION_MAX = 21\n",
        "        GENERATION_SIZE = 5\n",
        "\n",
        "        # init population\n",
        "        # tao ra population ban dau\n",
        "        population, MIN_LOG_SCORE = init_population(POPULATION_MAX, from_files=True)\n",
        "        print(\"Running times {}, min log score {}\".format(counter, MIN_LOG_SCORE))\n",
        "\n",
        "        # evolve\n",
        "        for i in range(GENERATION_SIZE):\n",
        "            new_solutions = []\n",
        "            population_size = len(population)\n",
        "            num_of_selected_parents = 15 # 3 * (population_size // 6)\n",
        "            print(\"Generation {}, input size {}, num of selected parents {}\".format(i + 1, population_size, num_of_selected_parents))\n",
        "\n",
        "            # generate dna for mutation and crossover\n",
        "            selected_indices = random.sample(range(population_size), num_of_selected_parents)\n",
        "\n",
        "            for j in range(0, num_of_selected_parents, 3):\n",
        "                # generate new child from mutation\n",
        "                mutated_child = mutate_dna(copy_dna(population[selected_indices[j]][0]))\n",
        "                new_solutions.append(mutated_child)\n",
        "\n",
        "                mutated_child = mutate_dna(copy_dna(population[selected_indices[j + 1]][0]))\n",
        "                new_solutions.append(mutated_child)\n",
        "\n",
        "                mutated_child = mutate_dna(copy_dna(population[selected_indices[j + 2]][0]))\n",
        "                new_solutions.append(mutated_child)\n",
        "\n",
        "                # generate new children from crossover\n",
        "                crossover_child = crossover_dna(population[selected_indices[j+1]][0], population[selected_indices[j+2]][0])\n",
        "                new_solutions.append(crossover_child)\n",
        "\n",
        "            # evaluate new children\n",
        "            for dna in new_solutions:\n",
        "                #fitness_score = fitness_sequential(dna)\n",
        "                fitness_score = fitness_parallel(dna, using_train=True)\n",
        "                if fitness_score > MIN_LOG_SCORE:\n",
        "                    save_dna_to_file([dna, fitness_score])\n",
        "                population.append([dna, fitness_score])\n",
        "\n",
        "            # randomly select individuals from new population for the next generation\n",
        "            population = select(population, POPULATION_MAX)\n",
        "\n",
        "        # output final population\n",
        "        '''\n",
        "        for item in population:\n",
        "            save_dna_to_file(item)\n",
        "        '''\n",
        "\n",
        "        # update good solutions for a new round\n",
        "        update_files(POPULATION_MAX)\n",
        "        counter += 1\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwJMC0X3cOGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}