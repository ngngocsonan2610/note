{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4.Math.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["3m0ZStxWGXWw","2HjhP48zUy_D","BeRl04ASW1QO","gjDAVLYCXK6R","fdAmcAAIFYEX"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"S4LU_eUwCQZZ","colab_type":"text"},"source":["**Khóa học và tài liệu**\n","\n","- https://web.stanford.edu/class/stats60/\n","\n","\n","**Sách**\n","\n","- https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"7gt15GWASHdd","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJjgxJRjDFgk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":621},"outputId":"3edfc7a8-40ab-4e9e-b3f2-86a56a8de3f0","executionInfo":{"status":"ok","timestamp":1566119840118,"user_tz":-420,"elapsed":780,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmIzVzuSJNfcPFlxXSQsn53YMiTeTUYIBv6V-0=s64","userId":"08833028508227386449"}}},"source":["from IPython.display import IFrame\n","IFrame(\"https://nbviewer.jupyter.org/github/tiepvupsu/tiepvupsu.github.io/blob/master/ML_math.pdf\",\n","       width=800, height=600)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","        <iframe\n","            width=\"800\"\n","            height=\"600\"\n","            src=\"https://nbviewer.jupyter.org/github/tiepvupsu/tiepvupsu.github.io/blob/master/ML_math.pdf\"\n","            frameborder=\"0\"\n","            allowfullscreen\n","        ></iframe>\n","        "],"text/plain":["<IPython.lib.display.IFrame at 0x7fbdec632438>"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"3m0ZStxWGXWw","colab_type":"text"},"source":["# 1.Linear Algebra\n","**Basic**\n","- https://aivietnam.ai/courses/aisummer2019/lessons/dai-so-tuyen-tinh-co-ban/\n","\n","**RL-Standford**\n","\n","- http://cs229.stanford.edu/section/cs229-linalg.pdf\n","\n","**VEF-ML course**\n","\n","\n","*   HW1-https://drive.google.com/file/d/1LyqGLUHqEd4EX8vLaU8wlG3GvBfaVflh/view\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"i_1FeIG-XoDu","colab_type":"code","colab":{}},"source":["'''\n","Solutions\n","'''\n","from IPython.display import IFrame\n","IFrame(\"https://piazza-resources.s3.amazonaws.com/joewsz9eh732za/jon9ptnmgnau7/solutions_1.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAR6AWVCBX2V2X222Q%2F20190811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190811T153501Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEDMaCXVzLWVhc3QtMSJHMEUCIFfZR7fgpxKhlJiv0XdfPGAAVtVIjq52ID%2FBJTRuEjDDAiEAukFDV2XibRLZoM%2Fjc9NWDvca6A9FP%2BqD%2FA3tY9A4uE8q4wMIvP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwxMzMxOTE1MDM5ODMiDJDLWloXUe2HGIAo1yq3A18pH9qHhM%2BaRj%2F7FnpgyQ0RUf9K4sg4KHHq3WwmdOsA7aRK9IDUDqvMa5QjgdnsD3BzQ1%2FRIOm8eHTRFoZrjjbQzx6ncBFRf9HJBs13gOtADx5j03y9i8wU0KgP5cDBo5PI0xPy2tCf63AOfkUxE50vWqXn8tAg6FqjiKsMe2x%2BaHsyAxiIP9qmgBZdaNyxO60%2F%2FarUKOpeGOyONTI2KnQiHKTh8Uxs9Sz%2FmypNBSoc%2FAbzYvBTItHm47H40MevvZhyb1piPf%2BF%2BXqiMmkazXCOFn2VrUZpEP3ZkgQxWL0BoLiKGPMCgPJxbjtewwI4C7apEvgfMkVEPUWamswV63qmy0leR6Ep1rRTHwMtNcjgy1Q7jH36%2FiB5eWEi5RZAOVL79U7%2FUPET97W6XS%2F%2F%2FkRzSYxE70L4uHBM83iARcjIPMISPHJ6o0xhq3yKhLsOm%2Bg1BRevV9hsNw5HkBigREbLugS%2Fbkp8k4ozrHuwYaGoBKZ8CpMpmaoctlHmylTa3JIombCgOaJO2hNJxak5pB1t%2BsED7O5a%2FCvkU4EgMv%2BPsC5KmKd1L4O6AZ05FimXz1S48yWJDRQwx%2BO%2F6gU6tAHmu8kq1XdRr0mMKoyMB0bCaJDjEsrdv0awB%2FiUwrQ0BYqD9Bj1q2WndT18g5T15zEdc2E%2BvssfWA3pNZBhhwc4Wu6i1cnJ5m6hDQZhI133D38gYcnjl%2BeBMH866AQO3HM39TTc1od5OtjB5heSG8P5ibnxwWZlktyDroFlvjkSqRwqlSx3E9gVJ8dJdfiqFVL%2BBICHFNYd8T%2B05l%2ByGxvzE%2FXvOx3soxN3u4qZxwIAyD5jKMs%3D&X-Amz-Signature=4d5430a088c8e91f575aabd58101776f95a4dd8612eb84ce5d0685c21a9bcf3b\",\n","       width=1000, height=600)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gFPMPH9tcgKS","colab_type":"text"},"source":["# 2.Optimization\n","\n","---\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qk_-3iNSpwdm","colab_type":"code","colab":{}},"source":["    \n","from IPython.display import IFrame\n","\n","IFrame(src='https://metacademy.org/browse', width=1000, height=600)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfFLejuDgc2f","colab_type":"text"},"source":["# Convex Optimization\n","## Convex sets and convex function\n","* Machine Learning và Optimization có quan hệ mật thiết với nhau. Trong Optimization, Convex Optimization là quan trọng nhất. Một bài toán là convex optimization nếu hàm mục tiêu là convex và tập hợp các điểm thỏa mãn các điều kiện ràng buộc là một convex set.\n","*  Trong convex set, mọi đoạn thẳng nối hai điểm bất kỳ trong tập đó sẽ nằm hoàn toàn trong tập đó. Tập hợp các giao điểm của các convex sets là một convex set.\n","*  Một hàm số là convex nếu đoạn thẳng nối hai điểm bất kỳ trên đồ thì hàm số đó không nằm dưới đồ thị đó.\n","*  Một hàm số khả vi là convex nếu tập xác định của nó là convex và đường (mặt) tiếp tuyến không nằm phía trên đồ thị (bề mặt) của hàm số đó.\n","*  Các norms là các hàm lồi, được sử dụng nhiều trong tối ưu.\n","\n","## Convex Optimization Problem\n","*  Các bài toán tối ưu xuất hiện rất nhiều trong thực tế, trong đó Tối Ưu Lồi đóng một vai trò quan trọng. Trong bài toán Tối Ưu Lồi, nếu tìm được cực trị thì cực trị đó chính là một điểm optimal của bài toán (nghiệm của bài toán).\n","*  Có nhiều bài toán tối ưu không được viết dưới dạng convex nhưng có thể biến đổi về dạng convex, ví dụ như bài toán Geometric Programming.\n","*  Linear Programming và Quadratic Programming đóng một vài trò quan trọng trong toán tối ưu, được sử dụng nhiều trong các thuật toán Machine Learning.\n","*  Thư viện CVXOPT được dùng để tối ưu nhiều bài toán tối ưu lồi, rất dễ sử dụng và thời gian chạy tương đối nhanh.\n","\n","## Duality\n","* Các bài toán tối ưu với chỉ ràng buộc là đẳng thức có thể được giải quyết bằng phương pháp nhân tử Lagrange. Ta cũng có định nghĩa về Lagrangian. Điều kiện cần để một điểm là nghiệm của bài toán tối ưu là nó phải làm cho đạo hàm của Lagrangian bằng 0.\n","\n","\n","source\n","[1](https://machinelearningcoban.com/2017/03/12/convexity/),\n","[pdf](https://nbviewer.jupyter.org/github/tiepvupsu/tiepvupsu.github.io/blob/master/assets/latex/book_CVX.pdf)"]},{"cell_type":"markdown","metadata":{"id":"2HjhP48zUy_D","colab_type":"text"},"source":["## Gradient descent\n","\n","Source:\n","[1](https://machinelearningcoban.com/2017/01/16/gradientdescent2/#-batch-gradient-descent),\n","[2](http://ruder.io/optimizing-gradient-descent/index.html)-fullGD,\n"]},{"cell_type":"markdown","metadata":{"id":"BeRl04ASW1QO","colab_type":"text"},"source":["### Classic (batch) Gradient descent\n","\n","Ý tưởng: tìm tối ưu bằng cách di chuyển ngược đạo hàm.\n","- batch (tất cả), sử dụng tất cả các điểm dữ liệu để tính đạo hàm mỗi lần cập nhật \n","- mini-batch (1 sample đại điện), sử dụng tất cả các điểm dữ liệu trong 1 sample đại diện (có thể lấy bằng ngẫu nhiên, hoặc lấy khoảng) để tính đạo hàm mỗi lần cập nhật \n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8mVxGtZRxskC","colab_type":"text"},"source":["Consider a convex quadratic function in n-dimensional space of the form\n","\n","$$f(x) = \\frac{1}{2}\\mathrm x^TA\\mathrm x + b^T\\mathrm x,$$\n","\n","where A is a symmetric, positive semidefinite matrix of size $n \\times n$ and b is a vector\n","of size $n$. Write a Python program that takes A and b as inputs and optimizes\n","function f using Gradient Descent algorithm. Note: initialization is important.\n","<br>\n","<br>\n","\n","**Solution:** Access [here](https://colab.research.google.com/drive/190GPiQz4yQHTPYKH41BoFoT9Ugr1hWFP) for IPYTHON viewing\n","\n","From the question, we have that $f$ is a *convex* quadratic function\n","\n","$$f (\\mathrm x) := \\frac 12 \\mathrm x^\\top \\mathrm A \\,\\mathrm x + \\mathrm b^\\top \\mathrm x$$\n","\n","where matrix $\\rm A$ is symmetric and positive semidefinite. The gradient of $f$ is\n","\n","$$\\nabla f (\\mathrm x) = \\mathrm A \\mathrm x + \\mathrm b$$\n","\n","Using **gradient descent** with step $\\mu$,\n","\n","$$\\begin{array}{rl}\\mathrm x_{k+1} &= \\mathrm x_k - \\mu \\nabla f (\\mathrm x_k)\\\\ &= \\mathrm x_k - \\mu \\left( \\mathrm A \\mathrm x_k - \\mathrm b \\right)\\\\ &= \\left( \\mathrm I - \\mu \\mathrm A \\right) \\mathrm x_k + \\mu \\mathrm b\\end{array}$$\n","\n","Finanlly, choose step $\\mu$ such that the (real) eigenvalues of $\\mathrm I - \\mu \\mathrm A$ are in the open interval $(-1,1)$.\n","Remark that A should be the symmetric positive semidefinite matrix"]},{"cell_type":"markdown","metadata":{"id":"gjDAVLYCXK6R","colab_type":"text"},"source":["#### Exp 1"]},{"cell_type":"code","metadata":{"id":"8LI_gAz_GOYr","colab_type":"code","colab":{}},"source":["import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mEryWHWiyfoN","colab":{}},"source":["# function for generating A,b \n","def gen_Ab(size):\n","  A = np.random.randint(0,5, size=(size, size))\n","  A = np.dot(A,A.transpose()) #for sure that A is positive semidefinite \n","  A = (A + A.T)/2  # for sure that A is symmetric\n","  b = np.random.randint(0,5, size=(size, 1))\n","  print('matrix A')\n","  for x in A:\n","    print(*x, sep=\" \")\n","  print('\\nb:')\n","  for x in b.transpose():\n","    print(*x, sep=\" \")\n","  return A,b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SkuXlmNI7gG","colab_type":"code","outputId":"55819140-57f7-4aff-deda-1e2937621cbc","executionInfo":{"status":"ok","timestamp":1542393474179,"user_tz":-420,"elapsed":759,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# example 2x2 matrix\n","A,b = gen_Ab(2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["matrix A\n","2.0 2.0\n","2.0 4.0\n","\n","b:\n","2 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cbWzoJEQqvQ6","colab_type":"code","colab":{}},"source":["def gradient_descent(A, b, mu, num_iters, threshold): \n","  n = A.shape[0]\n","  x = np.random.randint(10, size=(n, 1))\n","  array_x = []\n","  array_fx = []\n","  array_x.append(x)  \n","  array_fx.append(1/2*np.dot(np.dot(x.transpose(),A),x) \n","                       + np.dot(b.transpose(),x))\n","  \n","  for i in range(0, num_iters):\n","    delta_fx =  np.dot(A,x) + b  \n","    x = x - mu*delta_fx\n","    fx = np.around((1/2*np.dot(np.dot(x.transpose(),A),x) + np.dot(b.transpose(),x)),9)    \n","    if i == 0:\n","      array_x.append(x) \n","      array_fx.append(fx)\n","    else:\n","        #array_x.append(x) \n","        #array_fx.append([fx])\n","      if abs(fx - array_fx[i-1]) >= threshold:\n","        array_x.append(x) \n","        array_fx.append(fx)\n","      else:\n","        break                \n"," \n","  return array_x,array_fx\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O42z_215MAEY","colab_type":"code","outputId":"ed078769-5706-4f16-e126-db8af7e06fc1","executionInfo":{"status":"ok","timestamp":1542393688722,"user_tz":-420,"elapsed":932,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["# A is matrix 1x1\n","A,b = gen_Ab(1)\n","x, fx = gradient_descent(A, b, 0.1, 1000, 10**(-9))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["matrix A\n","4.0\n","\n","b:\n","4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JP7bICQAMAMt","colab_type":"code","outputId":"3acbb3a4-ef0f-40e1-f2e6-c445251d8afe","executionInfo":{"status":"ok","timestamp":1542393697483,"user_tz":-420,"elapsed":767,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["x[-10:-1]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[-0.99939064]]),\n"," array([[-0.99963438]]),\n"," array([[-0.99978063]]),\n"," array([[-0.99986838]]),\n"," array([[-0.99992103]]),\n"," array([[-0.99995262]]),\n"," array([[-0.99997157]]),\n"," array([[-0.99998294]]),\n"," array([[-0.99998977]])]"]},"metadata":{"tags":[]},"execution_count":520}]},{"cell_type":"code","metadata":{"id":"W4NFIP2KQCCK","colab_type":"code","outputId":"5a6ba976-dafb-4532-fee5-41ecc91d7b79","executionInfo":{"status":"ok","timestamp":1542393693316,"user_tz":-420,"elapsed":766,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["print('Number of gradient steps: %d' %len(fx))\n","print('fx has minimum value = %s' %fx[-1])\n","fx[-10:-1]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of gradient steps: 29\n","fx has minimum value = [[-2.]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([[-1.99999926]]),\n"," array([[-1.99999973]]),\n"," array([[-1.9999999]]),\n"," array([[-1.99999996]]),\n"," array([[-1.99999999]]),\n"," array([[-2.]]),\n"," array([[-2.]]),\n"," array([[-2.]]),\n"," array([[-2.]])]"]},"metadata":{"tags":[]},"execution_count":519}]},{"cell_type":"code","metadata":{"id":"nBBQvTyGzz2b","colab_type":"code","outputId":"a45f0ca7-28b3-427e-c258-0bbf3c62c6c3","executionInfo":{"status":"ok","timestamp":1542393384355,"user_tz":-420,"elapsed":766,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["A,b = gen_Ab(2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["matrix A: [[4. 4.]\n"," [4. 8.]]\n","vector b: [[0 3]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J-YjMRbQhfLp","colab_type":"code","outputId":"b3cd2d1b-e4a0-4365-e3c6-e3fe4db8e1eb","executionInfo":{"status":"ok","timestamp":1542442457453,"user_tz":-420,"elapsed":920,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# A is matrix 2x2\n","A,b = gen_Ab(2)\n","x, fx = gradient_descent(A, b, 0.01, 1000, 10**(-9))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["matrix A\n","18.0 18.0\n","18.0 20.0\n","\n","b:\n","0 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NOZ_bqENvc-A","colab_type":"code","outputId":"19bf9ec2-22f8-4f60-bc4f-75c0e68ad019","executionInfo":{"status":"ok","timestamp":1542442579881,"user_tz":-420,"elapsed":1744,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":191}},"source":["x[-10:-1]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[ 0.49976283],\n","        [-0.49977564]]), array([[ 0.49976514],\n","        [-0.49977782]]), array([[ 0.49976742],\n","        [-0.49977998]]), array([[ 0.49976968],\n","        [-0.49978212]]), array([[ 0.49977192],\n","        [-0.49978424]]), array([[ 0.49977414],\n","        [-0.49978634]]), array([[ 0.49977633],\n","        [-0.49978841]]), array([[ 0.49977851],\n","        [-0.49979047]]), array([[ 0.49978066],\n","        [-0.49979251]])]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"PjPndI3Oilu4","colab_type":"code","outputId":"2c91ff7f-34c9-4cfb-f61e-c9ff4ae5d4c5","executionInfo":{"status":"ok","timestamp":1542442463652,"user_tz":-420,"elapsed":1146,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["print('Number of gradient steps: %d' %len(fx))\n","print('fx has minimum value = %s' %fx[-1])\n","fx[-10:-1]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of gradient steps: 736\n","fx has minimum value = [[-0.24999996]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999995]]),\n"," array([[-0.24999996]])]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"YyiRQXbFvJSv","colab_type":"code","outputId":"47dc212d-a002-469b-8e23-ab9b94fd9c8b","executionInfo":{"status":"ok","timestamp":1542442843227,"user_tz":-420,"elapsed":853,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["z = np.linalg.solve(A, -b)\n","z"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 0.5],\n","       [-0.5]])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"KSKfzMjlvSeO","colab_type":"code","outputId":"e194d303-1692-4898-b152-5735dd8f133a","executionInfo":{"status":"ok","timestamp":1542442845971,"user_tz":-420,"elapsed":861,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["np.around((1/2*np.dot(np.dot(z.transpose(),A),z) + np.dot(b.transpose(),z)),9)  "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.25]])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"STqkR4Rwy_Wy","colab_type":"code","outputId":"3303a635-f0b8-403b-8879-44b733113e5c","executionInfo":{"status":"ok","timestamp":1542393565582,"user_tz":-420,"elapsed":749,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["# A is matrix 3x3\n","A,b = gen_Ab(3)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["matrix A\n","41.0 18.0 16.0\n","18.0 9.0 8.0\n","16.0 8.0 16.0\n","\n","b:\n","2 4 1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nBpTWTDx0fyv","colab_type":"code","colab":{}},"source":["x, fx = gradient_descent(A, b, 0.03, 1000, 10**(-9))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8SgTe0G-kPMD","colab_type":"code","outputId":"5b09e98f-b18b-400d-ef4e-07500644876f","executionInfo":{"status":"ok","timestamp":1542393619867,"user_tz":-420,"elapsed":739,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["print('Number of gradient steps: %d' %len(fx))\n","print('fx has minimum value = %s' %fx[-1])\n","fx[-10:-1]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of gradient steps: 377\n","fx has minimum value = [[-4.85624999]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]]),\n"," array([[-4.85624999]])]"]},"metadata":{"tags":[]},"execution_count":513}]},{"cell_type":"code","metadata":{"id":"fd4BlsrCkPHP","colab_type":"code","outputId":"dfc18ee6-4d96-48fe-ba7e-05358426d38e","executionInfo":{"status":"ok","timestamp":1542393645560,"user_tz":-420,"elapsed":721,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["# A is matrix 4x4\n","A,b = gen_Ab(4)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["matrix A\n","34.0 27.0 9.0 14.0\n","27.0 35.0 9.0 14.0\n","9.0 9.0 6.0 6.0\n","14.0 14.0 6.0 12.0\n","\n","b:\n","1 2 3 4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7sq4Miv30y-z","colab_type":"code","colab":{}},"source":["x, fx = gradient_descent(A, b, 0.001, 10000, 10**(-9))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFXnhmenwi0g","colab_type":"code","outputId":"e66dc9e0-4965-48c1-d956-1224a49fc2f2","executionInfo":{"status":"ok","timestamp":1542393651261,"user_tz":-420,"elapsed":800,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh6.googleusercontent.com/-dAJU-LdW30Y/AAAAAAAAAAI/AAAAAAAAABk/T1hVzkS2WvM/s64/photo.jpg","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["print('After %d gradient steps' %len(fx))\n","print('fx has minimum value = %s\\n' %fx[-1])\n","fx[-10:-1]\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["After 4014 gradient steps\n","fx has minimum value = [[-1.43877531]]\n","\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[array([[-1.4387753]]),\n"," array([[-1.4387753]]),\n"," array([[-1.4387753]]),\n"," array([[-1.43877531]]),\n"," array([[-1.43877531]]),\n"," array([[-1.43877531]]),\n"," array([[-1.43877531]]),\n"," array([[-1.43877531]]),\n"," array([[-1.43877531]])]"]},"metadata":{"tags":[]},"execution_count":516}]},{"cell_type":"markdown","metadata":{"id":"RJGBwaKMXESq","colab_type":"text"},"source":["#### Exp 2"]},{"cell_type":"code","metadata":{"id":"7Hav62A9U1KS","colab_type":"code","outputId":"d6d4468a-6a35-4bdc-b656-ffec40161d66","executionInfo":{"status":"ok","timestamp":1565538057227,"user_tz":-420,"elapsed":1139,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmIzVzuSJNfcPFlxXSQsn53YMiTeTUYIBv6V-0=s64","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import numpy as np\n","def objective(A,b,x):\n","  return(np.dot(x.T,np.dot(A,x)))+np.dot(b,x)\n","\n","def gradientdescent(A,b,stepSize=0.01,niters=1000,epsilon=0.01):\n","  n=A.shape[0]\n","  x=np.random.rand(n)\n","  for i in range(niters):\n","    print(\"iteration %d\"%(i)) \n","    grad=np.dot(A,x)+ b \n","    print(\"gradient norm = %.4f\"%(np.linalg.norm(grad)))\n","    print(\"    function value = %.4f\"%(objective(A,b,x)))\n","    if np.linalg.norm(grad) < epsilon:\n","      print(\"∗∗∗optimal solution found\")\n","      break \n","    x = x - stepSize*grad\n","    print('−−−')\n","  return x,objective(A,b,x)\n","#\n","A=np.array([[1,0,0],[0,1,0],[0,0,1]])\n","b=np.array([0,0,0])\n","x,obj = gradientdescent(A,b)\n","print(\"\\n\\n\\nbest solution found:{}\".format(x))\n","print(\"best objective value:{}\".format(obj))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["iteration 0\n","gradient norm = 1.4577\n","    function value = 2.1250\n","−−−\n","iteration 1\n","gradient norm = 1.4432\n","    function value = 2.0827\n","−−−\n","iteration 2\n","gradient norm = 1.4287\n","    function value = 2.0412\n","−−−\n","iteration 3\n","gradient norm = 1.4144\n","    function value = 2.0006\n","−−−\n","iteration 4\n","gradient norm = 1.4003\n","    function value = 1.9608\n","−−−\n","iteration 5\n","gradient norm = 1.3863\n","    function value = 1.9218\n","−−−\n","iteration 6\n","gradient norm = 1.3724\n","    function value = 1.8836\n","−−−\n","iteration 7\n","gradient norm = 1.3587\n","    function value = 1.8461\n","−−−\n","iteration 8\n","gradient norm = 1.3451\n","    function value = 1.8093\n","−−−\n","iteration 9\n","gradient norm = 1.3317\n","    function value = 1.7733\n","−−−\n","iteration 10\n","gradient norm = 1.3183\n","    function value = 1.7380\n","−−−\n","iteration 11\n","gradient norm = 1.3052\n","    function value = 1.7034\n","−−−\n","iteration 12\n","gradient norm = 1.2921\n","    function value = 1.6696\n","−−−\n","iteration 13\n","gradient norm = 1.2792\n","    function value = 1.6363\n","−−−\n","iteration 14\n","gradient norm = 1.2664\n","    function value = 1.6038\n","−−−\n","iteration 15\n","gradient norm = 1.2537\n","    function value = 1.5718\n","−−−\n","iteration 16\n","gradient norm = 1.2412\n","    function value = 1.5406\n","−−−\n","iteration 17\n","gradient norm = 1.2288\n","    function value = 1.5099\n","−−−\n","iteration 18\n","gradient norm = 1.2165\n","    function value = 1.4799\n","−−−\n","iteration 19\n","gradient norm = 1.2043\n","    function value = 1.4504\n","−−−\n","iteration 20\n","gradient norm = 1.1923\n","    function value = 1.4216\n","−−−\n","iteration 21\n","gradient norm = 1.1804\n","    function value = 1.3933\n","−−−\n","iteration 22\n","gradient norm = 1.1686\n","    function value = 1.3655\n","−−−\n","iteration 23\n","gradient norm = 1.1569\n","    function value = 1.3384\n","−−−\n","iteration 24\n","gradient norm = 1.1453\n","    function value = 1.3117\n","−−−\n","iteration 25\n","gradient norm = 1.1339\n","    function value = 1.2856\n","−−−\n","iteration 26\n","gradient norm = 1.1225\n","    function value = 1.2600\n","−−−\n","iteration 27\n","gradient norm = 1.1113\n","    function value = 1.2350\n","−−−\n","iteration 28\n","gradient norm = 1.1002\n","    function value = 1.2104\n","−−−\n","iteration 29\n","gradient norm = 1.0892\n","    function value = 1.1863\n","−−−\n","iteration 30\n","gradient norm = 1.0783\n","    function value = 1.1627\n","−−−\n","iteration 31\n","gradient norm = 1.0675\n","    function value = 1.1396\n","−−−\n","iteration 32\n","gradient norm = 1.0568\n","    function value = 1.1169\n","−−−\n","iteration 33\n","gradient norm = 1.0463\n","    function value = 1.0947\n","−−−\n","iteration 34\n","gradient norm = 1.0358\n","    function value = 1.0729\n","−−−\n","iteration 35\n","gradient norm = 1.0254\n","    function value = 1.0515\n","−−−\n","iteration 36\n","gradient norm = 1.0152\n","    function value = 1.0306\n","−−−\n","iteration 37\n","gradient norm = 1.0050\n","    function value = 1.0101\n","−−−\n","iteration 38\n","gradient norm = 0.9950\n","    function value = 0.9900\n","−−−\n","iteration 39\n","gradient norm = 0.9850\n","    function value = 0.9703\n","−−−\n","iteration 40\n","gradient norm = 0.9752\n","    function value = 0.9510\n","−−−\n","iteration 41\n","gradient norm = 0.9654\n","    function value = 0.9321\n","−−−\n","iteration 42\n","gradient norm = 0.9558\n","    function value = 0.9135\n","−−−\n","iteration 43\n","gradient norm = 0.9462\n","    function value = 0.8953\n","−−−\n","iteration 44\n","gradient norm = 0.9368\n","    function value = 0.8775\n","−−−\n","iteration 45\n","gradient norm = 0.9274\n","    function value = 0.8600\n","−−−\n","iteration 46\n","gradient norm = 0.9181\n","    function value = 0.8429\n","−−−\n","iteration 47\n","gradient norm = 0.9089\n","    function value = 0.8262\n","−−−\n","iteration 48\n","gradient norm = 0.8998\n","    function value = 0.8097\n","−−−\n","iteration 49\n","gradient norm = 0.8908\n","    function value = 0.7936\n","−−−\n","iteration 50\n","gradient norm = 0.8819\n","    function value = 0.7778\n","−−−\n","iteration 51\n","gradient norm = 0.8731\n","    function value = 0.7623\n","−−−\n","iteration 52\n","gradient norm = 0.8644\n","    function value = 0.7472\n","−−−\n","iteration 53\n","gradient norm = 0.8557\n","    function value = 0.7323\n","−−−\n","iteration 54\n","gradient norm = 0.8472\n","    function value = 0.7177\n","−−−\n","iteration 55\n","gradient norm = 0.8387\n","    function value = 0.7034\n","−−−\n","iteration 56\n","gradient norm = 0.8303\n","    function value = 0.6894\n","−−−\n","iteration 57\n","gradient norm = 0.8220\n","    function value = 0.6757\n","−−−\n","iteration 58\n","gradient norm = 0.8138\n","    function value = 0.6623\n","−−−\n","iteration 59\n","gradient norm = 0.8057\n","    function value = 0.6491\n","−−−\n","iteration 60\n","gradient norm = 0.7976\n","    function value = 0.6362\n","−−−\n","iteration 61\n","gradient norm = 0.7896\n","    function value = 0.6235\n","−−−\n","iteration 62\n","gradient norm = 0.7817\n","    function value = 0.6111\n","−−−\n","iteration 63\n","gradient norm = 0.7739\n","    function value = 0.5989\n","−−−\n","iteration 64\n","gradient norm = 0.7662\n","    function value = 0.5870\n","−−−\n","iteration 65\n","gradient norm = 0.7585\n","    function value = 0.5753\n","−−−\n","iteration 66\n","gradient norm = 0.7509\n","    function value = 0.5639\n","−−−\n","iteration 67\n","gradient norm = 0.7434\n","    function value = 0.5527\n","−−−\n","iteration 68\n","gradient norm = 0.7360\n","    function value = 0.5417\n","−−−\n","iteration 69\n","gradient norm = 0.7286\n","    function value = 0.5309\n","−−−\n","iteration 70\n","gradient norm = 0.7213\n","    function value = 0.5203\n","−−−\n","iteration 71\n","gradient norm = 0.7141\n","    function value = 0.5100\n","−−−\n","iteration 72\n","gradient norm = 0.7070\n","    function value = 0.4998\n","−−−\n","iteration 73\n","gradient norm = 0.6999\n","    function value = 0.4899\n","−−−\n","iteration 74\n","gradient norm = 0.6929\n","    function value = 0.4801\n","−−−\n","iteration 75\n","gradient norm = 0.6860\n","    function value = 0.4706\n","−−−\n","iteration 76\n","gradient norm = 0.6791\n","    function value = 0.4612\n","−−−\n","iteration 77\n","gradient norm = 0.6723\n","    function value = 0.4520\n","−−−\n","iteration 78\n","gradient norm = 0.6656\n","    function value = 0.4430\n","−−−\n","iteration 79\n","gradient norm = 0.6590\n","    function value = 0.4342\n","−−−\n","iteration 80\n","gradient norm = 0.6524\n","    function value = 0.4256\n","−−−\n","iteration 81\n","gradient norm = 0.6458\n","    function value = 0.4171\n","−−−\n","iteration 82\n","gradient norm = 0.6394\n","    function value = 0.4088\n","−−−\n","iteration 83\n","gradient norm = 0.6330\n","    function value = 0.4007\n","−−−\n","iteration 84\n","gradient norm = 0.6267\n","    function value = 0.3927\n","−−−\n","iteration 85\n","gradient norm = 0.6204\n","    function value = 0.3849\n","−−−\n","iteration 86\n","gradient norm = 0.6142\n","    function value = 0.3772\n","−−−\n","iteration 87\n","gradient norm = 0.6081\n","    function value = 0.3697\n","−−−\n","iteration 88\n","gradient norm = 0.6020\n","    function value = 0.3624\n","−−−\n","iteration 89\n","gradient norm = 0.5960\n","    function value = 0.3552\n","−−−\n","iteration 90\n","gradient norm = 0.5900\n","    function value = 0.3481\n","−−−\n","iteration 91\n","gradient norm = 0.5841\n","    function value = 0.3412\n","−−−\n","iteration 92\n","gradient norm = 0.5782\n","    function value = 0.3344\n","−−−\n","iteration 93\n","gradient norm = 0.5725\n","    function value = 0.3277\n","−−−\n","iteration 94\n","gradient norm = 0.5667\n","    function value = 0.3212\n","−−−\n","iteration 95\n","gradient norm = 0.5611\n","    function value = 0.3148\n","−−−\n","iteration 96\n","gradient norm = 0.5555\n","    function value = 0.3085\n","−−−\n","iteration 97\n","gradient norm = 0.5499\n","    function value = 0.3024\n","−−−\n","iteration 98\n","gradient norm = 0.5444\n","    function value = 0.2964\n","−−−\n","iteration 99\n","gradient norm = 0.5390\n","    function value = 0.2905\n","−−−\n","iteration 100\n","gradient norm = 0.5336\n","    function value = 0.2847\n","−−−\n","iteration 101\n","gradient norm = 0.5282\n","    function value = 0.2790\n","−−−\n","iteration 102\n","gradient norm = 0.5230\n","    function value = 0.2735\n","−−−\n","iteration 103\n","gradient norm = 0.5177\n","    function value = 0.2680\n","−−−\n","iteration 104\n","gradient norm = 0.5126\n","    function value = 0.2627\n","−−−\n","iteration 105\n","gradient norm = 0.5074\n","    function value = 0.2575\n","−−−\n","iteration 106\n","gradient norm = 0.5024\n","    function value = 0.2524\n","−−−\n","iteration 107\n","gradient norm = 0.4973\n","    function value = 0.2473\n","−−−\n","iteration 108\n","gradient norm = 0.4924\n","    function value = 0.2424\n","−−−\n","iteration 109\n","gradient norm = 0.4874\n","    function value = 0.2376\n","−−−\n","iteration 110\n","gradient norm = 0.4826\n","    function value = 0.2329\n","−−−\n","iteration 111\n","gradient norm = 0.4777\n","    function value = 0.2282\n","−−−\n","iteration 112\n","gradient norm = 0.4730\n","    function value = 0.2237\n","−−−\n","iteration 113\n","gradient norm = 0.4682\n","    function value = 0.2192\n","−−−\n","iteration 114\n","gradient norm = 0.4635\n","    function value = 0.2149\n","−−−\n","iteration 115\n","gradient norm = 0.4589\n","    function value = 0.2106\n","−−−\n","iteration 116\n","gradient norm = 0.4543\n","    function value = 0.2064\n","−−−\n","iteration 117\n","gradient norm = 0.4498\n","    function value = 0.2023\n","−−−\n","iteration 118\n","gradient norm = 0.4453\n","    function value = 0.1983\n","−−−\n","iteration 119\n","gradient norm = 0.4408\n","    function value = 0.1943\n","−−−\n","iteration 120\n","gradient norm = 0.4364\n","    function value = 0.1905\n","−−−\n","iteration 121\n","gradient norm = 0.4321\n","    function value = 0.1867\n","−−−\n","iteration 122\n","gradient norm = 0.4277\n","    function value = 0.1830\n","−−−\n","iteration 123\n","gradient norm = 0.4235\n","    function value = 0.1793\n","−−−\n","iteration 124\n","gradient norm = 0.4192\n","    function value = 0.1757\n","−−−\n","iteration 125\n","gradient norm = 0.4150\n","    function value = 0.1722\n","−−−\n","iteration 126\n","gradient norm = 0.4109\n","    function value = 0.1688\n","−−−\n","iteration 127\n","gradient norm = 0.4068\n","    function value = 0.1655\n","−−−\n","iteration 128\n","gradient norm = 0.4027\n","    function value = 0.1622\n","−−−\n","iteration 129\n","gradient norm = 0.3987\n","    function value = 0.1589\n","−−−\n","iteration 130\n","gradient norm = 0.3947\n","    function value = 0.1558\n","−−−\n","iteration 131\n","gradient norm = 0.3907\n","    function value = 0.1527\n","−−−\n","iteration 132\n","gradient norm = 0.3868\n","    function value = 0.1496\n","−−−\n","iteration 133\n","gradient norm = 0.3830\n","    function value = 0.1467\n","−−−\n","iteration 134\n","gradient norm = 0.3791\n","    function value = 0.1437\n","−−−\n","iteration 135\n","gradient norm = 0.3753\n","    function value = 0.1409\n","−−−\n","iteration 136\n","gradient norm = 0.3716\n","    function value = 0.1381\n","−−−\n","iteration 137\n","gradient norm = 0.3679\n","    function value = 0.1353\n","−−−\n","iteration 138\n","gradient norm = 0.3642\n","    function value = 0.1326\n","−−−\n","iteration 139\n","gradient norm = 0.3606\n","    function value = 0.1300\n","−−−\n","iteration 140\n","gradient norm = 0.3569\n","    function value = 0.1274\n","−−−\n","iteration 141\n","gradient norm = 0.3534\n","    function value = 0.1249\n","−−−\n","iteration 142\n","gradient norm = 0.3498\n","    function value = 0.1224\n","−−−\n","iteration 143\n","gradient norm = 0.3463\n","    function value = 0.1200\n","−−−\n","iteration 144\n","gradient norm = 0.3429\n","    function value = 0.1176\n","−−−\n","iteration 145\n","gradient norm = 0.3395\n","    function value = 0.1152\n","−−−\n","iteration 146\n","gradient norm = 0.3361\n","    function value = 0.1129\n","−−−\n","iteration 147\n","gradient norm = 0.3327\n","    function value = 0.1107\n","−−−\n","iteration 148\n","gradient norm = 0.3294\n","    function value = 0.1085\n","−−−\n","iteration 149\n","gradient norm = 0.3261\n","    function value = 0.1063\n","−−−\n","iteration 150\n","gradient norm = 0.3228\n","    function value = 0.1042\n","−−−\n","iteration 151\n","gradient norm = 0.3196\n","    function value = 0.1021\n","−−−\n","iteration 152\n","gradient norm = 0.3164\n","    function value = 0.1001\n","−−−\n","iteration 153\n","gradient norm = 0.3132\n","    function value = 0.0981\n","−−−\n","iteration 154\n","gradient norm = 0.3101\n","    function value = 0.0962\n","−−−\n","iteration 155\n","gradient norm = 0.3070\n","    function value = 0.0942\n","−−−\n","iteration 156\n","gradient norm = 0.3039\n","    function value = 0.0924\n","−−−\n","iteration 157\n","gradient norm = 0.3009\n","    function value = 0.0905\n","−−−\n","iteration 158\n","gradient norm = 0.2979\n","    function value = 0.0887\n","−−−\n","iteration 159\n","gradient norm = 0.2949\n","    function value = 0.0870\n","−−−\n","iteration 160\n","gradient norm = 0.2920\n","    function value = 0.0852\n","−−−\n","iteration 161\n","gradient norm = 0.2890\n","    function value = 0.0835\n","−−−\n","iteration 162\n","gradient norm = 0.2861\n","    function value = 0.0819\n","−−−\n","iteration 163\n","gradient norm = 0.2833\n","    function value = 0.0802\n","−−−\n","iteration 164\n","gradient norm = 0.2804\n","    function value = 0.0787\n","−−−\n","iteration 165\n","gradient norm = 0.2776\n","    function value = 0.0771\n","−−−\n","iteration 166\n","gradient norm = 0.2749\n","    function value = 0.0756\n","−−−\n","iteration 167\n","gradient norm = 0.2721\n","    function value = 0.0740\n","−−−\n","iteration 168\n","gradient norm = 0.2694\n","    function value = 0.0726\n","−−−\n","iteration 169\n","gradient norm = 0.2667\n","    function value = 0.0711\n","−−−\n","iteration 170\n","gradient norm = 0.2640\n","    function value = 0.0697\n","−−−\n","iteration 171\n","gradient norm = 0.2614\n","    function value = 0.0683\n","−−−\n","iteration 172\n","gradient norm = 0.2588\n","    function value = 0.0670\n","−−−\n","iteration 173\n","gradient norm = 0.2562\n","    function value = 0.0656\n","−−−\n","iteration 174\n","gradient norm = 0.2536\n","    function value = 0.0643\n","−−−\n","iteration 175\n","gradient norm = 0.2511\n","    function value = 0.0630\n","−−−\n","iteration 176\n","gradient norm = 0.2486\n","    function value = 0.0618\n","−−−\n","iteration 177\n","gradient norm = 0.2461\n","    function value = 0.0606\n","−−−\n","iteration 178\n","gradient norm = 0.2436\n","    function value = 0.0594\n","−−−\n","iteration 179\n","gradient norm = 0.2412\n","    function value = 0.0582\n","−−−\n","iteration 180\n","gradient norm = 0.2388\n","    function value = 0.0570\n","−−−\n","iteration 181\n","gradient norm = 0.2364\n","    function value = 0.0559\n","−−−\n","iteration 182\n","gradient norm = 0.2340\n","    function value = 0.0548\n","−−−\n","iteration 183\n","gradient norm = 0.2317\n","    function value = 0.0537\n","−−−\n","iteration 184\n","gradient norm = 0.2294\n","    function value = 0.0526\n","−−−\n","iteration 185\n","gradient norm = 0.2271\n","    function value = 0.0516\n","−−−\n","iteration 186\n","gradient norm = 0.2248\n","    function value = 0.0505\n","−−−\n","iteration 187\n","gradient norm = 0.2226\n","    function value = 0.0495\n","−−−\n","iteration 188\n","gradient norm = 0.2203\n","    function value = 0.0485\n","−−−\n","iteration 189\n","gradient norm = 0.2181\n","    function value = 0.0476\n","−−−\n","iteration 190\n","gradient norm = 0.2160\n","    function value = 0.0466\n","−−−\n","iteration 191\n","gradient norm = 0.2138\n","    function value = 0.0457\n","−−−\n","iteration 192\n","gradient norm = 0.2117\n","    function value = 0.0448\n","−−−\n","iteration 193\n","gradient norm = 0.2095\n","    function value = 0.0439\n","−−−\n","iteration 194\n","gradient norm = 0.2074\n","    function value = 0.0430\n","−−−\n","iteration 195\n","gradient norm = 0.2054\n","    function value = 0.0422\n","−−−\n","iteration 196\n","gradient norm = 0.2033\n","    function value = 0.0413\n","−−−\n","iteration 197\n","gradient norm = 0.2013\n","    function value = 0.0405\n","−−−\n","iteration 198\n","gradient norm = 0.1993\n","    function value = 0.0397\n","−−−\n","iteration 199\n","gradient norm = 0.1973\n","    function value = 0.0389\n","−−−\n","iteration 200\n","gradient norm = 0.1953\n","    function value = 0.0381\n","−−−\n","iteration 201\n","gradient norm = 0.1934\n","    function value = 0.0374\n","−−−\n","iteration 202\n","gradient norm = 0.1914\n","    function value = 0.0366\n","−−−\n","iteration 203\n","gradient norm = 0.1895\n","    function value = 0.0359\n","−−−\n","iteration 204\n","gradient norm = 0.1876\n","    function value = 0.0352\n","−−−\n","iteration 205\n","gradient norm = 0.1857\n","    function value = 0.0345\n","−−−\n","iteration 206\n","gradient norm = 0.1839\n","    function value = 0.0338\n","−−−\n","iteration 207\n","gradient norm = 0.1820\n","    function value = 0.0331\n","−−−\n","iteration 208\n","gradient norm = 0.1802\n","    function value = 0.0325\n","−−−\n","iteration 209\n","gradient norm = 0.1784\n","    function value = 0.0318\n","−−−\n","iteration 210\n","gradient norm = 0.1766\n","    function value = 0.0312\n","−−−\n","iteration 211\n","gradient norm = 0.1749\n","    function value = 0.0306\n","−−−\n","iteration 212\n","gradient norm = 0.1731\n","    function value = 0.0300\n","−−−\n","iteration 213\n","gradient norm = 0.1714\n","    function value = 0.0294\n","−−−\n","iteration 214\n","gradient norm = 0.1697\n","    function value = 0.0288\n","−−−\n","iteration 215\n","gradient norm = 0.1680\n","    function value = 0.0282\n","−−−\n","iteration 216\n","gradient norm = 0.1663\n","    function value = 0.0277\n","−−−\n","iteration 217\n","gradient norm = 0.1646\n","    function value = 0.0271\n","−−−\n","iteration 218\n","gradient norm = 0.1630\n","    function value = 0.0266\n","−−−\n","iteration 219\n","gradient norm = 0.1614\n","    function value = 0.0260\n","−−−\n","iteration 220\n","gradient norm = 0.1597\n","    function value = 0.0255\n","−−−\n","iteration 221\n","gradient norm = 0.1581\n","    function value = 0.0250\n","−−−\n","iteration 222\n","gradient norm = 0.1566\n","    function value = 0.0245\n","−−−\n","iteration 223\n","gradient norm = 0.1550\n","    function value = 0.0240\n","−−−\n","iteration 224\n","gradient norm = 0.1534\n","    function value = 0.0235\n","−−−\n","iteration 225\n","gradient norm = 0.1519\n","    function value = 0.0231\n","−−−\n","iteration 226\n","gradient norm = 0.1504\n","    function value = 0.0226\n","−−−\n","iteration 227\n","gradient norm = 0.1489\n","    function value = 0.0222\n","−−−\n","iteration 228\n","gradient norm = 0.1474\n","    function value = 0.0217\n","−−−\n","iteration 229\n","gradient norm = 0.1459\n","    function value = 0.0213\n","−−−\n","iteration 230\n","gradient norm = 0.1445\n","    function value = 0.0209\n","−−−\n","iteration 231\n","gradient norm = 0.1430\n","    function value = 0.0205\n","−−−\n","iteration 232\n","gradient norm = 0.1416\n","    function value = 0.0200\n","−−−\n","iteration 233\n","gradient norm = 0.1402\n","    function value = 0.0196\n","−−−\n","iteration 234\n","gradient norm = 0.1388\n","    function value = 0.0193\n","−−−\n","iteration 235\n","gradient norm = 0.1374\n","    function value = 0.0189\n","−−−\n","iteration 236\n","gradient norm = 0.1360\n","    function value = 0.0185\n","−−−\n","iteration 237\n","gradient norm = 0.1347\n","    function value = 0.0181\n","−−−\n","iteration 238\n","gradient norm = 0.1333\n","    function value = 0.0178\n","−−−\n","iteration 239\n","gradient norm = 0.1320\n","    function value = 0.0174\n","−−−\n","iteration 240\n","gradient norm = 0.1307\n","    function value = 0.0171\n","−−−\n","iteration 241\n","gradient norm = 0.1293\n","    function value = 0.0167\n","−−−\n","iteration 242\n","gradient norm = 0.1281\n","    function value = 0.0164\n","−−−\n","iteration 243\n","gradient norm = 0.1268\n","    function value = 0.0161\n","−−−\n","iteration 244\n","gradient norm = 0.1255\n","    function value = 0.0158\n","−−−\n","iteration 245\n","gradient norm = 0.1243\n","    function value = 0.0154\n","−−−\n","iteration 246\n","gradient norm = 0.1230\n","    function value = 0.0151\n","−−−\n","iteration 247\n","gradient norm = 0.1218\n","    function value = 0.0148\n","−−−\n","iteration 248\n","gradient norm = 0.1206\n","    function value = 0.0145\n","−−−\n","iteration 249\n","gradient norm = 0.1194\n","    function value = 0.0142\n","−−−\n","iteration 250\n","gradient norm = 0.1182\n","    function value = 0.0140\n","−−−\n","iteration 251\n","gradient norm = 0.1170\n","    function value = 0.0137\n","−−−\n","iteration 252\n","gradient norm = 0.1158\n","    function value = 0.0134\n","−−−\n","iteration 253\n","gradient norm = 0.1147\n","    function value = 0.0131\n","−−−\n","iteration 254\n","gradient norm = 0.1135\n","    function value = 0.0129\n","−−−\n","iteration 255\n","gradient norm = 0.1124\n","    function value = 0.0126\n","−−−\n","iteration 256\n","gradient norm = 0.1112\n","    function value = 0.0124\n","−−−\n","iteration 257\n","gradient norm = 0.1101\n","    function value = 0.0121\n","−−−\n","iteration 258\n","gradient norm = 0.1090\n","    function value = 0.0119\n","−−−\n","iteration 259\n","gradient norm = 0.1079\n","    function value = 0.0117\n","−−−\n","iteration 260\n","gradient norm = 0.1069\n","    function value = 0.0114\n","−−−\n","iteration 261\n","gradient norm = 0.1058\n","    function value = 0.0112\n","−−−\n","iteration 262\n","gradient norm = 0.1047\n","    function value = 0.0110\n","−−−\n","iteration 263\n","gradient norm = 0.1037\n","    function value = 0.0108\n","−−−\n","iteration 264\n","gradient norm = 0.1027\n","    function value = 0.0105\n","−−−\n","iteration 265\n","gradient norm = 0.1016\n","    function value = 0.0103\n","−−−\n","iteration 266\n","gradient norm = 0.1006\n","    function value = 0.0101\n","−−−\n","iteration 267\n","gradient norm = 0.0996\n","    function value = 0.0099\n","−−−\n","iteration 268\n","gradient norm = 0.0986\n","    function value = 0.0097\n","−−−\n","iteration 269\n","gradient norm = 0.0976\n","    function value = 0.0095\n","−−−\n","iteration 270\n","gradient norm = 0.0966\n","    function value = 0.0093\n","−−−\n","iteration 271\n","gradient norm = 0.0957\n","    function value = 0.0092\n","−−−\n","iteration 272\n","gradient norm = 0.0947\n","    function value = 0.0090\n","−−−\n","iteration 273\n","gradient norm = 0.0938\n","    function value = 0.0088\n","−−−\n","iteration 274\n","gradient norm = 0.0928\n","    function value = 0.0086\n","−−−\n","iteration 275\n","gradient norm = 0.0919\n","    function value = 0.0084\n","−−−\n","iteration 276\n","gradient norm = 0.0910\n","    function value = 0.0083\n","−−−\n","iteration 277\n","gradient norm = 0.0901\n","    function value = 0.0081\n","−−−\n","iteration 278\n","gradient norm = 0.0892\n","    function value = 0.0080\n","−−−\n","iteration 279\n","gradient norm = 0.0883\n","    function value = 0.0078\n","−−−\n","iteration 280\n","gradient norm = 0.0874\n","    function value = 0.0076\n","−−−\n","iteration 281\n","gradient norm = 0.0865\n","    function value = 0.0075\n","−−−\n","iteration 282\n","gradient norm = 0.0857\n","    function value = 0.0073\n","−−−\n","iteration 283\n","gradient norm = 0.0848\n","    function value = 0.0072\n","−−−\n","iteration 284\n","gradient norm = 0.0840\n","    function value = 0.0070\n","−−−\n","iteration 285\n","gradient norm = 0.0831\n","    function value = 0.0069\n","−−−\n","iteration 286\n","gradient norm = 0.0823\n","    function value = 0.0068\n","−−−\n","iteration 287\n","gradient norm = 0.0815\n","    function value = 0.0066\n","−−−\n","iteration 288\n","gradient norm = 0.0807\n","    function value = 0.0065\n","−−−\n","iteration 289\n","gradient norm = 0.0798\n","    function value = 0.0064\n","−−−\n","iteration 290\n","gradient norm = 0.0790\n","    function value = 0.0062\n","−−−\n","iteration 291\n","gradient norm = 0.0783\n","    function value = 0.0061\n","−−−\n","iteration 292\n","gradient norm = 0.0775\n","    function value = 0.0060\n","−−−\n","iteration 293\n","gradient norm = 0.0767\n","    function value = 0.0059\n","−−−\n","iteration 294\n","gradient norm = 0.0759\n","    function value = 0.0058\n","−−−\n","iteration 295\n","gradient norm = 0.0752\n","    function value = 0.0057\n","−−−\n","iteration 296\n","gradient norm = 0.0744\n","    function value = 0.0055\n","−−−\n","iteration 297\n","gradient norm = 0.0737\n","    function value = 0.0054\n","−−−\n","iteration 298\n","gradient norm = 0.0729\n","    function value = 0.0053\n","−−−\n","iteration 299\n","gradient norm = 0.0722\n","    function value = 0.0052\n","−−−\n","iteration 300\n","gradient norm = 0.0715\n","    function value = 0.0051\n","−−−\n","iteration 301\n","gradient norm = 0.0708\n","    function value = 0.0050\n","−−−\n","iteration 302\n","gradient norm = 0.0701\n","    function value = 0.0049\n","−−−\n","iteration 303\n","gradient norm = 0.0694\n","    function value = 0.0048\n","−−−\n","iteration 304\n","gradient norm = 0.0687\n","    function value = 0.0047\n","−−−\n","iteration 305\n","gradient norm = 0.0680\n","    function value = 0.0046\n","−−−\n","iteration 306\n","gradient norm = 0.0673\n","    function value = 0.0045\n","−−−\n","iteration 307\n","gradient norm = 0.0666\n","    function value = 0.0044\n","−−−\n","iteration 308\n","gradient norm = 0.0660\n","    function value = 0.0044\n","−−−\n","iteration 309\n","gradient norm = 0.0653\n","    function value = 0.0043\n","−−−\n","iteration 310\n","gradient norm = 0.0647\n","    function value = 0.0042\n","−−−\n","iteration 311\n","gradient norm = 0.0640\n","    function value = 0.0041\n","−−−\n","iteration 312\n","gradient norm = 0.0634\n","    function value = 0.0040\n","−−−\n","iteration 313\n","gradient norm = 0.0627\n","    function value = 0.0039\n","−−−\n","iteration 314\n","gradient norm = 0.0621\n","    function value = 0.0039\n","−−−\n","iteration 315\n","gradient norm = 0.0615\n","    function value = 0.0038\n","−−−\n","iteration 316\n","gradient norm = 0.0609\n","    function value = 0.0037\n","−−−\n","iteration 317\n","gradient norm = 0.0603\n","    function value = 0.0036\n","−−−\n","iteration 318\n","gradient norm = 0.0597\n","    function value = 0.0036\n","−−−\n","iteration 319\n","gradient norm = 0.0591\n","    function value = 0.0035\n","−−−\n","iteration 320\n","gradient norm = 0.0585\n","    function value = 0.0034\n","−−−\n","iteration 321\n","gradient norm = 0.0579\n","    function value = 0.0034\n","−−−\n","iteration 322\n","gradient norm = 0.0573\n","    function value = 0.0033\n","−−−\n","iteration 323\n","gradient norm = 0.0567\n","    function value = 0.0032\n","−−−\n","iteration 324\n","gradient norm = 0.0562\n","    function value = 0.0032\n","−−−\n","iteration 325\n","gradient norm = 0.0556\n","    function value = 0.0031\n","−−−\n","iteration 326\n","gradient norm = 0.0550\n","    function value = 0.0030\n","−−−\n","iteration 327\n","gradient norm = 0.0545\n","    function value = 0.0030\n","−−−\n","iteration 328\n","gradient norm = 0.0540\n","    function value = 0.0029\n","−−−\n","iteration 329\n","gradient norm = 0.0534\n","    function value = 0.0029\n","−−−\n","iteration 330\n","gradient norm = 0.0529\n","    function value = 0.0028\n","−−−\n","iteration 331\n","gradient norm = 0.0524\n","    function value = 0.0027\n","−−−\n","iteration 332\n","gradient norm = 0.0518\n","    function value = 0.0027\n","−−−\n","iteration 333\n","gradient norm = 0.0513\n","    function value = 0.0026\n","−−−\n","iteration 334\n","gradient norm = 0.0508\n","    function value = 0.0026\n","−−−\n","iteration 335\n","gradient norm = 0.0503\n","    function value = 0.0025\n","−−−\n","iteration 336\n","gradient norm = 0.0498\n","    function value = 0.0025\n","−−−\n","iteration 337\n","gradient norm = 0.0493\n","    function value = 0.0024\n","−−−\n","iteration 338\n","gradient norm = 0.0488\n","    function value = 0.0024\n","−−−\n","iteration 339\n","gradient norm = 0.0483\n","    function value = 0.0023\n","−−−\n","iteration 340\n","gradient norm = 0.0478\n","    function value = 0.0023\n","−−−\n","iteration 341\n","gradient norm = 0.0473\n","    function value = 0.0022\n","−−−\n","iteration 342\n","gradient norm = 0.0469\n","    function value = 0.0022\n","−−−\n","iteration 343\n","gradient norm = 0.0464\n","    function value = 0.0022\n","−−−\n","iteration 344\n","gradient norm = 0.0459\n","    function value = 0.0021\n","−−−\n","iteration 345\n","gradient norm = 0.0455\n","    function value = 0.0021\n","−−−\n","iteration 346\n","gradient norm = 0.0450\n","    function value = 0.0020\n","−−−\n","iteration 347\n","gradient norm = 0.0446\n","    function value = 0.0020\n","−−−\n","iteration 348\n","gradient norm = 0.0441\n","    function value = 0.0019\n","−−−\n","iteration 349\n","gradient norm = 0.0437\n","    function value = 0.0019\n","−−−\n","iteration 350\n","gradient norm = 0.0433\n","    function value = 0.0019\n","−−−\n","iteration 351\n","gradient norm = 0.0428\n","    function value = 0.0018\n","−−−\n","iteration 352\n","gradient norm = 0.0424\n","    function value = 0.0018\n","−−−\n","iteration 353\n","gradient norm = 0.0420\n","    function value = 0.0018\n","−−−\n","iteration 354\n","gradient norm = 0.0415\n","    function value = 0.0017\n","−−−\n","iteration 355\n","gradient norm = 0.0411\n","    function value = 0.0017\n","−−−\n","iteration 356\n","gradient norm = 0.0407\n","    function value = 0.0017\n","−−−\n","iteration 357\n","gradient norm = 0.0403\n","    function value = 0.0016\n","−−−\n","iteration 358\n","gradient norm = 0.0399\n","    function value = 0.0016\n","−−−\n","iteration 359\n","gradient norm = 0.0395\n","    function value = 0.0016\n","−−−\n","iteration 360\n","gradient norm = 0.0391\n","    function value = 0.0015\n","−−−\n","iteration 361\n","gradient norm = 0.0387\n","    function value = 0.0015\n","−−−\n","iteration 362\n","gradient norm = 0.0383\n","    function value = 0.0015\n","−−−\n","iteration 363\n","gradient norm = 0.0380\n","    function value = 0.0014\n","−−−\n","iteration 364\n","gradient norm = 0.0376\n","    function value = 0.0014\n","−−−\n","iteration 365\n","gradient norm = 0.0372\n","    function value = 0.0014\n","−−−\n","iteration 366\n","gradient norm = 0.0368\n","    function value = 0.0014\n","−−−\n","iteration 367\n","gradient norm = 0.0365\n","    function value = 0.0013\n","−−−\n","iteration 368\n","gradient norm = 0.0361\n","    function value = 0.0013\n","−−−\n","iteration 369\n","gradient norm = 0.0357\n","    function value = 0.0013\n","−−−\n","iteration 370\n","gradient norm = 0.0354\n","    function value = 0.0013\n","−−−\n","iteration 371\n","gradient norm = 0.0350\n","    function value = 0.0012\n","−−−\n","iteration 372\n","gradient norm = 0.0347\n","    function value = 0.0012\n","−−−\n","iteration 373\n","gradient norm = 0.0343\n","    function value = 0.0012\n","−−−\n","iteration 374\n","gradient norm = 0.0340\n","    function value = 0.0012\n","−−−\n","iteration 375\n","gradient norm = 0.0336\n","    function value = 0.0011\n","−−−\n","iteration 376\n","gradient norm = 0.0333\n","    function value = 0.0011\n","−−−\n","iteration 377\n","gradient norm = 0.0330\n","    function value = 0.0011\n","−−−\n","iteration 378\n","gradient norm = 0.0326\n","    function value = 0.0011\n","−−−\n","iteration 379\n","gradient norm = 0.0323\n","    function value = 0.0010\n","−−−\n","iteration 380\n","gradient norm = 0.0320\n","    function value = 0.0010\n","−−−\n","iteration 381\n","gradient norm = 0.0317\n","    function value = 0.0010\n","−−−\n","iteration 382\n","gradient norm = 0.0314\n","    function value = 0.0010\n","−−−\n","iteration 383\n","gradient norm = 0.0310\n","    function value = 0.0010\n","−−−\n","iteration 384\n","gradient norm = 0.0307\n","    function value = 0.0009\n","−−−\n","iteration 385\n","gradient norm = 0.0304\n","    function value = 0.0009\n","−−−\n","iteration 386\n","gradient norm = 0.0301\n","    function value = 0.0009\n","−−−\n","iteration 387\n","gradient norm = 0.0298\n","    function value = 0.0009\n","−−−\n","iteration 388\n","gradient norm = 0.0295\n","    function value = 0.0009\n","−−−\n","iteration 389\n","gradient norm = 0.0292\n","    function value = 0.0009\n","−−−\n","iteration 390\n","gradient norm = 0.0289\n","    function value = 0.0008\n","−−−\n","iteration 391\n","gradient norm = 0.0286\n","    function value = 0.0008\n","−−−\n","iteration 392\n","gradient norm = 0.0284\n","    function value = 0.0008\n","−−−\n","iteration 393\n","gradient norm = 0.0281\n","    function value = 0.0008\n","−−−\n","iteration 394\n","gradient norm = 0.0278\n","    function value = 0.0008\n","−−−\n","iteration 395\n","gradient norm = 0.0275\n","    function value = 0.0008\n","−−−\n","iteration 396\n","gradient norm = 0.0272\n","    function value = 0.0007\n","−−−\n","iteration 397\n","gradient norm = 0.0270\n","    function value = 0.0007\n","−−−\n","iteration 398\n","gradient norm = 0.0267\n","    function value = 0.0007\n","−−−\n","iteration 399\n","gradient norm = 0.0264\n","    function value = 0.0007\n","−−−\n","iteration 400\n","gradient norm = 0.0262\n","    function value = 0.0007\n","−−−\n","iteration 401\n","gradient norm = 0.0259\n","    function value = 0.0007\n","−−−\n","iteration 402\n","gradient norm = 0.0256\n","    function value = 0.0007\n","−−−\n","iteration 403\n","gradient norm = 0.0254\n","    function value = 0.0006\n","−−−\n","iteration 404\n","gradient norm = 0.0251\n","    function value = 0.0006\n","−−−\n","iteration 405\n","gradient norm = 0.0249\n","    function value = 0.0006\n","−−−\n","iteration 406\n","gradient norm = 0.0246\n","    function value = 0.0006\n","−−−\n","iteration 407\n","gradient norm = 0.0244\n","    function value = 0.0006\n","−−−\n","iteration 408\n","gradient norm = 0.0241\n","    function value = 0.0006\n","−−−\n","iteration 409\n","gradient norm = 0.0239\n","    function value = 0.0006\n","−−−\n","iteration 410\n","gradient norm = 0.0237\n","    function value = 0.0006\n","−−−\n","iteration 411\n","gradient norm = 0.0234\n","    function value = 0.0005\n","−−−\n","iteration 412\n","gradient norm = 0.0232\n","    function value = 0.0005\n","−−−\n","iteration 413\n","gradient norm = 0.0230\n","    function value = 0.0005\n","−−−\n","iteration 414\n","gradient norm = 0.0227\n","    function value = 0.0005\n","−−−\n","iteration 415\n","gradient norm = 0.0225\n","    function value = 0.0005\n","−−−\n","iteration 416\n","gradient norm = 0.0223\n","    function value = 0.0005\n","−−−\n","iteration 417\n","gradient norm = 0.0221\n","    function value = 0.0005\n","−−−\n","iteration 418\n","gradient norm = 0.0218\n","    function value = 0.0005\n","−−−\n","iteration 419\n","gradient norm = 0.0216\n","    function value = 0.0005\n","−−−\n","iteration 420\n","gradient norm = 0.0214\n","    function value = 0.0005\n","−−−\n","iteration 421\n","gradient norm = 0.0212\n","    function value = 0.0004\n","−−−\n","iteration 422\n","gradient norm = 0.0210\n","    function value = 0.0004\n","−−−\n","iteration 423\n","gradient norm = 0.0208\n","    function value = 0.0004\n","−−−\n","iteration 424\n","gradient norm = 0.0206\n","    function value = 0.0004\n","−−−\n","iteration 425\n","gradient norm = 0.0204\n","    function value = 0.0004\n","−−−\n","iteration 426\n","gradient norm = 0.0201\n","    function value = 0.0004\n","−−−\n","iteration 427\n","gradient norm = 0.0199\n","    function value = 0.0004\n","−−−\n","iteration 428\n","gradient norm = 0.0197\n","    function value = 0.0004\n","−−−\n","iteration 429\n","gradient norm = 0.0196\n","    function value = 0.0004\n","−−−\n","iteration 430\n","gradient norm = 0.0194\n","    function value = 0.0004\n","−−−\n","iteration 431\n","gradient norm = 0.0192\n","    function value = 0.0004\n","−−−\n","iteration 432\n","gradient norm = 0.0190\n","    function value = 0.0004\n","−−−\n","iteration 433\n","gradient norm = 0.0188\n","    function value = 0.0004\n","−−−\n","iteration 434\n","gradient norm = 0.0186\n","    function value = 0.0003\n","−−−\n","iteration 435\n","gradient norm = 0.0184\n","    function value = 0.0003\n","−−−\n","iteration 436\n","gradient norm = 0.0182\n","    function value = 0.0003\n","−−−\n","iteration 437\n","gradient norm = 0.0180\n","    function value = 0.0003\n","−−−\n","iteration 438\n","gradient norm = 0.0179\n","    function value = 0.0003\n","−−−\n","iteration 439\n","gradient norm = 0.0177\n","    function value = 0.0003\n","−−−\n","iteration 440\n","gradient norm = 0.0175\n","    function value = 0.0003\n","−−−\n","iteration 441\n","gradient norm = 0.0173\n","    function value = 0.0003\n","−−−\n","iteration 442\n","gradient norm = 0.0172\n","    function value = 0.0003\n","−−−\n","iteration 443\n","gradient norm = 0.0170\n","    function value = 0.0003\n","−−−\n","iteration 444\n","gradient norm = 0.0168\n","    function value = 0.0003\n","−−−\n","iteration 445\n","gradient norm = 0.0166\n","    function value = 0.0003\n","−−−\n","iteration 446\n","gradient norm = 0.0165\n","    function value = 0.0003\n","−−−\n","iteration 447\n","gradient norm = 0.0163\n","    function value = 0.0003\n","−−−\n","iteration 448\n","gradient norm = 0.0162\n","    function value = 0.0003\n","−−−\n","iteration 449\n","gradient norm = 0.0160\n","    function value = 0.0003\n","−−−\n","iteration 450\n","gradient norm = 0.0158\n","    function value = 0.0003\n","−−−\n","iteration 451\n","gradient norm = 0.0157\n","    function value = 0.0002\n","−−−\n","iteration 452\n","gradient norm = 0.0155\n","    function value = 0.0002\n","−−−\n","iteration 453\n","gradient norm = 0.0154\n","    function value = 0.0002\n","−−−\n","iteration 454\n","gradient norm = 0.0152\n","    function value = 0.0002\n","−−−\n","iteration 455\n","gradient norm = 0.0151\n","    function value = 0.0002\n","−−−\n","iteration 456\n","gradient norm = 0.0149\n","    function value = 0.0002\n","−−−\n","iteration 457\n","gradient norm = 0.0148\n","    function value = 0.0002\n","−−−\n","iteration 458\n","gradient norm = 0.0146\n","    function value = 0.0002\n","−−−\n","iteration 459\n","gradient norm = 0.0145\n","    function value = 0.0002\n","−−−\n","iteration 460\n","gradient norm = 0.0143\n","    function value = 0.0002\n","−−−\n","iteration 461\n","gradient norm = 0.0142\n","    function value = 0.0002\n","−−−\n","iteration 462\n","gradient norm = 0.0140\n","    function value = 0.0002\n","−−−\n","iteration 463\n","gradient norm = 0.0139\n","    function value = 0.0002\n","−−−\n","iteration 464\n","gradient norm = 0.0138\n","    function value = 0.0002\n","−−−\n","iteration 465\n","gradient norm = 0.0136\n","    function value = 0.0002\n","−−−\n","iteration 466\n","gradient norm = 0.0135\n","    function value = 0.0002\n","−−−\n","iteration 467\n","gradient norm = 0.0133\n","    function value = 0.0002\n","−−−\n","iteration 468\n","gradient norm = 0.0132\n","    function value = 0.0002\n","−−−\n","iteration 469\n","gradient norm = 0.0131\n","    function value = 0.0002\n","−−−\n","iteration 470\n","gradient norm = 0.0129\n","    function value = 0.0002\n","−−−\n","iteration 471\n","gradient norm = 0.0128\n","    function value = 0.0002\n","−−−\n","iteration 472\n","gradient norm = 0.0127\n","    function value = 0.0002\n","−−−\n","iteration 473\n","gradient norm = 0.0126\n","    function value = 0.0002\n","−−−\n","iteration 474\n","gradient norm = 0.0124\n","    function value = 0.0002\n","−−−\n","iteration 475\n","gradient norm = 0.0123\n","    function value = 0.0002\n","−−−\n","iteration 476\n","gradient norm = 0.0122\n","    function value = 0.0001\n","−−−\n","iteration 477\n","gradient norm = 0.0121\n","    function value = 0.0001\n","−−−\n","iteration 478\n","gradient norm = 0.0119\n","    function value = 0.0001\n","−−−\n","iteration 479\n","gradient norm = 0.0118\n","    function value = 0.0001\n","−−−\n","iteration 480\n","gradient norm = 0.0117\n","    function value = 0.0001\n","−−−\n","iteration 481\n","gradient norm = 0.0116\n","    function value = 0.0001\n","−−−\n","iteration 482\n","gradient norm = 0.0115\n","    function value = 0.0001\n","−−−\n","iteration 483\n","gradient norm = 0.0114\n","    function value = 0.0001\n","−−−\n","iteration 484\n","gradient norm = 0.0112\n","    function value = 0.0001\n","−−−\n","iteration 485\n","gradient norm = 0.0111\n","    function value = 0.0001\n","−−−\n","iteration 486\n","gradient norm = 0.0110\n","    function value = 0.0001\n","−−−\n","iteration 487\n","gradient norm = 0.0109\n","    function value = 0.0001\n","−−−\n","iteration 488\n","gradient norm = 0.0108\n","    function value = 0.0001\n","−−−\n","iteration 489\n","gradient norm = 0.0107\n","    function value = 0.0001\n","−−−\n","iteration 490\n","gradient norm = 0.0106\n","    function value = 0.0001\n","−−−\n","iteration 491\n","gradient norm = 0.0105\n","    function value = 0.0001\n","−−−\n","iteration 492\n","gradient norm = 0.0104\n","    function value = 0.0001\n","−−−\n","iteration 493\n","gradient norm = 0.0103\n","    function value = 0.0001\n","−−−\n","iteration 494\n","gradient norm = 0.0102\n","    function value = 0.0001\n","−−−\n","iteration 495\n","gradient norm = 0.0101\n","    function value = 0.0001\n","−−−\n","iteration 496\n","gradient norm = 0.0100\n","    function value = 0.0001\n","∗∗∗optimal solution found\n","\n","\n","\n","best solution found:[0.00677831 0.00362348 0.00635166]\n","best objective value:9.941871520213981e-05\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FHIB1cGzxHqE","colab_type":"text"},"source":["\n"]},{"cell_type":"code","metadata":{"id":"XStwe4frHKbM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdAmcAAIFYEX","colab_type":"text"},"source":["### Stochastic Gradient Descent\n","Ý tưởng: tìm tối ưu bằng cách di chuyển ngược đạo hàm.\n","- stochastic (ngẫu nhiên), chỉ xử dụng 1 điểm dữ liệu $x+i$ để tính đạo hàm rồi cập nhật GD"]},{"cell_type":"markdown","metadata":{"id":"nGvgFgUlILwj","colab_type":"text"},"source":["có code mẫu ở trang MLc cơ bản"]},{"cell_type":"markdown","metadata":{"id":"Wjl9jSnAFmcz","colab_type":"text"},"source":["### Tiêu chí khác\n","## Các thuật toán tối ưu Gradient Descent\n","- Momentum: ý tưởng là khi cho thêm 1 đại lượng vận tốc, để khi gặp local-minimun hàm GD có thể vượt qua, và thực sự hội tụ ở global-minimum. *Lượng thay đổi* = tổng hai vector momentum vector và gradient ở hiện tại\n","- Nesterov accelerated gradient (NAG): giải quyết đà của momentum để hội tụ nhanh hơn, điều chỉnh *lượng thay đổi*  = tổng hai vector momentum vector và gradient ở thời điểm được xấp xỉ là điểm tiếp theo\n","\n","## Stopping Criteria (điều kiện dừng)\n","\n","Có một điểm cũng quan trọng mà từ đầu tôi chưa nhắc đến: khi nào thì chúng ta biết thuật toán đã hội tụ và dừng lại?\n","\n","Trong thực nghiệm, có một vài phương pháp như dưới đây:\n","\n","*  Giới hạn số vòng lặp: đây là phương pháp phổ biến nhất và cũng để đảm bảo rằng chương trình chạy không quá lâu. Tuy nhiên, một nhược điểm của cách làm này là có thể thuật toán dừng lại trước khi đủ gần với nghiệm.\n","*  So sánh gradient của nghiệm tại hai lần cập nhật liên tiếp, khi nào giá trị này đủ nhỏ thì dừng lại. Phương pháp này cũng có một nhược điểm lớn là việc tính đạo hàm đôi khi trở nên quá phức tạp (ví dụ như khi có quá nhiều dữ liệu), nếu áp dụng phương pháp này thì coi như ta không được lợi khi sử dụng SGD và mini-batch GD.\n","*  So sánh giá trị của hàm mất mát của nghiệm tại hai lần cập nhật liên tiếp, khi nào giá trị này đủ nhỏ thì dừng lại. Nhược điểm của phương pháp này là nếu tại một thời điểm, đồ thị hàm số có dạng bẳng phẳng tại một khu vực nhưng khu vực đó không chứa điểm local minimum (khu vực này thường được gọi là saddle points), thuật toán cũng dừng lại trước khi đạt giá trị mong muốn.\n","*  Trong SGD và mini-batch GD, cách thường dùng là so sánh nghiệm sau một vài lần cập nhật. Trong đoạn code Python phía trên về SGD, tôi áp dụng việc so sánh này mỗi khi nghiệm được cập nhật 10 lần. Việc làm này cũng tỏ ra khá hiệu quả.\n","\n","\n","### Discuss and code\n","[1](https://machinelearningcoban.com/2017/01/16/gradientdescent2/#-cac-thuat-toan-toi-uu-gradient-descent)"]},{"cell_type":"code","metadata":{"id":"9VW9EeNSFXLm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RrB7-kxIg4r","colab_type":"text"},"source":["## Newton’s method"]},{"cell_type":"markdown","metadata":{"id":"UM82_bDlcfiG","colab_type":"text"},"source":["# 3.Probability\n","**RL-Standford**\n","\n","- http://cs229.stanford.edu/section/cs229-prob.pdf\n","\n","**VEF-ML**\n","\n","- VEF-[HW2 - General Probability](https://piazza-resources.s3.amazonaws.com/joewsz9eh732za/jooz16tbwt4381/HW_2.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAR6AWVCBXQV3LK55Z%2F20190811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190811T155612Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEDQaCXVzLWVhc3QtMSJHMEUCIQC23VphLywkz%2BqhRKJeBYI2kNH78ukglfgKZGiIQ3UFmQIgICPELbRf91SZ3mLNXmq5JLwQLMm0JyqxfmXWG%2FWfcp0q4wMIvP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwxMzMxOTE1MDM5ODMiDKI48V%2FKrlL0N2jsvyq3AzvVGse5DCKGdgp5%2BtA%2Fn6wUuwI0sb44vJXE4C5JC9vQn4uiZV5WBmszGQqrStuABTYt1KZBmWPA7xAG8SHtdMgUWxHgAb3bO8DpiDwOv2e8wE1SxYbzrXVSy03D%2B3VbuRYzxQHCP2qTSE9kXCQjP47OE1hndbs5wqg5V5K228XhV9%2FADzkbdA1KXMijGZLiMOoyxwIQFR5uA3hyL3mQDHZkOYC4Y6SY4BbDCdzRjr4Bnj71T5LrGCZMQzlkkFss4mg0rqtUmLZKEo7WbJD4c4YxnWq4GaK4GRiVPsE7JmUTVU08zZp%2FOvFpChKxDEFdAXsl0upsVY%2F25kjYsI1dNWgvF%2FzzOqTgpXxp26hh9hnkp5QBJe3WxpbfWe7yMOA6sluoV97D%2B6%2FODN5Hjx3nm%2FFIXhoy6a6y%2BO4g6nN32goD4t1IlzLo4UAYhXQwsBM7GIY26b8ERhCaqpUmW5iw7DGn6sdpLjdSXfkuO%2BpIYQQrFJ3BXrTFW7aX97VEb%2B1dDCTHDc3we8%2B1KfvAT1ZTqEsxt180dTH5GXUbu40qnJYzoIYvEhryuUGPVWp3Bz0H%2BAf5QWvICAQw9%2FG%2F6gU6tAGdHWqEEHLgpZ4JDdngtjN4ycxOpUuavPxBWWckwHdf9FVchAoTxNawDLYp5GYkkWgcFlLITNANSwR0UyHih5vgr29VL5rv3%2BDxcOqfWRHJ1x7x%2FmMJlqReVF%2FLjhETfHH0ov5Ge2vt6m%2FWvswKZ1R5aMi1Ek7LG4kSOxWrt6o27VjN2v1JmyfVVndgSyn%2BrGOiJyfPowsOHphPXRV%2BSeugs5wyJ1%2BYrlpfhSWvlrCpZbwNtTE%3D&X-Amz-Signature=6ec31dcf70a5fa71782a1b3f9a6e06cb4c20970aa5709323225cf619aac64aa1) - [Solution](https://piazza-resources.s3.amazonaws.com/joewsz9eh732za/jpk651z4wbh27r/solutions_2.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=ASIAR6AWVCBXQV3LK55Z%2F20190811%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190811T155615Z&X-Amz-Expires=3600&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEDQaCXVzLWVhc3QtMSJHMEUCIQC23VphLywkz%2BqhRKJeBYI2kNH78ukglfgKZGiIQ3UFmQIgICPELbRf91SZ3mLNXmq5JLwQLMm0JyqxfmXWG%2FWfcp0q4wMIvP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAAGgwxMzMxOTE1MDM5ODMiDKI48V%2FKrlL0N2jsvyq3AzvVGse5DCKGdgp5%2BtA%2Fn6wUuwI0sb44vJXE4C5JC9vQn4uiZV5WBmszGQqrStuABTYt1KZBmWPA7xAG8SHtdMgUWxHgAb3bO8DpiDwOv2e8wE1SxYbzrXVSy03D%2B3VbuRYzxQHCP2qTSE9kXCQjP47OE1hndbs5wqg5V5K228XhV9%2FADzkbdA1KXMijGZLiMOoyxwIQFR5uA3hyL3mQDHZkOYC4Y6SY4BbDCdzRjr4Bnj71T5LrGCZMQzlkkFss4mg0rqtUmLZKEo7WbJD4c4YxnWq4GaK4GRiVPsE7JmUTVU08zZp%2FOvFpChKxDEFdAXsl0upsVY%2F25kjYsI1dNWgvF%2FzzOqTgpXxp26hh9hnkp5QBJe3WxpbfWe7yMOA6sluoV97D%2B6%2FODN5Hjx3nm%2FFIXhoy6a6y%2BO4g6nN32goD4t1IlzLo4UAYhXQwsBM7GIY26b8ERhCaqpUmW5iw7DGn6sdpLjdSXfkuO%2BpIYQQrFJ3BXrTFW7aX97VEb%2B1dDCTHDc3we8%2B1KfvAT1ZTqEsxt180dTH5GXUbu40qnJYzoIYvEhryuUGPVWp3Bz0H%2BAf5QWvICAQw9%2FG%2F6gU6tAGdHWqEEHLgpZ4JDdngtjN4ycxOpUuavPxBWWckwHdf9FVchAoTxNawDLYp5GYkkWgcFlLITNANSwR0UyHih5vgr29VL5rv3%2BDxcOqfWRHJ1x7x%2FmMJlqReVF%2FLjhETfHH0ov5Ge2vt6m%2FWvswKZ1R5aMi1Ek7LG4kSOxWrt6o27VjN2v1JmyfVVndgSyn%2BrGOiJyfPowsOHphPXRV%2BSeugs5wyJ1%2BYrlpfhSWvlrCpZbwNtTE%3D&X-Amz-Signature=485f76b6ec74313f0c315a1abada42e761cc20cdcdab1f97663e038e70a9e7a2)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hKodVX5RIyxK","colab_type":"text"},"source":["## Maximum likelihood\n","chương 4 sách ML cơ bản"]},{"cell_type":"code","metadata":{"id":"qG6V05HJZ-fL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q-hURpxcIydw","colab_type":"text"},"source":[""]}]}