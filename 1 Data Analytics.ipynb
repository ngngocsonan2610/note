{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Data Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WzFvPys_F9KB"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/1%20Data%20Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvORvDCvAdH6",
        "colab_type": "text"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNimUPIBigtD",
        "colab_type": "code",
        "outputId": "9f9624ce-279e-4275-8b70-d705c63cc2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "%%html\n",
        "<iframe src=\"https://stackedit.io/app#\" width=\"1000\" height=\"600\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://stackedit.io/app#\" width=\"1000\" height=\"600\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmtsCrqtu-rI",
        "colab_type": "text"
      },
      "source": [
        "- [**A Comprehensive Guide to Data Exploration**](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)\n",
        "- [Data Analytics Collection - Hasbrain](https://note.hasbrain.com/5c875913b40548001824aa67?target=path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltyLBhWQm2YF",
        "colab_type": "text"
      },
      "source": [
        "# 1.EDA & Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI2U0U4dm7eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXBVoIJDDjN",
        "colab_type": "text"
      },
      "source": [
        "## Resume & Phân tích df\n",
        "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data.\n",
        "- Phân tích về min max, mean, median, mode, std\n",
        "- Phân tích về loại của feature (continous/categorical, numeric/text)\n",
        "- Đếm giá trị unique ở features/ giá trị missing \n",
        "\n",
        "**Sources**:\n",
        "- [Đọc thêm 1](https://towardsdatascience.com/understanding-descriptive-statistics-c9c2b0641291)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91jE-tpqCsjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function assits to expand Exploratory Data Analysis (EDA) is an open-ended \n",
        "#process where we calculate statistics and make figures to find trends, anomalies,\n",
        "#patterns, or relationships within the data. The goal of EDA is to learn what our\n",
        "#data can tell us. It generally starts out with a high level overview, \n",
        "#then narrows in to specific areas as we find intriguing areas of the data. \n",
        "\n",
        "def des_stat_analyze(df_input):\n",
        "    # check number of rows, cols\n",
        "    no_rows = df_input.shape[0]\n",
        "    no_cols = df_input.shape[1]\n",
        "    print(\"No. observations:\", no_rows )\n",
        "    print(\"No. features:\", no_cols )\n",
        "  \n",
        "    # checking type of features\n",
        "    name = []\n",
        "    cols_type = []\n",
        "    for n,t in df_input.dtypes.iteritems():\n",
        "        name.append(n)\n",
        "        cols_type.append(t)\n",
        "\n",
        "    # checking distinction (unique values) of features\n",
        "    ls_unique = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nunique = df_input[cname].nunique()\n",
        "            pct_unique = nunique*100.0/ no_rows\n",
        "            ls_unique.append(\"{} ({:0.2f}%)\".format(nunique, pct_unique))\n",
        "        except:\n",
        "            ls_unique.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue\n",
        "\n",
        "    # checking missing values of features\n",
        "    ls_miss = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nmiss = df_input[cname].isnull().sum()\n",
        "            pct_miss = nmiss*100.0/ no_rows\n",
        "            ls_miss.append(\"{} ({:0.2f}%)\".format(nmiss, pct_miss))\n",
        "        except:\n",
        "            ls_miss.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue \n",
        "      \n",
        "    # checking zeros\n",
        "    ls_zeros = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nzeros = (df_input[cname] == 0).sum()\n",
        "            pct_zeros = nzeros * 100.0/ no_rows\n",
        "            ls_zeros.append(\"{} ({:0.2f}%)\".fornat(nzeros, pct_zeros))\n",
        "        except:\n",
        "            ls_zeros.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue\n",
        "      \n",
        "    # checking negative values\n",
        "    ls_neg = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nneg = (df_input[cname].astype(\"float\")<0).sum()\n",
        "            pct_neg =nneg * 100.0 / no_rows\n",
        "            ls_neg.append(\"{} ({:0.2f}%)\".format(nneg, pct_neg))\n",
        "        except:\n",
        "            ls_neg.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue\n",
        "      \n",
        "    # extracting the output\n",
        "    data = {\n",
        "      \"name\": name,\n",
        "      \"col_type\": cols_type,\n",
        "      \"n_unique\": ls_unique,\n",
        "      \"n_miss\": ls_miss,\n",
        "      \"n_zeros\":ls_zeros,\n",
        "      \"n_neg\":ls_neg      \n",
        "    }\n",
        "  \n",
        "    # statistical info\n",
        "    df_stats = df_input.describe().transpose()\n",
        "    ex_stats = pd.concat([df_input.median(),df_input.kurtosis(),df_input.skew()], axis = 1)\n",
        "    ex_stats = ex_stats.rename(columns={0:\"median\",1:\"kurtosis\",2:\"skew\"})\n",
        "    df_stats = df_stats.merge(ex_stats, left_index=True, right_index=True)\n",
        "    #ls_stats = []\n",
        "    for stat in df_stats.columns:\n",
        "        data[stat] = []\n",
        "        for cname in df_input.columns:\n",
        "            try:\n",
        "                data[stat].append(df_stats.loc[cname, stat])\n",
        "            except:\n",
        "                data[stat].append(\"NaN\")\n",
        "        \n",
        "    # take samples\n",
        "    df_sample = df_input.sample(frac = .5).head().transpose()\n",
        "    df_sample.columns = [\"sample_{}\".format(i) for i in range(5)]\n",
        "  \n",
        "    # repair the output\n",
        "    col_ordered = [\"name\",\"col_type\",\"count\",\"n_unique\",\"n_miss\",\"n_zeros\",\"n_neg\",\n",
        "                \"25%\",\"50%\",\"75%\",\"max\",\"min\",\"mean\",\"median\",\"std\",\"kurtosis\",\"skew\"]\n",
        "    df_data = pd.DataFrame(data, columns = col_ordered).set_index(\"name\")\n",
        "    df_data = pd.concat([df_data, df_sample], axis = 1)\n",
        "\n",
        "    return df_data   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEhCkATJnX0l",
        "colab_type": "text"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "Xem correlation của các features với nhau: \n",
        "- 2 features có chỉ số correlation dương, có nghĩa khi A tăng thì B tăng (if A increase, B will increase)\n",
        "- 2 features có chỉ số correlation âm, có nghĩa khi A tăng thì B giảm (if A increase, B will decrease)\n",
        "- - 2 features có chỉ số correlation cao, có nghĩa là 2 features này có khả năng thay thế nhau trong model (replaces)\n",
        "\n",
        "Sources:\n",
        "-  [Covariance vs Correlation](https://forum.machinelearningcoban.com/t/covariance-va-correlation/767)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGrqIFBwDAUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_corr(df):\n",
        "    correlations = df.corr()\n",
        "\n",
        "    # Using seaborn package\n",
        "    # Generate a mask for the upper triangle\n",
        "    mask = np.zeros_like(correlations, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "    # Set up the matplotlib figure\n",
        "    f, ax = plt.subplots(figsize=(16, 16))\n",
        "\n",
        "    # Generate a custom diverging colormap\n",
        "    cmap = sns.diverging_palette(260, 10, as_cmap=True)\n",
        "\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(correlations, mask=mask, cmap=cmap, vmin = -1, vmax= 1 , center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "    plt.yticks(rotation=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TkVsIzFm7mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_topcorr_bycol(col):\n",
        "    df_corr = df.corr().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "    df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}\n",
        "                , inplace=True)     \n",
        "    return df_corr[df_corr[\"Feature 1\"]==col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUdukOL-nnMp",
        "colab_type": "text"
      },
      "source": [
        "## Plot continous / categorical features\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLva7-YEm7x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"Plotting the continous features :    \n",
        "1. A box plot (or box-and-whisker plot) shows the distribution of quantitative data \n",
        "in a way that facilitates comparisons between variables.\n",
        "2. Distribution graph :to check the linearity of the variables and look \n",
        "for skewness of features.\"\"\"\n",
        "def plot_continous(cols, df, target=None, info = False, corr = False, drop_3Qplot=False):\n",
        "    # Using boxplot to analyze the continous feature\n",
        "    if corr == True:\n",
        "      df_corr = df.corr().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "      df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}\n",
        "                , inplace=True)      \n",
        "\n",
        "    for col in cols:\n",
        "        print(f'\\n >Column name: {col}')\n",
        "        plt.figure(figsize=(15,8))\n",
        "        \n",
        "        if corr == True:\n",
        "          print(\"Top 5 positive correlated w/ featuers \\n{}\".format(\n",
        "              df_corr[(df_corr[\"Feature 1\"]==col) & (df_corr[\"Feature 2\"]!=col)].head()))\n",
        "          print(\"Top 5 negative correlated w/ featuers \\n{}\".format(\n",
        "              df_corr[(df_corr[\"Feature 1\"]==col) & (df_corr[\"Feature 2\"]!=col)].tail()))\n",
        "        if info == True:\n",
        "\n",
        "          print(\"max: {}, min: {}, mean: {}, std: {}, median: {}\".format(\n",
        "              df[col].max(),df[col].min(),\n",
        "              round(df[col].mean(),3),round(df[col].std(),3),\n",
        "              df[col].median()))\n",
        "          print(f\"Quantiles | {col}\\n\",df[col].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))\n",
        "          print(\"No unique: {}, No null: {}\".format(df[col].nunique(),df[col].isnull().sum()))\n",
        "          if df[col].nunique() < 10 :\n",
        "            print(f\"Value counts in {col} \\n\",format(df[col].value_counts()))\n",
        "\n",
        "        # plot  \n",
        "        plt.subplot(2, 2, 1)\n",
        "        if target == None:\n",
        "            fig = sns.boxplot(col, whis=1.5, data=df)\n",
        "            fig.legend()\n",
        "        else:\n",
        "            fig = sns.boxplot(x=target, y=col, whis=1.5, data=df)\n",
        "            fig.legend()\n",
        "        # which defined as the proportion of the IQR past the low and high quartiles to extend the plot whiskers \n",
        "        # or interquartile range (IQR)\n",
        "        # therefore, maximum = Q3 + 1.5*IQR , min = Q1 - 1.5*IQR\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1    \n",
        "        \n",
        "        plt.subplot(2, 2, 2)\n",
        "        fig = sns.distplot(df[col].dropna())#.hist(bins=20)\n",
        "        fig.set_ylabel('Volumn')\n",
        "        fig.set_xlabel(col)      \n",
        "        \n",
        "        print('No data out of range between Q1 : {} and Q3 : {} = {}' .format(Q1,Q3,((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()))\n",
        "\n",
        "        if drop_3Qplot==True:          \n",
        "          plt.subplot(2, 2, 3)\n",
        "          tmp = df[(df[col] >= Q1)&(df[col] <= Q3)]#[col]\n",
        "          if target == None:\n",
        "              fig = sns.boxplot(col, whis=1.5, data=tmp)\n",
        "              fig.legend()\n",
        "          else:\n",
        "              fig = sns.boxplot(x=target, y=col, whis=1.5, data=tmp)\n",
        "              fig.legend()\n",
        "          \n",
        "          plt.subplot(2, 2, 4)\n",
        "          fig = sns.distplot(tmp[col])#.dropna())#.hist(bins=20)\n",
        "          fig.set_ylabel('Volumn')\n",
        "          fig.set_xlabel(col)   \n",
        "\n",
        "        plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72WTuvLn_aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_categorical(cols, target, df):\n",
        "  for col in cols:\n",
        "    if target == None:\n",
        "      sns.countplot(x=col, data=df[[col]] ,  palette=\"Reds_d\")      \n",
        "    else:\n",
        "      print('Column name: %s' %col)\n",
        "      sns.countplot(x=col, hue=target, data=df[[col,target]], palette=\"Reds_d\")\n",
        "      #sns.barplot(x=col, y=target, data=df[[col,target]]\n",
        "      #        , palette=\"Reds_d\", estimator = sum)\n",
        "      plt.legend()\n",
        "    plt.xticks(rotation=90)#-60    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNRYnCkWHyeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arothx_v_lC-",
        "colab_type": "text"
      },
      "source": [
        "# 2.Cleaning Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9dkPpmLmkDE",
        "colab_type": "text"
      },
      "source": [
        "## Outliers\n",
        "\n",
        "***Check outliers by plotting***\n",
        "\n",
        "[estimation cdf - plot](https://www.jddata22.com/home//plotting-an-empirical-cdf-in-python)\n",
        "  - check outlier by std and mean\n",
        "  \n",
        "[boxplot & quanlite](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n",
        "  - check outlier by \n",
        "    - IQR: interquantile range = Q3 - Q1\n",
        "    - “maximum”: Q3 + 1.5*IQR\n",
        "    - “minimum”: Q1 -1.5*IQR\n",
        "\n",
        "**Source**\n",
        "- [Three ways to detect outliers](http://colingorrie.github.io/outlier-detection.html)\n",
        "- [How to Use Statistics to Identify Outliers in Data](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)\n",
        "- [5 Ways to Find Outliers in Your Data](https://statisticsbyjim.com/basics/outliers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DReevHMvb0Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CalcOutliers(df_num): \n",
        "    '''\n",
        "    Find Outliers for continous cols\n",
        "    Tìm outliner dựa trên std, và mean\n",
        "\n",
        "    Sau khi phân tích quantile và mean, std\n",
        "    có thể biết outlier nằm ngoài vùng nào:\n",
        "    vd: cut = std * 3\n",
        "    '''\n",
        "    # calculating mean and std of the array\n",
        "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
        "\n",
        "    # seting the cut line to both higher and lower values\n",
        "    # You can change this value\n",
        "    cut = data_std * 3\n",
        "\n",
        "    #Calculating the higher and lower cut values\n",
        "    lower, upper = data_mean - cut, data_mean + cut\n",
        "\n",
        "    # creating an array of lower, higher and total outlier values \n",
        "    outliers_lower = [x for x in df_num if x < lower]\n",
        "    outliers_higher = [x for x in df_num if x > upper]\n",
        "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
        "\n",
        "    # array without outlier values\n",
        "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
        "    \n",
        "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
        "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
        "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
        "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
        "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAwK67oVoNiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Find Outliers for continous cols\n",
        "Tìm outliner dựa trên std, và mean\n",
        "\n",
        "Sau khi phân tích quantile và mean, std\n",
        "có thể biết outlier nằm ngoài vùng nào:\n",
        "vd: cut = std * 3\n",
        "'''\n",
        "\n",
        "def CalcOutliers(df_num): \n",
        "\n",
        "    # calculating mean and std of the array\n",
        "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
        "\n",
        "    # seting the cut line to both higher and lower values\n",
        "    # You can change this value\n",
        "    cut = data_std * 3\n",
        "\n",
        "    #Calculating the higher and lower cut values\n",
        "    lower, upper = data_mean - cut, data_mean + cut\n",
        "\n",
        "    # creating an array of lower, higher and total outlier values \n",
        "    outliers_lower = [x for x in df_num if x < lower]\n",
        "    outliers_higher = [x for x in df_num if x > upper]\n",
        "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
        "\n",
        "    # array without outlier values\n",
        "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
        "    \n",
        "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
        "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
        "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
        "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
        "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAcrST9rMts",
        "colab_type": "text"
      },
      "source": [
        "## High correlation features\n",
        "Có thể xóa bỏ bớt các cột có high correlate với nhau vì: \n",
        "- các cột có ý nghĩa như nhau với model\n",
        "- có cơ hội học được từ các cột khác\n",
        "- **quan trọng trong việc train NN** vì giảm được khối lượng dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLNYZQmereKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop columns with correlation greater than 0.9\n",
        "corr_matrix = df.drop(columns=\"isMale\").corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "print(\"high correlation features\", to_drop)\n",
        "df.drop(columns=to_drop, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKC6Lt1breyK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdqQPJ9zuPAh",
        "colab_type": "text"
      },
      "source": [
        "# 3.Statistical Checking\n",
        "Các kỹ thuật khác trong việc kiểm tra về mặt thống kê\n",
        "\n",
        "Source:\n",
        "- [Statistical Learning oxh](https://ongxuanhong.wordpress.com/category/kien-thuc/statistical-inference/)\n",
        "- [Video về Statistic](https://www.youtube.com/user/drnguyenvtuan/videos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foaLMVH2wNDH",
        "colab_type": "text"
      },
      "source": [
        "## Bayesian Statistic\n",
        "- [How Bayesian statistics convinced me to hit the gym](https://towardsdatascience.com/how-bayesian-statistics-convinced-me-to-hit-the-gym-fa737b0a7ac)\n",
        "- [Think you need to learn Bayesian Analysis? Read this first](https://peadarcoyle.com/2019/01/01/think-you-need-to-learn-bayesian-analysis-read-this-first/?fbclid=IwAR1KGEuQNfzzf0BexNampJsyZ3zNgcfzDOgNMsPvppjElAu4qOOPF8AwC7g)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVOwEo1KkGms",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 4.Before fitting model\n",
        "\n",
        "- Used in:\n",
        "  - Kaggle:[ Fraud Detection](https://www.kaggle.com/sonannguyenngoc/fraud-pca/edit/run/18581236) \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSzLvImTnBdI",
        "colab_type": "text"
      },
      "source": [
        "## Missing value \n",
        "- fill na : https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkZHnkG_nD1l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count null or nan\n",
        "df['Count_miss'] = df[col].isnull().sum(axis=1)\n",
        "\n",
        "df['nulls1'] = df.isna().sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTFgOYx5g8S9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Tìm các bất ổn về missing value và các cột có bật ổn về số lượng giá trị\n",
        "'''\n",
        "many_null_cols = [col for col in df_train.columns if df_train[col].isnull().sum() / df_train.shape[0] > 0.9]\n",
        "many_null_cols_test = [col for col in df_test.columns if df_test[col].isnull().sum() / df_test.shape[0] > 0.9]\n",
        "\n",
        "\n",
        "big_top_value_cols = ([col for col in train.columns if \n",
        "                       df_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9])\n",
        "big_top_value_cols_test = ([col for col in test.columns if \n",
        "                            df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9])\n",
        "one_value_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
        "one_value_cols_test = [col for col in df_test.columns if df_test[col].nunique() <= 1]\n",
        "one_value_cols == one_value_cols_test\n",
        "\n",
        "cols_to_drop = list(set(many_null_cols + many_null_cols_test + \n",
        "                        big_top_value_cols + big_top_value_cols_test + \n",
        "                        one_value_cols + one_value_cols_test))\n",
        "cols_to_drop.remove('isFraud')\n",
        "len(cols_to_drop)\n",
        "\n",
        "df = df.drop(cols_to_drop, axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v9q8-S-nEBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tìm na/nan/null trong từng cột\n",
        "# Hiện thị: Số na, %na, type\n",
        "# GPreda, missing data\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum()\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
        "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    types = []\n",
        "    for col in data.columns:\n",
        "        dtype = str(data[col].dtype)\n",
        "        types.append(dtype)\n",
        "    tt['Types'] = types\n",
        "    return(np.transpose(tt))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjWMnkr_AO1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "1 số cách fill NA\n",
        "interpolate from categorical features\n",
        "'''\n",
        "def df_processing(df):\n",
        "    df = df[df['MEMBER_GENDER'].notnull()]\n",
        "    df['MEMBER_OCCUPATION_CD'] = df['MEMBER_OCCUPATION_CD'].fillna(7)\n",
        "    df['MEMBER_OCCUPATION_CD'] = df['MEMBER_OCCUPATION_CD'].replace({4:3,5:3,7:3,6:4})\n",
        "    df['MEMBER_MARITAL_STATUS'] = df['MEMBER_MARITAL_STATUS'].fillna('UNK')\n",
        "    df['MEMBER_MARITAL_STATUS'] = df['MEMBER_MARITAL_STATUS'].replace({'S':'S-W-D','W':'S-W-D','D':'S-W-D'})\n",
        "\n",
        "    df = df[df['PAYMENT_MODE'] != 'SINGLE-PREMIUM']\n",
        "\n",
        "    df = df[df['ANNUAL_FEES'] > 0]\n",
        "    df = df[df['ANNUAL_FEES'] < df['MEMBER_ANNUAL_INCOME']]\n",
        "    df = df[df['ANNUAL_FEES'] < 1000000]\n",
        "    df['MEMBER_ANNUAL_INCOME'] = df.groupby(['MEMBER_OCCUPATION_CD','MEMBER_MARITAL_STATUS','MEMBER_GENDER'])\n",
        "                                  \\ ['MEMBER_ANNUAL_INCOME'].apply(lambda x: x.fillna(x.median()))\n",
        "    df = df[df['MEMBER_ANNUAL_INCOME'] < 9000000]\n",
        "    \n",
        "    # special kids\n",
        "    df = df[df[\"MEMBER_AGE_AT_ISSUE\"] > 12]\n",
        "    \n",
        "    df[\"PAYMENT_MODE_NUM\"] = df[\"PAYMENT_MODE\"].replace({'ANNUAL':1,'SEMI-ANNUAL':1/2,\n",
        "                                                        'QUARTERLY':1/4,'MONTHLY':1/12\n",
        "                                                        })\n",
        "    \n",
        "    df['MEMBERSHIP_TERM_YEARS_BIN'] = df.apply(lambda df:mem_term_lab(df), axis = 1)\n",
        "    # age group\n",
        "    x = pd.qcut(df[\"MEMBER_AGE_AT_ISSUE\"].to_list(),q = 8)\n",
        "    x.categories = [\"age_g1\",\"age_g2\",\"age_g3\",\"age_g4\",\"age_g5\",\"age_g6\",\"age_g7\",\"age_g8\"]\n",
        "    df[\"AGE_GROUP\"] = x.to_list()\n",
        "    \n",
        "    # manipuation agent\n",
        "    aggent_code_churn = df.groupby(['AGENT_CODE']).agg({\n",
        "        \"ANNUAL_FEES\":'mean',\n",
        "        \"MEMBER_ANNUAL_INCOME\":'mean',\n",
        "    }).reset_index()\n",
        "    x = pd.qcut(aggent_code_churn[\"ANNUAL_FEES\"].to_list(),q = 6,duplicates='drop')\n",
        "    x.categories = [1,2,3,4,5]\n",
        "    aggent_code_churn[\"AGENT_RANK_FEES\"] = x.to_list()\n",
        "    x = pd.qcut(aggent_code_churn[\"MEMBER_ANNUAL_INCOME\"].to_list(),q = 6,duplicates='drop')\n",
        "    x.categories = [1,2,3,4,5,6]\n",
        "    aggent_code_churn[\"AGENT_RANK_INCOMES\"] = x.to_list()\n",
        "    aggent_code_churn.drop(columns= [\"ANNUAL_FEES\",\"MEMBER_ANNUAL_INCOME\"], inplace=True)\n",
        "\n",
        "    df = pd.merge(df,aggent_code_churn, on = \"AGENT_CODE\")\n",
        "    \n",
        "    return df\n",
        "df = df_processing(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFvPys_F9KB",
        "colab_type": "text"
      },
      "source": [
        "## Reduce memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUyadfjFF_kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcX1fpPrrg-_",
        "colab_type": "text"
      },
      "source": [
        "## Transformation Scaling/Standardize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_a8z7m7uec_",
        "colab_type": "text"
      },
      "source": [
        "## Reduce dimension checking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDc3N94Mj8K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scaling only on tranform data\n",
        "df_V = df_V[c_V_trans] \n",
        "df_V_test = df_V_test[c_V_trans]\n",
        "\n",
        "#standardize scaled\n",
        "scaler = StandardScaler().fit(df_V)\n",
        "df_V_scaled = scaler.transform(df_V)\n",
        "print('Scaled mean df_V',df_V_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_scaled[:,0].std()) \n",
        "scaler = StandardScaler().fit(df_V_test)\n",
        "df_V_test_scaled = scaler.transform(df_V_test)\n",
        "print('Scaled mean df_V',df_V_test_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_test_scaled[:,0].std()) \n",
        "\n",
        "# plot cumulative explained variance\n",
        "# pca = PCA().fit(df_V_scaled)\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# #plt.xlim(0,7,1)\n",
        "# plt.xlabel('Number of components')\n",
        "# plt.ylabel('Cumulative explained variance')\n",
        "\n",
        "\n",
        "# Setup Principal component analysis\n",
        "pca = PCA(n_components=125) \n",
        "#pca = PCA(n_components=0.96)  #v2: n_components=0.95\n",
        "df_V_pca = pca.fit_transform(df_V_scaled)\n",
        "np.save('df_V_pca_v3.npy',df_V_pca)\n",
        "#df_V_pca.to_csv('df_V_pca', sep='\\t')\n",
        "df_V_test_pca = pca.fit_transform(df_V_test_scaled)\n",
        "np.save('df_V_test_pca_v3.npy',df_V_test_pca)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljSftUKgvtyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import io\n",
        "import plotly.offline as py#visualization\n",
        "py.init_notebook_mode(connected=True)#visualization\n",
        "import plotly.graph_objs as go#visualization\n",
        "import plotly.tools as tls#visualization\n",
        "import plotly.figure_factory as ff#visualization\n",
        "\n",
        "'''\n",
        "Scatter plot giữa 3 cột bất kỳ trong dữ liệu, được hue = TARGET\n",
        "cho cái nhìn về dữ liệu, xem liệu nó có khả năng phân tách không\n",
        "'''\n",
        "\n",
        "trace1 = go.Scatter3d(x = churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = churn[\"ANNUAL_FEES\"],\n",
        "                      z = churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      mode = \"markers\",\n",
        "                      name = \"Churn customers\",\n",
        "                      text = \"Id : \" + churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      marker = dict(size = 1,color = \"red\")\n",
        "                     )\n",
        "trace2 = go.Scatter3d(x = not_churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = not_churn[\"ANNUAL_FEES\"],\n",
        "                      z = not_churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      name = \"Non churn customers\",\n",
        "                      text = \"Id : \" + not_churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      mode = \"markers\",\n",
        "                      marker = dict(size = 1,color= \"green\")\n",
        "                     )\n",
        "\n",
        "layout = go.Layout(dict(title = \"Monthly charges,total charges & tenure in customer attrition\",\n",
        "                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n",
        "                                                   center=dict(x=0, y=0, z=0),\n",
        "                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n",
        "                                     xaxis  = dict(title = \"annual incomes\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'),\n",
        "                                     yaxis  = dict(title = \"annual fees\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  ),\n",
        "                                     zaxis  = dict(title = \"term years\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  )\n",
        "                                    ),\n",
        "                        height = 700,\n",
        "                       )\n",
        "                  )\n",
        "                  \n",
        "\n",
        "data = [trace1,trace2]\n",
        "fig  = go.Figure(data = data,layout = layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQI7G83cDx2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "X = tmp[[i for i in tmp.columns if i not in Id_col + target_col]]\n",
        "Y = tmp[target_col + Id_col]\n",
        "\n",
        "principal_components = pca.fit_transform(X)\n",
        "pca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\n",
        "pca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\n",
        "pca_data[\"CHURN\"] = pca_data[\"CHURN\"].replace({1:\"CANCELLED\",0:\"INFORCE\"})\n",
        "\n",
        "def pca_scatter(target,color) :\n",
        "    tracer = go.Scatter(x = pca_data[pca_data[\"CHURN\"] == target][\"PC1\"] ,\n",
        "                        y = pca_data[pca_data[\"CHURN\"] == target][\"PC2\"],\n",
        "                        name = target,mode = \"markers\",\n",
        "                        marker = dict(color = color,\n",
        "                                      line = dict(width = .5),\n",
        "                                      symbol =  \"diamond-open\"),\n",
        "                        text = (\"Customer Id : \" + \n",
        "                                pca_data[pca_data[\"CHURN\"] == target]['MEMBERSHIP_NUMBER'])\n",
        "                       )\n",
        "    return tracer\n",
        "\n",
        "layout = go.Layout(dict(title = \"Visualising data with principal components\",\n",
        "                        plot_bgcolor  = \"rgb(243,243,243)\",\n",
        "                        paper_bgcolor = \"rgb(243,243,243)\",\n",
        "                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 1\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 2\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        height = 600\n",
        "                       )\n",
        "                  )\n",
        "trace1 = pca_scatter(\"CANCELLED\",'red')\n",
        "trace2 = pca_scatter(\"INFORCE\",'royalblue')\n",
        "data = [trace2,trace1]\n",
        "fig = go.Figure(data=data,layout=layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJUeNk50VTOP",
        "colab_type": "text"
      },
      "source": [
        "# New\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "--0q9U8aV58T"
      },
      "source": [
        "# New\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6pKlx2ezV6F5"
      },
      "source": [
        "# New\n",
        "\n"
      ]
    }
  ]
}