{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.PreProcessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/1.PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3188bu2pVCw2",
        "colab_type": "toc"
      },
      "source": [
        ">>[Preprocessing](#scrollTo=-2COeSc5ml1_)\n",
        "\n",
        ">>[Transformation Scaling/Standardize](#scrollTo=PcX1fpPrrg-_)\n",
        "\n",
        ">>[Encoding](#scrollTo=qeLaVzFW0Se6)\n",
        "\n",
        ">>>[Onehot Encode](#scrollTo=HULldo6Zm6l1)\n",
        "\n",
        ">>>[Label Encode](#scrollTo=Hz-vTHED0Vbc)\n",
        "\n",
        ">>>[Reduce memory](#scrollTo=WzFvPys_F9KB)\n",
        "\n",
        ">>>[Reduce dimensional checking](#scrollTo=a_a8z7m7uec_)\n",
        "\n",
        ">>[Feature Generation](#scrollTo=ffkEEAsbC4EW)\n",
        "\n",
        ">>[Feature Selection](#scrollTo=LfdUhklbDAQa)\n",
        "\n",
        ">>>[High correlation](#scrollTo=k9prSardKWXm)\n",
        "\n",
        ">>>[WOE & IV](#scrollTo=t-8cO9PJ3ryk)\n",
        "\n",
        ">>>[RFE](#scrollTo=cmnU0NRff_ck)\n",
        "\n",
        ">>[Sampling](#scrollTo=JfMtLkkO7fxr)\n",
        "\n",
        ">>>[Under-sampling](#scrollTo=5mzGc0BE7xWq)\n",
        "\n",
        ">>>[Over-sampling](#scrollTo=4il7y5CI8O8D)\n",
        "\n",
        ">>>>[RandomOverSampler](#scrollTo=2ioqMRx98Syz)\n",
        "\n",
        ">>>>[SMOTE](#scrollTo=8A8WLw5B8dnc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2COeSc5ml1_",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcX1fpPrrg-_",
        "colab_type": "text"
      },
      "source": [
        "## Transformation Scaling/Standardize\n",
        "News:\n",
        "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
        "\n",
        "\n",
        "Căn bản:\n",
        "- [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n",
        "![alt text](https://miro.medium.com/max/4800/1*z-C9ANBC4rjsk-ZK4wzijg.png)\n",
        "- [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
        "\n",
        "\n",
        "- Đối với từng model nên xét thêm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn4UO8yW5hMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWrridnMkIwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "nottrans_num_col = ['AGSegCode', 'MAPASegCode',\n",
        "                    'Leader_AGSegCode', 'Leader_MAPASegCode']\n",
        "                    \n",
        "scale_col = [c for c in WOE_numeric_col if c not in nottrans_num_col]\n",
        "print('scale cols' ,scale_col)\n",
        "robust_scale = RobustScaler().fit(round(df_WOE[scale_col],2))\n",
        "df_WOE[scale_col] = robust_scale.transform(df_WOE[scale_col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeLaVzFW0Se6",
        "colab_type": "text"
      },
      "source": [
        "## Encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HULldo6Zm6l1",
        "colab_type": "text"
      },
      "source": [
        "### Onehot Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfS1ArWkI61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummies_col = []\n",
        "for c in ['Leader_RegionCode', 'RegionCode', 'ContactProvince_Code']:\n",
        "    print('> Get dummies with prefix {}'.format(c+'_'))\n",
        "    df_dummies = pd.get_dummies(df[c], prefix = c ) \n",
        "    dummies_col = dummies_col +(df_dummies.columns.to_list())\n",
        "    df = pd.concat( [df, df_dummies] , axis = 1)\n",
        "    \n",
        "print('dummies col: ',dummies_col)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz-vTHED0Vbc",
        "colab_type": "text"
      },
      "source": [
        "### Label Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOaNKw9Q0WS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.replace({\n",
        "             'Leader_AGLevel' : {'PM':1,'UM':2,'BM':3}\n",
        "           })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3mZzSk0Waf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in ['Leader_RegionCode', 'RegionCode', 'ContactProvince_Code','EducationCode']:\n",
        "    print('> ',col)\n",
        "    df[col] = df[col].astype('category').cat.codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4OLaR35bbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFvPys_F9KB",
        "colab_type": "text"
      },
      "source": [
        "### Reduce memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUyadfjFF_kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgZr6swygYbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvTRFClqgYjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_a8z7m7uec_",
        "colab_type": "text"
      },
      "source": [
        "### Reduce dimensional checking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDc3N94Mj8K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scaling only on tranform data\n",
        "df_V = df_V[c_V_trans] \n",
        "df_V_test = df_V_test[c_V_trans]\n",
        "\n",
        "#standardize scaled\n",
        "scaler = StandardScaler().fit(df_V)\n",
        "df_V_scaled = scaler.transform(df_V)\n",
        "print('Scaled mean df_V',df_V_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_scaled[:,0].std()) \n",
        "scaler = StandardScaler().fit(df_V_test)\n",
        "df_V_test_scaled = scaler.transform(df_V_test)\n",
        "print('Scaled mean df_V',df_V_test_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_test_scaled[:,0].std()) \n",
        "\n",
        "# plot cumulative explained variance\n",
        "# pca = PCA().fit(df_V_scaled)\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# #plt.xlim(0,7,1)\n",
        "# plt.xlabel('Number of components')\n",
        "# plt.ylabel('Cumulative explained variance')\n",
        "\n",
        "\n",
        "# Setup Principal component analysis\n",
        "pca = PCA(n_components=125) \n",
        "#pca = PCA(n_components=0.96)  #v2: n_components=0.95\n",
        "df_V_pca = pca.fit_transform(df_V_scaled)\n",
        "np.save('df_V_pca_v3.npy',df_V_pca)\n",
        "#df_V_pca.to_csv('df_V_pca', sep='\\t')\n",
        "df_V_test_pca = pca.fit_transform(df_V_test_scaled)\n",
        "np.save('df_V_test_pca_v3.npy',df_V_test_pca)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljSftUKgvtyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import io\n",
        "import plotly.offline as py#visualization\n",
        "py.init_notebook_mode(connected=True)#visualization\n",
        "import plotly.graph_objs as go#visualization\n",
        "import plotly.tools as tls#visualization\n",
        "import plotly.figure_factory as ff#visualization\n",
        "\n",
        "'''\n",
        "Scatter plot giữa 3 cột bất kỳ trong dữ liệu, được hue = TARGET\n",
        "cho cái nhìn về dữ liệu, xem liệu nó có khả năng phân tách không\n",
        "'''\n",
        "\n",
        "trace1 = go.Scatter3d(x = churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = churn[\"ANNUAL_FEES\"],\n",
        "                      z = churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      mode = \"markers\",\n",
        "                      name = \"Churn customers\",\n",
        "                      text = \"Id : \" + churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      marker = dict(size = 1,color = \"red\")\n",
        "                     )\n",
        "trace2 = go.Scatter3d(x = not_churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = not_churn[\"ANNUAL_FEES\"],\n",
        "                      z = not_churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      name = \"Non churn customers\",\n",
        "                      text = \"Id : \" + not_churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      mode = \"markers\",\n",
        "                      marker = dict(size = 1,color= \"green\")\n",
        "                     )\n",
        "\n",
        "layout = go.Layout(dict(title = \"Monthly charges,total charges & tenure in customer attrition\",\n",
        "                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n",
        "                                                   center=dict(x=0, y=0, z=0),\n",
        "                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n",
        "                                     xaxis  = dict(title = \"annual incomes\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'),\n",
        "                                     yaxis  = dict(title = \"annual fees\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  ),\n",
        "                                     zaxis  = dict(title = \"term years\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  )\n",
        "                                    ),\n",
        "                        height = 700,\n",
        "                       )\n",
        "                  )\n",
        "                  \n",
        "\n",
        "data = [trace1,trace2]\n",
        "fig  = go.Figure(data = data,layout = layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQI7G83cDx2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "X = tmp[[i for i in tmp.columns if i not in Id_col + target_col]]\n",
        "Y = tmp[target_col + Id_col]\n",
        "\n",
        "principal_components = pca.fit_transform(X)\n",
        "pca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\n",
        "pca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\n",
        "pca_data[\"CHURN\"] = pca_data[\"CHURN\"].replace({1:\"CANCELLED\",0:\"INFORCE\"})\n",
        "\n",
        "def pca_scatter(target,color) :\n",
        "    tracer = go.Scatter(x = pca_data[pca_data[\"CHURN\"] == target][\"PC1\"] ,\n",
        "                        y = pca_data[pca_data[\"CHURN\"] == target][\"PC2\"],\n",
        "                        name = target,mode = \"markers\",\n",
        "                        marker = dict(color = color,\n",
        "                                      line = dict(width = .5),\n",
        "                                      symbol =  \"diamond-open\"),\n",
        "                        text = (\"Customer Id : \" + \n",
        "                                pca_data[pca_data[\"CHURN\"] == target]['MEMBERSHIP_NUMBER'])\n",
        "                       )\n",
        "    return tracer\n",
        "\n",
        "layout = go.Layout(dict(title = \"Visualising data with principal components\",\n",
        "                        plot_bgcolor  = \"rgb(243,243,243)\",\n",
        "                        paper_bgcolor = \"rgb(243,243,243)\",\n",
        "                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 1\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 2\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        height = 600\n",
        "                       )\n",
        "                  )\n",
        "trace1 = pca_scatter(\"CANCELLED\",'red')\n",
        "trace2 = pca_scatter(\"INFORCE\",'royalblue')\n",
        "data = [trace2,trace1]\n",
        "fig = go.Figure(data=data,layout=layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZwNeiLgSMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffkEEAsbC4EW",
        "colab_type": "text"
      },
      "source": [
        "## Feature Generation\n",
        "\n",
        "[Simple FE](https://machinelearningcoban.com/general/2017/02/06/featureengineering/)\n",
        "-    Trực tiếp lấy raw data\n",
        "-    Bag-of-words\n",
        "  -    Bag-of-Words trong Computer Vision\n",
        "-    Feature Scaling and Normalization\n",
        "  -        Rescaling\n",
        "  -        Standardization\n",
        "  -        Scaling to unit length\n",
        "\n",
        "  - Feature Generation: ở bước này tập trung các kĩ thuật để tạo ra feature: xử lí nlp, image, binning, scaling, grouping, aggregate.\n",
        "  - Kết hợp với Modelling để tìm ra nhóm feature tốt và tập trung mạnh vào đó\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfdUhklbDAQa",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JiEiWrFdiZp",
        "colab_type": "text"
      },
      "source": [
        "Research:\n",
        "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
        "- [2](https://www.kaggle.com/sz8416/6-ways-for-feature-selection) example selectkbes-RFE in kaggle\n",
        "- [3](https://www.kaggle.com/dkim1992/feature-selection-ranking) chua doc\n",
        "- [4](https://machinelearningmastery.com/feature-selection-machine-learning-python/) tong quat\n",
        "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "- [Cách tìm feature bằng cách chạy các linear reg](https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/) chua doc\n",
        "- [6. Blog tổng hợp các method](https://mlwhiz.com/blog/2019/08/07/feature_selection/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb6TIKC408SK",
        "colab_type": "text"
      },
      "source": [
        "- Feature Selection: ở bước này tập trung các kĩ thuật chọn feature để tốt ưu mô hình\n",
        "  - Chọn được feature tốt và tối ưu được bộ nhớ\n",
        "  - Chọn được model parameters tốt nhất phù hợp với bộ features tốt nhất\n",
        "  - Kết hợp với các kĩ thuật chia dataset (CV, leave-one)\n",
        "\n",
        "- Tiêu chí đánh giá **metric of feature**\n",
        "  - AUC: thể hiện khả năng dự đoán\n",
        "  - Correlation: kiểm tra độ tương quan giữa feature với target hoặc với các feature quan trọng khác\n",
        "  - Converage: kiểm tra null, null nhiều thì ít thông tin\n",
        "  - Weighted of evidence, and information value:\n",
        "    - $\\ln \\left(\\frac{P(\\text {Good})}{P(\\text {Bad})}\\right)$\n",
        "    - \\begin{array}{l}{\\text { Information value }} \\\\ {\\qquad \\sum(P(G o o d)-P(B a d)) * \\ln \\left(\\frac{P(G o d)}{P(B a d)}\\right)}\\end{array}\n",
        "\n",
        "\n",
        "\n",
        "[Brute Force Approach](https://www.kdnuggets.com/2017/11/rapidminer-basic-concepts-feature-selection.html)\n",
        "- Cách tiếp cận bằng các chạy thử thất cả các feature combination và so sánh trên các metrics of feature\n",
        "\n",
        "[Feature Selection]\n",
        "- Forwarding: ta bắt đầu với tập feature rỗng, sau đó lần lượt add thêm feature vào tập này. Nếu thấy performance của model tăng ta sẽ tiếp tục quá trình này, ngược lại sẽ dừng lại.\n",
        "- Backwarding: ta bắt đầu với toàn bộ tập feature, sau đó lần lượt remove từng feature khỏi tập này. Nếu thấy performance của model tăng hoặc giảm không quá nhiều, ta sẽ tiếp tục quá trình này, ngược lại nếu performance bị drop quá mạnh sẽ dừng lại.\n",
        "- Hybridge: kết hợp cả 2 hướng trên\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9prSardKWXm",
        "colab_type": "text"
      },
      "source": [
        "### High correlation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IAcrST9rMts",
        "colab_type": "text"
      },
      "source": [
        "**High correlation features**\n",
        "Có thể xóa bỏ bớt các cột có high correlate với nhau vì: \n",
        "- các cột có ý nghĩa như nhau với model\n",
        "- có cơ hội học được từ các cột khác\n",
        "- **quan trọng trong việc train NN** vì giảm được khối lượng dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLNYZQmereKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop columns with correlation greater than 0.9\n",
        "corr_matrix = df.drop(columns=\"isMale\").corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "print(\"high correlation features\", to_drop)\n",
        "df.drop(columns=to_drop, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuorNYw7KV4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TESTING FOR MULTICOLLINEARITY:\n",
        "from patsy import dmatrices\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "#RUNNING FOR MULTIPLE REGRESSION:\n",
        "# %%capture\n",
        "#gather features\n",
        "vif_test = df_train\n",
        "vif_test = vif_test.drop('new_Active_Net.1', axis = 1)\n",
        "to_drop = ['new_Leader_ActNet_ratio_6m','new_Leader_RegionCode','new_Leader_APE_ratio_3m','new_NewRe_APE_ratio_3m','new_CaseNet_SumLast3m','new_Leader_APE_ratio_6m','new_NewRe_Manpower_SumLas3p','new_Leader_Case_ratio_3m','new_AG_Case_ratio_3m','new_NewRe_Case_Net_SumLas3p']\n",
        "vif_test = vif_test.drop(to_drop,axis =1)\n",
        "features = \"+\".join(vif_test.drop('label_1',axis=1).columns)\n",
        "# get y and X dataframes based on this regression:\n",
        "y, X = dmatrices('label_1 ~' + features, df_train, return_type='dataframe')\n",
        "vif = pd.DataFrame()\n",
        "range = np.arange(0,X.shape[1],1)\n",
        "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range]\n",
        "vif[\"features\"] = X.columns\n",
        "\n",
        "col_vif = vif.drop(0).features\n",
        "df_train = df_train.drop(to_drop,axis =1)\n",
        "df_test = df_test.drop(to_drop,axis =1)\n",
        "vif.sort_values('VIF Factor',ascending = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-8cO9PJ3ryk",
        "colab_type": "text"
      },
      "source": [
        "### WOE & IV\n",
        "- https://www.one-tab.com/page/0V6bYIX4ShSQmuW4bpD7Zg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avH4UCL53u9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import re\n",
        "import traceback\n",
        "import string\n",
        "\n",
        "max_bin = 20\n",
        "force_bin = 3\n",
        "\n",
        "# define a binning function\n",
        "def mono_bin(Y, X, n = max_bin):\n",
        "    \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    r = 0\n",
        "    while np.abs(r) < 1:\n",
        "        try:\n",
        "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "            d2 = d1.groupby('Bucket', as_index=True)\n",
        "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "            n = n - 1 \n",
        "        except Exception as e:\n",
        "            n = n - 1\n",
        "\n",
        "    if len(d2) == 1:\n",
        "        n = force_bin         \n",
        "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "        if len(np.unique(bins)) == 2:\n",
        "            bins = np.insert(bins, 0, 1)\n",
        "            bins[1] = bins[1]-(bins[1]/2)\n",
        "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
        "        d2 = d1.groupby('Bucket', as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"MIN_VALUE\"] = d2.min().X\n",
        "    d3[\"MAX_VALUE\"] = d2.max().X\n",
        "    d3[\"COUNT\"] = d2.count().Y\n",
        "    d3[\"EVENT\"] = d2.sum().Y\n",
        "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "    d3=d3.reset_index(drop=True)\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def char_bin(Y, X):\n",
        "        \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
        "    df2 = notmiss.groupby('X',as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"COUNT\"] = df2.count().Y\n",
        "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "    d3[\"EVENT\"] = df2.sum().Y\n",
        "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    d3 = d3.reset_index(drop=True)\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def data_vars(df1, target):\n",
        "    \n",
        "    stack = traceback.extract_stack()\n",
        "    filename, lineno, function_name, code = stack[-2]\n",
        "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "    \n",
        "    x = df1.dtypes.index\n",
        "    count = -1\n",
        "    \n",
        "    for i in x:\n",
        "        if i.upper() not in (final.upper()):\n",
        "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "                conv = mono_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "            else:\n",
        "                conv = char_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i            \n",
        "                count = count + 1\n",
        "                \n",
        "            if count == 0:\n",
        "                iv_df = conv\n",
        "            else:\n",
        "                iv_df = iv_df.append(conv,ignore_index=True)\n",
        "    \n",
        "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "    iv = iv.reset_index()\n",
        "    return(iv_df,iv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KQqwlOh02BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tqdm import tqdm\n",
        "woe = pd.DataFrame()\n",
        "for col in WOE_cat_col:\n",
        "    l = []\n",
        "    t_final_iv = final_iv[final_iv.VAR_NAME == col]\n",
        "    print('> ',col)\n",
        "    print(' no of bins: ',t_final_iv.shape[0])\n",
        "    if t_final_iv.shape[0] <= 10:\n",
        "        print(t_final_iv[['VAR_NAME','MIN_VALUE']])\n",
        "    else:\n",
        "        print(t_final_iv.sample(5)[['VAR_NAME','MIN_VALUE']])\n",
        "    print('\\n')\n",
        "    for x in df_WOE[col]:\n",
        "        l.append(t_final_iv[t_final_iv.MIN_VALUE == x].WOE.values[0])\n",
        "    woe[col+'_woe'] = l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQp7xf0C5Rp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in WOE_numeric_col:\n",
        "    l = []\n",
        "    t_final_iv = final_iv[final_iv.VAR_NAME == col]\n",
        "    print('> ',col)\n",
        "    print(' no of bins: ',t_final_iv.shape[0])\n",
        "    if t_final_iv.shape[0] <= 10:\n",
        "        print(t_final_iv[['VAR_NAME','MIN_VALUE','MAX_VALUE']])\n",
        "    else:\n",
        "        print(t_final_iv.sample(5)[['VAR_NAME','MIN_VALUE','MAX_VALUE']])\n",
        "    print('\\n')\n",
        "    for x in df_WOE[col]:\n",
        "        l.append(t_final_iv[(t_final_iv.MIN_VALUE <= x)& (t_final_iv.MAX_VALUE >= x)].WOE.values[0])\n",
        "    woe[col+'_woe'] = l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oGlijJm5RxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmnU0NRff_ck",
        "colab_type": "text"
      },
      "source": [
        "### **RFE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7hYFfmv7cbk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "[theory](https://www.kaggle.com/nroman/recursive-feature-elimination)\n",
        "\n",
        "Trong mini course này, tôi sẽ áp dụng hướng “backwarding”. Các bước thực hiện như sau:\n",
        "\n",
        "- Đặt: n là số lần lặp feature selection, k là số feature sẽ drop ở mỗi lần lặp, p là AUC sau mỗi lần train\n",
        "Train model với XGboost\n",
        "- Lấy kết quả feature important sắp xếp giảm dần và loại ra k feature có giá trị thấp nhất\n",
        "- Lưu lại performance hiện tại để so sánh với performance tiếp theo.\n",
        "- Nếu thấp hơn ngưỡng p sẽ dừng\n",
        "- Tiếp tục quá trình selection\n",
        "\n",
        "Tuỳ theo số lượng feature và cài đặt hyper-parameter của model thì thời gian sẽ nhanh chậm khác nhau.\n",
        "\n",
        "Additional:\n",
        "- Genetic algorithm for feature selection\n",
        "\n",
        "Source:\n",
        "- [1](https://ongxuanhong.wordpress.com/2019/04/17/data-science-mini-course/#more-15645)\n",
        "- [2](https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15) doc them\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UIowwhg2A3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "#lr_best = LogisticRegressionCV\n",
        "#lr_best = LogisticRegression(C=7.7, penalty='l2' ,class_weight='balanced')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2roeUhZt6FzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "# Create the RFE object and compute a cross-validated score.\n",
        "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
        "rfecv = RFECV(estimator=lr_best, step=1, cv=10, scoring='roc_auc')\n",
        "rfecv.fit(X_train, y_train)\n",
        "\n",
        "best_features = X_train.columns[rfecv.support_].tolist()\n",
        "\n",
        "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
        "print('Selected features: %s' % best_features)\n",
        "\n",
        "\n",
        "# Plot number of features VS. cross-validation scores\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
        "plt.plot(np.arange(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcTxoLSpcNt_",
        "colab_type": "text"
      },
      "source": [
        "### "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emnNq67vcM3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYU-SnthcM-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqatZvAQcNBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUr63QqacNFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfMtLkkO7fxr",
        "colab_type": "text"
      },
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mzGc0BE7xWq",
        "colab_type": "text"
      },
      "source": [
        "### Under-sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeblIals7lTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# under-sampling 7:3\n",
        "sample = train[train.label_2==1].shape[0]/3*7\n",
        "train_US73=pd.concat([train[train.label_2!=1].sample(int(sample)),train[train.label_2==1]], axis = 0, ignore_index=True)\n",
        "print('sample shape',train_US73.shape)\n",
        "\n",
        "# under-sampling 5:5\n",
        "# sample = train[train.label_2==1].shape[0]/5*5\n",
        "# train_US55=pd.concat([train[train.label_2!=1].sample(int(sample)),train[train.label_2==1]], axis = 0, ignore_index=True\n",
        "# print('sample shape',train_US73.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHICQmTi7nqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = train[dummies_col+numeric_col]\n",
        "X_train = train_US73[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_train = train_US73['label_2']\n",
        "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_test = test['label_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4il7y5CI8O8D",
        "colab_type": "text"
      },
      "source": [
        "### Over-sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ioqMRx98Syz",
        "colab_type": "text"
      },
      "source": [
        "#### RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49htNR8a7n1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.base import BaseSampler\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdEAQb4H8XfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = train[dummies_col+numeric_col]\n",
        "X_train = train[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_train = train['label_2']\n",
        "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_test = test['label_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI2Up4po8Xit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_smote = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(RandomOverSampler(sampling_strategy=1, random_state=0), model_smote)\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwL1Z1yO8Xlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_smote_predictions = pipe.predict_proba(X_test)\n",
        "model_smote_pred_label = pipe.predict(X_test) \n",
        "model_smote_roc_score = roc_auc_score( y_test, model_smote_predictions[:,1])\n",
        "model_smote_f1_score = f1_score(y_test, model_smote_pred_label)\n",
        "print('random forest roc score on test: ', model_smote_roc_score)\n",
        "print('random forest f1 score on test: ', model_smote_f1_score)\n",
        "\n",
        "confu_matrix = confusion_matrix(y_test, model_smote_pred_label) \n",
        "sns.heatmap(confu_matrix , annot=True, fmt='d')\n",
        "print(classification_report(y_test, model_smote_pred_label) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A8WLw5B8dnc",
        "colab_type": "text"
      },
      "source": [
        "#### SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSansvBy8fIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = train[dummies_col+numeric_col]\n",
        "X_train = train[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_train = train['label_2']\n",
        "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_test = test['label_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1aaJroW8fQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.base import BaseSampler\n",
        "\n",
        "rf_clf = LogisticRegression()\n",
        "\n",
        "smotes = {0 : 'SMOTE',\n",
        "          1 : 'BorderlineSMOTE',\n",
        "          2 : 'SVMSMOTE',\n",
        "          3 : 'ADASYN'}\n",
        "\n",
        "\n",
        "for i, sampler in enumerate((SMOTE(sampling_strategy = 1, random_state=0),\n",
        "                BorderlineSMOTE(sampling_strategy = 1, random_state=0, kind='borderline-1'),\n",
        "                SVMSMOTE(sampling_strategy = 1, random_state=0),\n",
        "                ADASYN(sampling_strategy = 1, random_state=0))):\n",
        "    pipe_line = make_pipeline(sampler, rf_clf)\n",
        "    pipe_line.fit(X_train, y_train)\n",
        "    rf_predictions = pipe_line.predict_proba(X_test)\n",
        "    rf_pred_label = pipe_line.predict(X_test) \n",
        "    rf_roc_score = roc_auc_score(y_test, rf_predictions[:,1])\n",
        "    rf_f1_score = f1_score(y_test, rf_pred_label)\n",
        "    print('------------------------------------------------')\n",
        "    print('SMOTE method: ', smotes[i])\n",
        "    print('random forest roc score on test: ', rf_roc_score)\n",
        "    print('random forest f1 score on test: ', rf_f1_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzF3WgLN8fTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q46qxVdk8fWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}