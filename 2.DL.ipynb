{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.DL.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dRSO8KKkHbxA","colab_type":"text"},"source":["# Theory"]},{"cell_type":"markdown","metadata":{"id":"2dVNXFhf2_B7","colab_type":"text"},"source":["## Multi-Perceptron\n","[softmax,cross-entropy](https://machinelearningcoban.com/2017/02/17/softmax/#-cross-entropy)\n","\n","* NN - activation = logistic regression\n","\n","\n","* NN - activation = softmax regresion\n","\n","\n","\n","- [Perceptron](https://dlapplications.github.io/2018-06-11-perceptron/)\n","- [Multi-perceptron](https://dlapplications.github.io/2018-06-15-MLP/)"]},{"cell_type":"markdown","metadata":{"id":"dFUr3XT-2-4E","colab_type":"text"},"source":["## Feedforward & Backpropagation\n","\n","\n","\n","Source:\n","[1](https://machinelearningcoban.com/2017/02/24/mlp/),"]},{"cell_type":"code","metadata":{"id":"GYmdWybCIHKc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YlXZvJGGI_yM","colab_type":"text"},"source":["# Neural Network"]},{"cell_type":"markdown","metadata":{"id":"J1bwPIlsJDvE","colab_type":"text"},"source":["## Fully connected\n"]},{"cell_type":"markdown","metadata":{"id":"fhes7yDS-F5H","colab_type":"text"},"source":["## Convolution NN\n","\n","[understand](https://topdev.vn/blog/thuat-toan-cnn-convolutional-neural-network/)\n","\n","\n","---\n","\n","\n","\n","[Các cấu trúc phổ biến của mạng CNN](https://forum.machinelearningcoban.com/t/kien-truc-cac-mang-cnn-noi-tieng-phan-1-alex-lenet-inception-vgg/2582)\n","\n","[Qua trinh phat trien NN](https://dlapplications.github.io/2018-07-06-CNN/)\n","\n"," - LeNet\n"," -    AlexNet\n"," -    VGGNet\n"," -  GoogLeNet\n"," -   ResNet\n"," -   ZFNet"]},{"cell_type":"markdown","metadata":{"id":"JTYoH2vxCfM8","colab_type":"text"},"source":["# Validation\n","\n","## Overfitting\n","- [dropout and noise layer](https://medium.com/@maksutov.rn/deep-study-of-a-not-very-deep-neural-network-part-5-dropout-and-noise-29d980ece933)"]},{"cell_type":"code","metadata":{"id":"Kyj4umHkdAg8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y_91auMEHhV6","colab_type":"text"},"source":["# Code - Tf/Keras"]},{"cell_type":"markdown","metadata":{"id":"xrrSMqQT2_Ed","colab_type":"text"},"source":["## Active function\n","- [ML-Regression](https://colab.research.google.com/drive/15RAbgNf-2Lkv_TOjgYtDafWvy3vASbzy#scrollTo=IDHLmr9DPmak)\n","- https://aivietnam.ai/courses/aisummer2019/lessons/ham-activations/\n","- [Tf-keras activation](https://www.tensorflow.org/api_docs/python/tf/keras/activations), [keras](https://keras.io/activations/), [more-explain](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jjNhjU1IAp7V","colab_type":"text"},"source":["**Choosing the right Activation Function**\n","\n","Now that we have seen so many activation  functions, we need some logic / heuristics to know which activation function should be used in which situation. Good or bad – there is no rule of thumb.\n","\n","However depending upon the properties of the problem we might be able to make a better choice for easy and quicker convergence of the network.\n","\n","*  **Sigmoid** functions and their combinations generally work better in the case of classifiers\n","*  **Sigmoids and tanh** functions are sometimes avoided due to the vanishing gradient problem\n","*  **ReLU** function is a general activation function and is used in most cases these days\n","*  If we encounter a case of dead neurons in our networks the leaky **ReLU** function is the best choice\n","*  Always keep in mind that **ReLU** function should only be used in the hidden layers\n","*  As a rule of thumb, you can begin with using **ReLU** function and then move over to other activation functions in case **ReLU** doesn’t provide with optimum results"]},{"cell_type":"markdown","metadata":{"id":"vljP43l2tsWh","colab_type":"text"},"source":["## Layers\n","\n","Source [tf-layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)"]},{"cell_type":"markdown","metadata":{"id":"iSQHglhXLzjx","colab_type":"text"},"source":["**Core layers**\n","\n","- **Dropout**\n","  - [understand](https://www.phamduytung.com/blog/2019-05-05-deep-learning-dropout/)\n"]},{"cell_type":"markdown","metadata":{"id":"RvWgqmoQMGvm","colab_type":"text"},"source":["**Convolutional  & Pooling & Connected Layers**\n","[theory](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n","\n","**Convolutional Layers**\n","- The role of the ConvNet is to reduce the images into a form which is easier to process, without losing features which are critical for getting a good prediction. \n","\n","Source: [keras-convol](https://keras.io/layers/convolutional/),\n","\n","\n","**Pooling Layer**\n","\n","The Pooling layer is responsible for reducing the spatial size of the Convolved Feature. \n","- This is to decrease the computational power required to process the data through dimensionality reduction. \n","- Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n","\n","Source: [keras-pooling](https://keras.io/layers/pooling/)\n","\n","**Connected Layer**\n","[understand1](https://pennlio.wordpress.com/2014/04/11/fully-connected-locally-connected-and-shared-weights-layer-in-neural-networks/), [understand-2](https://prateekvjoshi.com/2016/04/12/understanding-locally-connected-layers-in-convolutional-neural-networks/)\n","\n","Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer\n","- Densly\n","- Fully\n","- Locally\n","\n","Source: [keras-connected](https://keras.io/layers/local/)\n"]},{"cell_type":"markdown","metadata":{"id":"2xNH4-iqpmT4","colab_type":"text"},"source":["**Embedding & Merge Layers**\n","\n","[understand](https://medium.com/@satnalikamayank12/on-learning-embeddings-for-categorical-data-using-keras-165ff2773fc9)\n","\n","Source: [keraas-embed](https://keras.io/layers/embeddings/)"]},{"cell_type":"markdown","metadata":{"id":"-kEQh8u6p5w2","colab_type":"text"},"source":["**Normalization Layers**\n","\n","[understand](https://forum.machinelearningcoban.com/t/correct-me-if-i-am-wrong-batch-normalize/2561)\n","\n","\n","Source: [Batch-norm](https://keras.io/layers/normalization/)\n"]},{"cell_type":"markdown","metadata":{"id":"VelRQ0AUMpjE","colab_type":"text"},"source":["**Recurrent Layers - RNN**\n","\n","\n","Source: [keras-RN](https://keras.io/layers/recurrent/)"]},{"cell_type":"markdown","metadata":{"id":"yY7oooODCt5J","colab_type":"text"},"source":["**Noise Layer**\n","\n","[understand](https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3)\n","\n","source: [keras-noise](https://keras.io/layers/noise/)\n"]},{"cell_type":"markdown","metadata":{"id":"Vfe-ZpIwKLST","colab_type":"text"},"source":["# Transfer Learning\n","\n","[understand](https://dlapplications.github.io/2018-07-15-Transfer-Learning-Basic/)"]},{"cell_type":"code","metadata":{"id":"dfnHjfRK-G_D","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QPbU5I5_2-eU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rbczoGOz22o7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}