{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2.RL.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_hJYM4-Sw_Vk","colab_type":"code","colab":{}},"source":["    \n","from IPython.display import IFrame\n","import os\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CtRKKbYJSb03","colab_type":"text"},"source":["# [Math Review](https://colab.research.google.com/drive/1K_7S4jYNOFdZuE6uzPPhDrKXMUzpJHZR?authuser=1)"]},{"cell_type":"code","metadata":{"id":"2JAYgNcPSdQX","colab_type":"code","colab":{}},"source":["%%html\n","<iframe src=\"http://louiskirsch.com/maps/reinforcement-learning\" width=\"600\" height=\"500\"></iframe>"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AwFhh6JYyv3-","colab_type":"code","colab":{}},"source":["report_path = \"http://web.stanford.edu/class/cs234/slides/lnotes_intro.pdf\"\n","rel_report_path = os.path.relpath(report_path)\n","from IPython.display import IFrame    \n","IFrame(rel_report_path, width=900, height=650)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"awHE5GzY2fHt","colab_type":"code","colab":{}},"source":["<iframe width=\"1354\" height=\"480\" src=\"https://www.youtube.com/embed/FgzM3zpZ55o?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZhbYGZHq6kuh","colab_type":"text"},"source":["# General\n","\n","[**Map of RL**](http://louiskirsch.com/maps/reinforcement-learning)\n","\n","## Method\n","<img src = \"https://drive.google.com/uc?id=1fRPH__9S0xyIKZIfCFoROZCi_HX8OLZM\" width=600 height=300>\n"]},{"cell_type":"markdown","metadata":{"id":"UXIdlbW8ABj1","colab_type":"text"},"source":["# Value based\n","- [Q-learning](https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56)\n"]},{"cell_type":"code","metadata":{"id":"jz3GMUOE72z_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"trGK1vEN6SKP","colab_type":"text"},"source":["# Standford-RL"]},{"cell_type":"markdown","metadata":{"id":"yt_k-JbqGBj9","colab_type":"text"},"source":["## Intro\n","in a Reinforcement Learning setting, we are dealing with making decisions and comparing actions that could be taken, rather than making predictions. \n","\n","A Reinforcement Learning agent may interact with the world, and receive some immediate, partial feedback signal - commonly called a Reward - for each interaction.\n","\n","- Về cơ bản, agent phải có thể tối ưu hóa hành động của nó và đạt được lượng lớn nhất rewards.\n","    - Trade-off giữa exploration and exploitation\n","    - Agent có thể generalize experience hay k? (khái quát hóa các stage mặc dù nó không đi qua trong quá khứ)\n","    - Delayed consiquences (thời gian và kết quả)\n","    "]},{"cell_type":"code","metadata":{"id":"SlU4xp241dkG","colab_type":"code","outputId":"5477a87b-acb6-4ab4-8594-bf513c4b0dab","executionInfo":{"status":"ok","timestamp":1565671977961,"user_tz":-420,"elapsed":691,"user":{"displayName":"An Nguyễn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAmIzVzuSJNfcPFlxXSQsn53YMiTeTUYIBv6V-0=s64","userId":"08833028508227386449"}},"colab":{"base_uri":"https://localhost:8080/","height":421}},"source":["from IPython.display import HTML\n","\n","# Youtube\n","HTML('<iframe width=\"800\" height=\"400\" src=\"https://www.youtube.com/embed/FgzM3zpZ55o?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<iframe width=\"800\" height=\"400\" src=\"https://www.youtube.com/embed/FgzM3zpZ55o?list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"HXymMy4y08BN","colab_type":"code","colab":{}},"source":["%%html\n","<iframe src=\"http://web.stanford.edu/class/cs234/slides/lnotes_intro.pdf\" width=\"600\" height=\"600\"></iframe>"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyqqZ4WbK5nJ","colab_type":"text"},"source":["## 1 Overview of reinforcement learning\n","Về cơ bản, có RL 2 phần chính 1 gọi là world và agent, thì agent sẽ make chuỗi actions là $a_t$ khi đối mặt với observation $o_t$ và nhận reward $r_t$.\n","thì history of time là $h_t=\\{a_1,0_1,r_1,...,a_t,0_t,r_t\\}$ do đó action tại t+1 là $a_{t+1}=f(h_t)$ \n","\n","Hành động ở t+1 là kết quả của tất cả các hành động trước bắt đầu từ t = 1.\n","### 1.1 Sequential Decision Making\n","- Some deterministic and fnite settings AI techniques like A* search and minimax can be used to find an optimal sequence of actions\n","- Nhưng bị khi số lượng stage lớn thì gần như không thể (mất thời gian)\n","> Do đó cần có biện pháp giải quyết\n","\n","### 1.2 Modeling the world\n","A is set of actions, a{t} - sequence of actions, chuỗi các hành động\n","\n","S is set of stages, s{t} - sequence of states, chuỗi các trạng thái\n","\n","In RL, thường giả định 1 **Markov Property** là:\n","*transition dynamics* (động lực chuyển trạng thái): Xác suất có điều kiện của trạng thái s{t+1} given by chuỗi tất cả các trạng thái và hành động ở quá khứ = xác suất có điều kiện trạng thái s{t+1} given by trạng tái ở 1 thời điểm s{t} và a{t}\n","$$P\\left(s_{t+1} | s_{t}, a_{t}, \\ldots, s_{1}, a_{1}\\right)=P\\left(s_{t+1} | s_{t}, a_{t}\\right)$$\n","**Reward function** (phương trình thưởng): reward r{t} là phần thưởng khi đổi trạng thái s{t}->^a{t}->s{t+1}\n","$$R(s)=\\mathbb{E}\\left[r_{t} | s_{t}=s\\right]$$ or $$R(s, a)=\\mathbb{E}\\left[r_{t} | s_{t}=s, a_{t}=a\\right]$$\n","\n","- $R\\{s\\}$ phan thuong cho 1 stage s = expectation r_t (reward tai t) given by stage s_t = s, hay \n","- $R\\{s,a\\}$ phan thuong cho 1 stage s, a = r_t (reward tai t) given by stage s_t = s, và a_t = a (trạng thái s và hành động a).\n","\n","### 1.3 Components of an reinforcement learning agent\n","agent stage sẽ là kết quả của 1 hàm của quá khứ $s^a_t=g(h_t)$ \n","- policy $\\pi$ mapping from stage $s$ to action $a$ (nối từ trạng thái sang hành động), $\\pi\\left(s_{t}^{a}\\right) \\in A$\n","- value function $V^\\pi$ sẽ bằng mong đợi của tổng kết quả tại thời điểm t và discounted rewards cho bới trạng tái s\n","$$V^{\\pi}(s)=\\mathbb{E}_{\\pi}\\left[r_{t}+\\gamma r_{t+1}+\\gamma^{2} r_{t+2}+\\ldots | s_{t}=s\\right]$$\n","\n","discount factor $\\gamma$ is used to weigh immediate rewards versus delayed rewards"]},{"cell_type":"markdown","metadata":{"id":"EyfTxFfoENAC","colab_type":"text"},"source":["## 3.Making Good Decisions Given a Model of the World\n","## 3.1 Markov process (MP)\n","là 1 quá trình ngẫu nhiên (stochastic process) mà nó thóa Markov property, Markov process là *memoryless*\n","\n","2 additional assumptions:\n","- *Finite state space*: trạng thái của MP là hữu hạn\n","- *Stationary transition probabilities* : tính cố định của xác suất chuyển tráng thái, xác suất chuyển tráng thái thì phụ thuốc vào thời gian\n","$$P\\left(s_{i}=s^{\\prime} | s_{i-1}=s\\right)=P\\left(s_{j}=s^{\\prime} | s_{j-1}=s\\right), \\quad \\forall s, s^{\\prime} \\in S, \\quad \\forall i, j=1,2, \\dots$$\n","\n","Khi thỏa các đk trên MP còn được gọi là Markov chain (MC) và góp phần tạo 1 tính chất tốt của ma trận xác suất chuyển trạng thái transition probability matrix $\\mathbf{P}$ of size $|S| \\times|S|$, whose $(i, j)$ entry is given by $P_{i j}=P(j | i)$, with $i, j$ referring to the st ates of $S$ ordered arbitrarily. \n","Do đó, MP được xác định bởi 1 tuple $(S, \\mathbf{P})$ (dãy giá trị có hữu hạng) \n","- A finite st ate space.\n","- P : $A$ transition probability model that specifies $P\\left(s^{\\prime} | s\\right)$\n","\n","**Characteristic**\n","- P is a row-stochastic matrix.\n","- 1 is an eigenvalue of any row-stochastic matrix\n","- any eigenvalue of a row-stochastic matrix has maximum absolute value 1\n","\n","Example:\n","\n","![alt text](https://cdn.mathpix.com/snip/images/KZ99pyMJvAOkj95rFmxdoTuQ3W-hzLdrrcIpFgDBKio.original.fullsize.png)\n","\n","transition probability matrix -\n","$\\mathbf{P}=\\left(\\begin{array}{ccccccc}{S 1} & {S 2} & {S 3} & {S 4} & {S 5} & {S 6} & {S 7} \\\\ {0.6} & {0.4} & {0} & {0} & {0} & {0} & {0} \\\\ {0.4} & {0.2} & {0.4} & {0} & {0} & {0} & {0} \\\\ {0} & {0.4} & {0.2} & {0.4} & {0} & {0} & {0} \\\\ {0} & {0} & {0} & {0.2} & {0.4} & {0} & {0} \\\\ {0} & {0} & {0} & {0.4} & {0.2} & {0.4} & {0} \\\\ {0} & {0} & {0} & {0} & {0.4} & {0.2} & {0.4} \\\\ {0} & {0} & {0} & {0} & {0} & {0.4} & {0.6}\\end{array}\\right) S 1 ... S 7 $\n","\n","## 3.2 Markov Reward Process (MRP)\n","Stationary rewards : The rewards in a Markov reward process are stationary which means that\n","they are time independent.\n","\n","## 3.3 Computing the value function of a Markov reward process\n","### 3.3.1 Monte Carlo simulation\n","### 3.3.2 Analytic solution \n","(work only) H = infinite, thi expectation của tổng các discounted value mới băng V(s')\n","\n","## 3.4 Markov decision process (MDP)\n","Khác với MP và MRP, thay vì xác suất chuyển trạng thái chỉ là phương trình của $s_t$, ở MDP xác suất chuyển trạng thái là pt của $s_t$ và $a_t$\n","### 3.4.1 MDP policies and policy evaluation\n","**Notes**: the policy may be varying with time, which is especially true in the case of finite horizon MDPs\n","\n","## 3.5 Bellman backup operators\n","### 3.5.1 Bellman expectation backup operator\n","for finite H, \n","Then for element $U \\in \\mathbb{R}^{|S|}$ the Bellman expectation backup operator $B^{\\pi}$ for the policy $\\pi$ is defined as\n","$$\n","\\left(B^{\\pi} U\\right)(s)=R^{\\pi}(s)+\\gamma \\sum_{s^{\\prime} \\in S} P^{\\pi}\\left(s^{\\prime} | s\\right) U\\left(s^{\\prime}\\right), \\forall s \\in S\n","$$\n","**Theorem 3.2.** The operator $B^{\\pi}$ defined in (28) is a contration map. If $\\gamma<1$ then it is a strict contraction and has a unique fixed point.\n","$$\\left|\\left(B^{\\pi} U_{1}\\right)(s)-\\left(B^{\\pi} U_{2}\\right)(s)\\right| =\\gamma\\left\\|U_{1}-U_{2}\\right\\|_{\\infty}$$\n","\n","### 3.5.2 Bellman optimality backup operator\n","Giúp đưa ra cách estimate $\\pi$\n","\n","\n","## 3.6 MDP control in the infinite horizon setting\n","estimate policy $\\pi$ trong trường hợp infinite\n","\n","## 3.7 MDP control for a finite horizon MDP\n","Value funtion trong trường hơp finite, và có sự tồn tại của $\\pi$\n"," ![alt text](https://cdn.mathpix.com/snip/images/2-WpCsd48LdhxGMLm14Lv6QtKsZzFZNhxvd2LtPFB8k.original.fullsize.png)"]},{"cell_type":"code","metadata":{"id":"mLrjTt7j9vhc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FXbsEF4tmYZU","colab_type":"text"},"source":["# Dynamic vs Reinforcement\n","\n","Dynamic:\n","Chỉ tính được khi xác suất từ trạng thái này đến trạng thái kia\n","là khi model is known\n","RL:\n","\n","-> model-based : ước tính model, rồi mới tính\n","-> model-free : \n"]},{"cell_type":"markdown","metadata":{"id":"Qeb_aSzRfith","colab_type":"text"},"source":["# Dynamic Programming\n","\n","- Mokov Process: hiện tại chỉ phụ thuộc vào quá khứ gần nhất. Xác suất chuyển trạng thái chỉ phụ thuộc và $s_t$\n","- MDP: Xác suất chuyển trạng thái chỉ phụ thuộc và $s_t$ và $a_t$,\n","  - environment fully observable (có thể biết tất cả về giá, giá đóng cửa, mở cửa, volumne...)\n","  - partially observed MDP: khi env bị giới hạn\n","  - state tránition matrix: chỉ có 2 trạng thái, s và s', là ma trận của s = [s1,..,sn] với s' = [s'1,...,s'n] (35/491)\n","- Reward (max), cost (min): \n","  - trong MP reward hàm của trạng thái, r(t+1) = f(s(t)và s(t+1)). Hành động tốt nhất ở hiện tại chưa chắc là tốt nhất cho cả quá trình.\n","  - trong MDP r(t+1) = f(s(t)và s(t+1) và a(t))\n","- Value: trong MP, trong tương lại lấy expected value là trung bình trọng số, của tất cả các state ở tương lai cho bởi s(t).\n","  - luôn có tập hơp S: là tập hợp của tất cả các hành động\n","  - policy là quy tắc, tức là hàm của trạng thái và trả ra kết quả. Nếu không có policy tức là actition \n","  - $V_\\pi(s)=$  \n","  \n","- Bellman Equation: giá trị của thời điểm hiện tại có ảnh hưởng bởi giá trị của thời điểm sau đó\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tq8U0i_ZxeoG","colab_type":"code","colab":{}},"source":["#"],"execution_count":0,"outputs":[]}]}