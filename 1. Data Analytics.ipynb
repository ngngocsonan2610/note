{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1. Data Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "WzFvPys_F9KB"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/1.%20Data%20Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mO6Wp3YBXYh",
        "colab_type": "toc"
      },
      "source": [
        ">[Sources](#scrollTo=BvORvDCvAdH6)\n",
        "\n",
        ">[1.EDA](#scrollTo=ltyLBhWQm2YF)\n",
        "\n",
        ">>[Resume & Phân tích df](#scrollTo=pmXBVoIJDDjN)\n",
        "\n",
        ">>[Correlation](#scrollTo=DEhCkATJnX0l)\n",
        "\n",
        ">>[Statistical Checking](#scrollTo=TdqQPJ9zuPAh)\n",
        "\n",
        ">>[Bayesian Statistic](#scrollTo=foaLMVH2wNDH)\n",
        "\n",
        ">[2.Cleaning Data](#scrollTo=arothx_v_lC-)\n",
        "\n",
        ">>[Missing value](#scrollTo=FSzLvImTnBdI)\n",
        "\n",
        ">>[Outliers](#scrollTo=G9dkPpmLmkDE)\n",
        "\n",
        ">[3.Features Processing](#scrollTo=81RBd7sRk0fu)\n",
        "\n",
        ">>[string processing /NLP processing](#scrollTo=OkQAo4CboTLF)\n",
        "\n",
        ">>[merge/concat](#scrollTo=pEwO7OIEmZ-B)\n",
        "\n",
        ">>[where](#scrollTo=0ZwKNHDtmQ4h)\n",
        "\n",
        ">>[group by](#scrollTo=GZ-VCSnElcgU)\n",
        "\n",
        ">>[Binning](#scrollTo=OcFpKNKUnEoX)\n",
        "\n",
        ">>[Time](#scrollTo=yWm9sbjolESN)\n",
        "\n",
        ">[4.Visualization](#scrollTo=zJUeNk50VTOP)\n",
        "\n",
        ">>[Note](#scrollTo=wuLwfvj3LvGi)\n",
        "\n",
        ">>[Plot continous features](#scrollTo=OUdukOL-nnMp)\n",
        "\n",
        ">>[Plot categorical](#scrollTo=QcyJ6xFLmJqM)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvORvDCvAdH6",
        "colab_type": "text"
      },
      "source": [
        "# Sources"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNimUPIBigtD",
        "colab_type": "code",
        "outputId": "9f9624ce-279e-4275-8b70-d705c63cc2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "source": [
        "%%html\n",
        "<iframe src=\"https://stackedit.io/app#\" width=\"800\" height=\"600\"></iframe>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<iframe src=\"https://stackedit.io/app#\" width=\"1000\" height=\"600\"></iframe>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmtsCrqtu-rI",
        "colab_type": "text"
      },
      "source": [
        "- [**A Comprehensive Guide to Data Exploration**](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)\n",
        "- [Data Analytics Collection - Hasbrain](https://note.hasbrain.com/5c875913b40548001824aa67?target=path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltyLBhWQm2YF",
        "colab_type": "text"
      },
      "source": [
        "# 1.EDA "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmXBVoIJDDjN",
        "colab_type": "text"
      },
      "source": [
        "## Resume & Phân tích df\n",
        "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data.\n",
        "- Phân tích về min max, mean, median, mode, std\n",
        "- Phân tích về loại của feature (continous/categorical, numeric/text)\n",
        "- Đếm giá trị unique ở features/ giá trị missing \n",
        "\n",
        "**Sources**:\n",
        "- [Đọc thêm 1](https://towardsdatascience.com/understanding-descriptive-statistics-c9c2b0641291)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB4wn9OSlKw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "\n",
        "WOE_numeric_col = [x for x in df_WOE.select_dtypes(include=numerics).columns\n",
        "              if x not in label+ ag_id+ time\n",
        "              ]\n",
        "WOE_cat_col = [x for x in df_WOE.columns \n",
        "           if x not in WOE_numeric_col + label + ag_id + time]\n",
        "\n",
        "ratio_col = [x for x in WOE_numeric_col if 'ratio' in x]\n",
        "\n",
        "print(' WOE_numeric_col: ', WOE_numeric_col)\n",
        "print('\\n WOE_cat_col: ', WOE_cat_col)\n",
        "print('\\n ratio_col: ', ratio_col)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91jE-tpqCsjb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function assits to expand Exploratory Data Analysis (EDA) is an open-ended \n",
        "#process where we calculate statistics and make figures to find trends, anomalies,\n",
        "#patterns, or relationships within the data. The goal of EDA is to learn what our\n",
        "#data can tell us. It generally starts out with a high level overview, \n",
        "#then narrows in to specific areas as we find intriguing areas of the data. \n",
        "\n",
        "def des_stat_analyze(df_input):\n",
        "    # check number of rows, cols\n",
        "    no_rows = df_input.shape[0]\n",
        "    no_cols = df_input.shape[1]\n",
        "    print(\"No. observations:\", no_rows )\n",
        "    print(\"No. features:\", no_cols )\n",
        "  \n",
        "    # checking type of features\n",
        "    name = []\n",
        "    cols_type = []\n",
        "    for n,t in df_input.dtypes.iteritems():\n",
        "        name.append(n)\n",
        "        cols_type.append(t)\n",
        "\n",
        "    # checking distinction (unique values) of features\n",
        "    ls_unique = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nunique = df_input[cname].nunique()\n",
        "            pct_unique = nunique*100.0/ no_rows\n",
        "            ls_unique.append(\"{} ({:0.2f}%)\".format(nunique, pct_unique))\n",
        "        except:\n",
        "            ls_unique.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue\n",
        "\n",
        "    # checking missing values of features\n",
        "    ls_miss = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nmiss = df_input[cname].isnull().sum()\n",
        "            pct_miss = nmiss*100.0/ no_rows\n",
        "            ls_miss.append(\"{} ({:0.2f}%)\".format(nmiss, pct_miss))\n",
        "        except:\n",
        "            ls_miss.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue \n",
        "      \n",
        "    # checking zeros\n",
        "    ls_zeros = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nzeros = (df_input[cname] == 0).sum()\n",
        "            pct_zeros = nzeros * 100.0/ no_rows\n",
        "            ls_zeros.append(\"{} ({:0.2f}%)\".fornat(nzeros, pct_zeros))\n",
        "        except:\n",
        "            ls_zeros.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue\n",
        "      \n",
        "    # checking negative values\n",
        "    ls_neg = []\n",
        "    for cname in df_input.columns:\n",
        "        try:\n",
        "            nneg = (df_input[cname].astype(\"float\")<0).sum()\n",
        "            pct_neg =nneg * 100.0 / no_rows\n",
        "            ls_neg.append(\"{} ({:0.2f}%)\".format(nneg, pct_neg))\n",
        "        except:\n",
        "            ls_neg.append(\"{} ({:0.2f}%)\".format(0,0))\n",
        "            continue\n",
        "      \n",
        "    # extracting the output\n",
        "    data = {\n",
        "      \"name\": name,\n",
        "      \"col_type\": cols_type,\n",
        "      \"n_unique\": ls_unique,\n",
        "      \"n_miss\": ls_miss,\n",
        "      \"n_zeros\":ls_zeros,\n",
        "      \"n_neg\":ls_neg      \n",
        "    }\n",
        "  \n",
        "    # statistical info\n",
        "    df_stats = df_input.describe().transpose()\n",
        "    ex_stats = pd.concat([df_input.median(),df_input.kurtosis(),df_input.skew()], axis = 1)\n",
        "    ex_stats = ex_stats.rename(columns={0:\"median\",1:\"kurtosis\",2:\"skew\"})\n",
        "    df_stats = df_stats.merge(ex_stats, left_index=True, right_index=True)\n",
        "    #ls_stats = []\n",
        "    for stat in df_stats.columns:\n",
        "        data[stat] = []\n",
        "        for cname in df_input.columns:\n",
        "            try:\n",
        "                data[stat].append(df_stats.loc[cname, stat])\n",
        "            except:\n",
        "                data[stat].append(\"NaN\")\n",
        "        \n",
        "    # take samples\n",
        "    df_sample = df_input.sample(frac = .5).head().transpose()\n",
        "    df_sample.columns = [\"sample_{}\".format(i) for i in range(5)]\n",
        "  \n",
        "    # repair the output\n",
        "    col_ordered = [\"name\",\"col_type\",\"count\",\"n_unique\",\"n_miss\",\"n_zeros\",\"n_neg\",\n",
        "                \"25%\",\"50%\",\"75%\",\"max\",\"min\",\"mean\",\"median\",\"std\",\"kurtosis\",\"skew\"]\n",
        "    df_data = pd.DataFrame(data, columns = col_ordered).set_index(\"name\")\n",
        "    df_data = pd.concat([df_data, df_sample], axis = 1)\n",
        "\n",
        "    return df_data.reset_index().sort_values(by=['col_type'])  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_Vn17UQPKQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resumetable(df):\n",
        "    print(f\"Dataset Shape: {df.shape}\")\n",
        "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
        "    summary = summary.reset_index()\n",
        "    summary['Name'] = summary['index']\n",
        "    summary = summary[['Name','dtypes']]\n",
        "    summary['Missing'] = df.isnull().sum().values    \n",
        "    summary['Uniques'] = df.nunique().values\n",
        "    summary['Max'] = df.max().values\n",
        "    summary['Min'] = df.min().values\n",
        "    summary['Mean'] = df.mean().values\n",
        "    summary['Var'] = df.std().values\n",
        "    summary['Q1'] = df.quantile(0.25).values\n",
        "    summary['Q3'] = df.quantile(0.75).values    \n",
        "    \n",
        "    \n",
        "    summary['First Value'] = df.loc[0].values\n",
        "    summary['Second Value'] = df.loc[1].values\n",
        "    summary['Third Value'] = df.loc[2].values\n",
        "#     summary['Sample 1'] = df.sample(1).values\n",
        "#     summary['Sample 2'] = df.sample(1).values\n",
        "#     summary['Sample 3'] = df.sample(1).values\n",
        "    \n",
        "\n",
        "    for name in summary['Name'].value_counts().index:\n",
        "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n",
        "    summary.sort_values(by=['Name', 'dtypes'])\n",
        "    return summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhDbpTYweqyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Tìm các bất ổn về missing value và các cột có bật ổn về số lượng giá trị\n",
        "'''\n",
        "many_null_cols = [col for col in df_train.columns if df_train[col].isnull().sum() / df_train.shape[0] > 0.9]\n",
        "many_null_cols_test = [col for col in df_test.columns if df_test[col].isnull().sum() / df_test.shape[0] > 0.9]\n",
        "\n",
        "\n",
        "big_top_value_cols = ([col for col in train.columns if \n",
        "                       df_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9])\n",
        "big_top_value_cols_test = ([col for col in test.columns if \n",
        "                            df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9])\n",
        "one_value_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
        "one_value_cols_test = [col for col in df_test.columns if df_test[col].nunique() <= 1]\n",
        "one_value_cols == one_value_cols_test\n",
        "\n",
        "cols_to_drop = list(set(many_null_cols + many_null_cols_test + \n",
        "                        big_top_value_cols + big_top_value_cols_test + \n",
        "                        one_value_cols + one_value_cols_test))\n",
        "cols_to_drop.remove('isFraud')\n",
        "len(cols_to_drop)\n",
        "\n",
        "df = df.drop(cols_to_drop, axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEhCkATJnX0l",
        "colab_type": "text"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "Xem correlation của các features với nhau: \n",
        "- 2 features có chỉ số correlation dương, có nghĩa khi A tăng thì B tăng (if A increase, B will increase)\n",
        "- 2 features có chỉ số correlation âm, có nghĩa khi A tăng thì B giảm (if A increase, B will decrease)\n",
        "- - 2 features có chỉ số correlation cao, có nghĩa là 2 features này có khả năng thay thế nhau trong model (replaces)\n",
        "\n",
        "Sources:\n",
        "-  [Covariance vs Correlation](https://forum.machinelearningcoban.com/t/covariance-va-correlation/767)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGrqIFBwDAUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_corr(df):\n",
        "    correlations = df.corr()\n",
        "\n",
        "    # Using seaborn package\n",
        "    # Generate a mask for the upper triangle\n",
        "    mask = np.zeros_like(correlations, dtype=np.bool)\n",
        "    mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "    # Set up the matplotlib figure\n",
        "    f, ax = plt.subplots(figsize=(16, 16))\n",
        "\n",
        "    # Generate a custom diverging colormap\n",
        "    cmap = sns.diverging_palette(260, 10, as_cmap=True)\n",
        "\n",
        "    # Draw the heatmap with the mask and correct aspect ratio\n",
        "    sns.heatmap(correlations, mask=mask, cmap=cmap, vmin = -1, vmax= 1 , center=0,\n",
        "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "    plt.yticks(rotation=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TkVsIzFm7mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_topcorr_bycol(col):\n",
        "    df_corr = df.corr().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "    df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}\n",
        "                , inplace=True)     \n",
        "    return df_corr[df_corr[\"Feature 1\"]==col]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rosahxWZdAEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdqQPJ9zuPAh",
        "colab_type": "text"
      },
      "source": [
        "## Statistical Checking\n",
        "Các kỹ thuật khác trong việc kiểm tra về mặt thống kê\n",
        "\n",
        "Source:\n",
        "- [Statistical Learning oxh](https://ongxuanhong.wordpress.com/category/kien-thuc/statistical-inference/)\n",
        "- [Video về Statistic](https://www.youtube.com/user/drnguyenvtuan/videos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foaLMVH2wNDH",
        "colab_type": "text"
      },
      "source": [
        "## Bayesian Statistic\n",
        "- [How Bayesian statistics convinced me to hit the gym](https://towardsdatascience.com/how-bayesian-statistics-convinced-me-to-hit-the-gym-fa737b0a7ac)\n",
        "- [Think you need to learn Bayesian Analysis? Read this first](https://peadarcoyle.com/2019/01/01/think-you-need-to-learn-bayesian-analysis-read-this-first/?fbclid=IwAR1KGEuQNfzzf0BexNampJsyZ3zNgcfzDOgNMsPvppjElAu4qOOPF8AwC7g)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xzrRN4mdAL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arothx_v_lC-",
        "colab_type": "text"
      },
      "source": [
        "# 2.Cleaning Data\n",
        "\n",
        "Used in:\n",
        "  - Kaggle:[ Fraud Detection](https://www.kaggle.com/sonannguyenngoc/fraud-pca/edit/run/18581236) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSzLvImTnBdI",
        "colab_type": "text"
      },
      "source": [
        "## Nan/Inf value\n",
        "- fill na : https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n",
        "\n",
        "- \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v9q8-S-nEBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tìm na/nan/null trong từng cột\n",
        "# Hiện thị: Số na, %na, type\n",
        "# GPreda, missing data\n",
        "def missing_data(data):\n",
        "    total = data.isnull().sum()\n",
        "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
        "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    types = []\n",
        "    for col in data.columns:\n",
        "        dtype = str(data[col].dtype)\n",
        "        types.append(dtype)\n",
        "    tt['Types'] = types\n",
        "    return(np.transpose(tt))\n",
        "\n",
        "df['Count_miss'] = df[col].isnull().sum(axis=1)\n",
        "\n",
        "df['nulls1'] = df.isna().sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjWMnkr_AO1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "1 số cách fill NA\n",
        "interpolate from categorical features\n",
        "'''\n",
        "\n",
        "\n",
        "df = df[df['MEMBER_GENDER'].notnull()]\n",
        "\n",
        "df['MEMBER_MARITAL_STATUS'] = df['MEMBER_MARITAL_STATUS'].fillna('UNK')\n",
        "df['MEMBER_MARITAL_STATUS'] = df['MEMBER_MARITAL_STATUS'].replace({'S':'S-W-D','W':'S-W-D','D':'S-W-D'})\n",
        "\n",
        "\n",
        "df['MEMBER_ANNUAL_INCOME'] = df.groupby(['MEMBER_OCCUPATION_CD','MEMBER_MARITAL_STATUS','MEMBER_GENDER'])\n",
        "                              \\ ['MEMBER_ANNUAL_INCOME'].apply(lambda x: x.fillna(x.median()))  \n",
        "\n",
        "titanic_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
        "\n",
        "titanic_all['Fare'] = titanic_all['Fare'].fillna(titanic_all.groupby(\n",
        "    ['Pclass', 'Parch', 'SibSp']).Fare.transform('mean'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJbGg1lePffW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.isinf(df[ratio_col]).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrDfAvxMHWGA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "đếm số lượng bằng 0 cho mỗi dòng trên toàn bộ data\n",
        "'''\n",
        "\n",
        "t1['count_zero'] = (t1 == 0).astype(int).sum(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Alus3s2GaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.groupby('name')['activity'].value_counts().unstack().fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhgqCpzs2HVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9dkPpmLmkDE",
        "colab_type": "text"
      },
      "source": [
        "## Outliers\n",
        "\n",
        "[estimation cdf - plot](https://www.jddata22.com/home//plotting-an-empirical-cdf-in-python)\n",
        "  - check outlier by std and mean\n",
        "  \n",
        "[boxplot & quanlite](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n",
        "  - check outlier by \n",
        "    - IQR: interquantile range = Q3 - Q1\n",
        "    - “maximum”: Q3 + 1.5*IQR\n",
        "    - “minimum”: Q1 -1.5*IQR\n",
        "\n",
        "z-score\n",
        "\n",
        "\n",
        "**News**\n",
        "- [Ways to Detect and Remove the Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n",
        "- [Three ways to detect outliers](http://colingorrie.github.io/outlier-detection.html)\n",
        "- [How to Use Statistics to Identify Outliers in Data](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)\n",
        "- [5 Ways to Find Outliers in Your Data](https://statisticsbyjim.com/basics/outliers/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DReevHMvb0Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CalcOutliers(df_num): \n",
        "    '''\n",
        "    Find Outliers for continous cols\n",
        "    Tìm outliner dựa trên std, và mean\n",
        "\n",
        "    Sau khi phân tích quantile và mean, std\n",
        "    có thể biết outlier nằm ngoài vùng nào:\n",
        "    vd: cut = std * 3\n",
        "    '''\n",
        "    # calculating mean and std of the array\n",
        "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
        "\n",
        "    # seting the cut line to both higher and lower values\n",
        "    # You can change this value\n",
        "    cut = data_std * 3\n",
        "\n",
        "    #Calculating the higher and lower cut values\n",
        "    lower, upper = data_mean - cut, data_mean + cut\n",
        "\n",
        "    # creating an array of lower, higher and total outlier values \n",
        "    outliers_lower = [x for x in df_num if x < lower]\n",
        "    outliers_higher = [x for x in df_num if x > upper]\n",
        "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
        "\n",
        "    # array without outlier values\n",
        "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
        "    \n",
        "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
        "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
        "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
        "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
        "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
        "    \n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAwK67oVoNiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "which defined as the proportion of the IQR past the low and high quartiles to extend the plot whiskers \n",
        "or interquartile range (IQR)\n",
        "therefore, maximum = Q3 + 1.5*IQR , min = Q1 - 1.5*IQ\n",
        "\n",
        "input = df[col] list của pd.DataFrame\n",
        "output = list [0,1] thể hiện giá trị nào là outlier\n",
        "\n",
        "'''\n",
        "\n",
        "def find_out_IQR_list(col) :\n",
        "    Q1 = col.quantile(0.25)\n",
        "    Q3 = col.quantile(0.75)\n",
        "    IQR = Q3 - Q1    \n",
        "    print('> {} : IQR range ({},{})'.format(col.name,(Q1 - 1.5 * IQR),(Q3 + 1.5 * IQR)))\n",
        "    \n",
        "    return np.where((col < (Q1 - 1.5 * IQR)) | (col > (Q3 + 1.5 * IQR)),1,0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOfFpvXue3We",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81RBd7sRk0fu",
        "colab_type": "text"
      },
      "source": [
        "# 3.Features Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkQAo4CboTLF",
        "colab_type": "text"
      },
      "source": [
        "## STR/NLP processing\n",
        " - https://stackoverflow.com/questions/37011734/pandas-dataframe-str-contains-and-operation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVAQeauxoSxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sum_str(x):\n",
        "    x = x.split(',')\n",
        "    l = []\n",
        "    for i in x:\n",
        "        if i != \"\":\n",
        "            l.append(int(i))\n",
        "    return sum(l)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKzGdey_oS4k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = df[['APE_Net_SumLast3m','Gender','EducationCode','OccupationCode']]\n",
        "t['APE_Net_SumLast3m_Bins'] = pd.qcut(df.APE_Net_SumLast3m, q=10)\n",
        "t['OccupationCode_fillna'] =  t.groupby(['Gender','EducationCode','APE_Net_SumLast3m_Bins'])['OccupationCode'].apply(lambda x : x.fillna(x.value_counts().index[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMU1ZmXxp46x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_lower_case(item):\n",
        "    return item.map(lambda x: x if type(x)!=str else x.lower())\n",
        "\n",
        "def to_upper_case(item):\n",
        "    return item.map(lambda x: x if type(x)!=str else x.upper())\n",
        "    \n",
        "''' example:\n",
        "train[\"province\"] = to_lower_case(train['province'])\n",
        "to_upper_case(pd.Series(['abc']))\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maKlgbsExUx5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def replace_data(data,key,values):\n",
        "  for index, value in enumerate(data):\n",
        "    if key in str(value):\n",
        "      data[index] = values\n",
        "    elif not str(value):\n",
        "      data[index] = data[index]\n",
        "  return data\n",
        "replace_data(train_df[\"maCv\"],'công nhân','CN')\n",
        "replace_data(train_df[\"maCv\"],'nhân viên','NV')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqOeXGnNqcOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " t.COMPONENT_CODE.str.contains(r'(OPW[0-9])|(UCW[0-9])|(DSR[0-9])')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAR4OFzSqcYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytqAL29TqcfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEwO7OIEmZ-B",
        "colab_type": "text"
      },
      "source": [
        "## merge/concat\n",
        "- https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4NlDZvymZUa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.concat([df1,df2], axis = 0).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ThYFF1smZlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ-VCSnElcgU",
        "colab_type": "text"
      },
      "source": [
        "## group by\n",
        "- http://esantorella.com/2016/06/16/groupby/\n",
        "- https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZgKYqBJlbKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Groupby - Agg join str to str-array.\n",
        "# Groupby hợp một cột category thành 1 chuỗi category\n",
        "#\n",
        "df_gb = df.groupby([\"AGCode\"], as_index = True).agg({\n",
        "    \"AGLevel\":';'.join\n",
        "    , 'Cnt':(lambda x: ';'.join(x.astype(str)))\n",
        "    , \"YearMonth_Start\" : [(lambda x: ';'.join(x.astype(str))),'min']\n",
        "  \n",
        "}).reset_index()\n",
        "df_gb.columns = ['_'.join(col) for col in df_gb.columns]\n",
        "df_gb.rename(columns={\"AGCode_\":\"AGCode\", \"AGLevel_join\": \"AGLevel_flow\"\n",
        "                      ,\"Cnt_<lambda>\": \"Cnt_flow\",\"YearMonth_Start_<lambda_0>\":\"YearMonth_flow\"\n",
        "                      , \"YearMonth_Start_min\":\"First_month\"\n",
        "                     }, inplace = True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1TMsHlolboh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Conditional groupby, các dạng custom agg của group \n",
        "#\n",
        "\n",
        "sum_unique = (lambda x : sum(x.unique()))\n",
        "mean_unique = (lambda x : np.mean(x.unique()))\n",
        "var_unique = (lambda x : np.var(x.unique()))\n",
        "mode = (lambda x: x.value_counts().index[0])\n",
        "quantile_mean = lambda x : x[(x>=x.quantile(0.25)) & (x<=x.quantile(0.75))].mean()\n",
        "\n",
        "t = df[(df.PM_Promo_Time != \"Nan\") & (df.PM_Promo_Time <= 12)].groupby([\"PM_Promo_Time\"]).agg({\n",
        "    ,\"Gr_APE_Net_SumNext6m\": ['mean','median']\n",
        "     ,\"No_Active_Net_Last3m\": ['mean',quantile_mean]\n",
        "\n",
        "}).reset_index()#.sort_values(by=[('PM_Promo_Time', '')])\n",
        "\n",
        "t.columns = ['_'.join(col) for col in t.columns]\n",
        "t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtQDTlTl3nyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# Groupby sử dụng lambda, và các advantage techniques\n",
        "#\n",
        "\n",
        "temp = temp.groupby(['Year','month','CHDRNUM','LIFCNUM']).apply(lambda x: sum(x[(x.LIFE == 1)&(x.COMPONENT_ID.isnull()==True)].SUM_ASSURED.fillna(0))).rename('SUM_SA').reset_index()\n",
        "#.apply(lambda x: sum(x[x.LA_INFO_ID == x.LA_INFO_ID.min()].SUM_ASSURED)) #.SUM_ASSURED.sum()\n",
        "df = df.merge(temp, on=['Year','month','CHDRNUM','LIFCNUM'])\n",
        "\n",
        "\n",
        "t = df.groupby(['c','d']).apply(lambda x: sum(x.a+x.b)).rename('e').reset_index()\n",
        "df.merge(t, on=['c','d'])\n",
        "\n",
        "# temp = temp.groupby(['Year','month','CHDRNUM','LIFCNUM']).apply(lambda x: sum(x[x.LA_INFO_ID == x.LA_INFO_ID.min()].SUM_ASSURED.fillna(0))).rename('SUM_SA').reset_index()\n",
        "# #.apply(lambda x: sum(x[x.LA_INFO_ID == x.LA_INFO_ID.min()].SUM_ASSURED)) #.SUM_ASSURED.sum()\n",
        "# df.merge(temp, on=['c','d'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSaaP1ZY0wb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGs5x0A3pPc",
        "colab_type": "text"
      },
      "source": [
        "**transform**\n",
        "\n",
        "---\n",
        "AFAIU I would change the lambda function as follows:\n",
        "```python\n",
        "df.groupby('Plate')['LogRatio'].transform(lambda s: s.loc[[True if v < s.quantile(q=0.8) and v > s.quantile(q=0.2) else False for v in s]].mean())\n",
        "```\n",
        "This the ```s.loc[]``` accepts an iterable with booleans in order to subset the LogRatio-Series\n",
        "\n",
        "In order to make it more readable, I'd go for the following solution:\n",
        "```python\n",
        "def quartile_subset(logratios,lower,upper):\n",
        "    # some comment to describe what you are doing\n",
        "    return logratios.loc[[True if v < logratios.quantile(q=upper) and v > logratios.quantile(q=lower) else False for v in logratios]]\n",
        "\n",
        "df.groupby('Plate')['LogRatio'].transform(lambda s: quartile_subset(s,0.2,0.8).mean())\n",
        "```\n",
        "\n",
        "---\n",
        "You can use GroupBy + transform with sum twice:\n",
        "```python\n",
        "df['e'] = df.groupby(['c', 'd'])[['a', 'b']].transform('sum').sum(1)\n",
        "\n",
        "print(df)\n",
        "   a  b  c  d   e\n",
        "0  1  1  q  z  12\n",
        "1  2  2  q  z  12\n",
        "2  3  3  q  z  12\n",
        "3  4  4  q  o   8\n",
        "4  5  5  w  o  22\n",
        "5  6  6  w  o  22\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W3wclF0Myj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSX93l7md6QW",
        "colab_type": "text"
      },
      "source": [
        "### crosstab/ pivot_table\n",
        "- https://pbpython.com/pandas-crosstab.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcFpKNKUnEoX",
        "colab_type": "text"
      },
      "source": [
        "### binning\n",
        "\n",
        "- https://pbpython.com/pandas-qcut-cut.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCqovAyHnEDB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bins = [0, 3, 6,9, 12]\n",
        "labels = ['1', '2', '3', '4']\n",
        "t['PM_Promo_Time_bins'] = pd.cut(t['PM_Promo_Time'], bins, labels=labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcFdwO8Cm9wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhtHY720NEw_",
        "colab_type": "text"
      },
      "source": [
        "## lag/roll\n",
        "- https://pandas.pydata.org/pandas-docs/version/0.21.1/generated/pandas.DataFrame.rolling.html\n",
        "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html\n",
        "- https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPqdQ76Zd9Z8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = t.set_index('YearMonth_').groupby(['LIFCNUM_']).rolling(window=365, freq='D',min_periods=1).agg({\n",
        "    \"UAB_PeriodSum\":'sum'    \n",
        "}).reset_index()\n",
        "t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_3Ircfld9TU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAOrwObgHbzF",
        "colab_type": "text"
      },
      "source": [
        "## sort\n",
        "\n",
        "## **sort a columns with custom sorter index**\n",
        "- https://stackoverflow.com/questions/23482668/sorting-by-a-custom-list-in-pandas\n",
        "- https://stackoverflow.com/questions/13838405/custom-sorting-in-pandas-dataframe\n",
        "- https://thispointer.com/pandas-sort-rows-or-columns-in-dataframe-based-on-values-using-dataframe-sort_values/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtuJAzIUHdou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPY955ebHdHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZwKNHDtmQ4h",
        "colab_type": "text"
      },
      "source": [
        "## where/select/query\n",
        "- https://kanoki.org/2020/01/21/pandas-dataframe-filter-with-multiple-conditions/\n",
        "- https://stackoverflow.com/questions/49228596/pandas-case-when-default-in-pandas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ee3X1YmR_9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1['a'] = np.select(\n",
        "        [\n",
        "            df1.AGLevel_flow.str.contains('AG;PM'),      \n",
        "        ],\n",
        "        [\n",
        "           df1.YearMonth_flow.str.split(\";\").str[1]\n",
        "        ]\n",
        "    ,        default = (pd.to_datetime(df1.First_month, format='%Y%m') + pd.DateOffset(months=6)).dt.strftime('%Y%m')\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoY-KoHXmSHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1['Promo_YearMonth'] = np.select(\n",
        "        [\n",
        "            df1.AGLevel_flow.isin((\"AG;PM;AG;PM;UM;PM\",\"AG;PM;AG;PM;UM;AG\",\n",
        "                                            \"AG;PM;AG;PM;UM;BM\",\"AG;PM;AG;PM;UM;PM;AG\",\"AG;PM;AG;PM;UM\"))\n",
        "            ,df1.AGLevel_flow == \"AG;UM\"\n",
        "            ,df1.AGLevel_flow == \"AG;PM;AG;PM;AG;PM;UM\"\n",
        "            ,df1.AGLevel_flow.isin((\"UM\",\"UM;AG\"))\n",
        "        ],\n",
        "        [\n",
        "           df1.YearMonth_flow.str.split(\";\").str[4]\n",
        "            ,df1.YearMonth_flow.str.split(\";\").str[1]\n",
        "            ,df1.YearMonth_flow.str.split(\";\").str[6]\n",
        "            ,df1.YearMonth_flow\n",
        "        ]\n",
        "    ,        default = df1.YearMonth_flow.str.split(\";\").str[2]\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHdSOvWumSKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_term = 1\n",
        "factor_1.query('(`Benefit term` == @query_term)').Factor_others"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BZls0VxFtYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t['New_Decision'] = np.where(t['LA\\'s Decision'].isnull()==False,t['LA\\'s Decision']\n",
        "         ,np.select(\n",
        "             [\n",
        "                ( (t.FACTOR5=='YES-UW') | (t.FACTOR3=='YES') | (t.FACTOR4=='YES')  | (t.FACTOR5=='YES')  \n",
        "                 | (t.FACTOR6=='YES')  | (t.FACTOR7=='YES')  | (t.FACTOR8=='YES') | (t.FACTOR8=='YES-UW') \n",
        "                 | (t.FACTOR9=='YES') )                 \n",
        "             ],\n",
        "             [\n",
        "                 'refer UW'                 \n",
        "             ]\n",
        "            \n",
        "         )\n",
        "        )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAD8B3cIFtTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWm9sbjolESN",
        "colab_type": "text"
      },
      "source": [
        "## Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayyixlm1lD3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['Age'] = round((pd.to_datetime(df.Promo_YearMonth, format='%Y%m') - \n",
        "                   pd.to_datetime(df.DateOfBirth, format='%Y/%m/%d')) / np.timedelta64(1, 'Y'),0)\n",
        "\n",
        "df['New_InvoiceDate'] = pd.to_datetime(df.InvoiceDate, format='%d/%m/%Y %H:%M').dt.strftime('%Y%m')\n",
        "\n",
        "(pd.to_datetime(df_analyze.NewRe_1st_YearMonth_End.fillna(200001), format='%Y%m') \n",
        "                                              + pd.DateOffset(months=1)).dt.strftime('%Y%m').astype(int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqO8pJBblHFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJUeNk50VTOP",
        "colab_type": "text"
      },
      "source": [
        "# 4.Visualization\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuLwfvj3LvGi",
        "colab_type": "text"
      },
      "source": [
        "## Notes\n",
        "- [note 1](https://www.kaggle.com/sonannguyenngoc/titanic-survival-prediction-end-to-end-ml-pipeline?scriptVersionId=33324915#Visualizations-&-Analyze), [note 2]()\n",
        "- [matplotlib guide](https://www.kaggle.com/grroverpr/matplotlib-plotting-guide), [top 50 matplotlib](https://nextjournal.com/sosiristseng/top-50-matplotlib-visualizations#categorical-plots)\n",
        "- [seaborn guide](https://www.kaggle.com/kralmachine/seaborn-tutorial-for-beginners), [seaborn 1](https://seaborn.pydata.org/tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUdukOL-nnMp",
        "colab_type": "text"
      },
      "source": [
        "## Plot continous features\n",
        "to see:\n",
        "- distribution\n",
        "- quantile\n",
        "- outlier identified by IQR\n",
        "- distribution & quantile after remove outlier outside IQR\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLva7-YEm7x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"Plotting the continous features :    \n",
        "1. A box plot (or box-and-whisker plot) shows the distribution of quantitative data \n",
        "in a way that facilitates comparisons between variables.\n",
        "2. Distribution graph :to check the linearity of the variables and look \n",
        "for skewness of features.\"\"\"\n",
        "def plot_continous(cols, df, target=None, info = False, corr = False, drop_3Qplot=False):\n",
        "    # Using boxplot to analyze the continous feature\n",
        "    if corr == True:\n",
        "      df_corr = df.corr().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
        "      df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}\n",
        "                , inplace=True)      \n",
        "\n",
        "    for col in cols:\n",
        "        print(f'\\n >Column name: {col}')\n",
        "        plt.figure(figsize=(15,8))\n",
        "        \n",
        "        if corr == True:\n",
        "          print(\"Top 5 positive correlated w/ featuers \\n{}\".format(\n",
        "              df_corr[(df_corr[\"Feature 1\"]==col) & (df_corr[\"Feature 2\"]!=col)].head()))\n",
        "          print(\"Top 5 negative correlated w/ featuers \\n{}\".format(\n",
        "              df_corr[(df_corr[\"Feature 1\"]==col) & (df_corr[\"Feature 2\"]!=col)].tail()))\n",
        "        if info == True:\n",
        "\n",
        "          print(\"max: {}, min: {}, mean: {}, std: {}, median: {}\".format(\n",
        "              df[col].max(),df[col].min(),\n",
        "              round(df[col].mean(),3),round(df[col].std(),3),\n",
        "              df[col].median()))\n",
        "          print(f\"Quantiles | {col}\\n\",df[col].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))\n",
        "          print(\"No unique: {}, No null: {}\".format(df[col].nunique(),df[col].isnull().sum()))\n",
        "          if df[col].nunique() < 10 :\n",
        "            print(f\"Value counts in {col} \\n\",format(df[col].value_counts()))\n",
        "\n",
        "        # plot  \n",
        "        plt.subplot(2, 2, 1)\n",
        "        if target == None:\n",
        "            fig = sns.boxplot(col, whis=1.5, data=df)\n",
        "            fig.legend()\n",
        "        else:\n",
        "            fig = sns.boxplot(x=target, y=col, whis=1.5, data=df)\n",
        "            fig.legend()\n",
        "        # which defined as the proportion of the IQR past the low and high quartiles to extend the plot whiskers \n",
        "        # or interquartile range (IQR)\n",
        "        # therefore, maximum = Q3 + 1.5*IQR , min = Q1 - 1.5*IQR\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1    \n",
        "        \n",
        "        plt.subplot(2, 2, 2)\n",
        "        fig = sns.distplot(df[col].dropna())#.hist(bins=20)\n",
        "        fig.set_ylabel('Volumn')\n",
        "        fig.set_xlabel(col)      \n",
        "        \n",
        "        print('No data out of range between Q1 : {} and Q3 : {} = {}' .format(Q1,Q3,((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()))\n",
        "\n",
        "        if drop_3Qplot==True:          \n",
        "          plt.subplot(2, 2, 3)\n",
        "          tmp = df[(df[col] >= Q1)&(df[col] <= Q3)]#[col]\n",
        "          if target == None:\n",
        "              fig = sns.boxplot(col, whis=1.5, data=tmp)\n",
        "              fig.legend()\n",
        "          else:\n",
        "              fig = sns.boxplot(x=target, y=col, whis=1.5, data=tmp)\n",
        "              fig.legend()\n",
        "          \n",
        "          plt.subplot(2, 2, 4)\n",
        "          fig = sns.distplot(tmp[col])#.dropna())#.hist(bins=20)\n",
        "          fig.set_ylabel('Volumn')\n",
        "          fig.set_xlabel(col)   \n",
        "\n",
        "        plt.show() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcyJ6xFLmJqM",
        "colab_type": "text"
      },
      "source": [
        "## Plot categorical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72WTuvLn_aZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_categorical(cols, target, df):\n",
        "  for col in cols:\n",
        "    if target == None:\n",
        "      sns.countplot(x=col, data=df[[col]] ,  palette=\"Reds_d\")      \n",
        "    else:\n",
        "      print('Column name: %s' %col)\n",
        "      sns.countplot(x=col, hue=target, data=df[[col,target]], palette=\"Reds_d\")\n",
        "      #sns.barplot(x=col, y=target, data=df[[col,target]]\n",
        "      #        , palette=\"Reds_d\", estimator = sum)\n",
        "      plt.legend()\n",
        "    plt.xticks(rotation=90)#-60    \n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEqPnlPhet0E",
        "colab_type": "text"
      },
      "source": [
        "## 3D plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5D_h94Fe2SF",
        "colab_type": "text"
      },
      "source": [
        "Sources:\n",
        "- https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html\n",
        "- https://matplotlib.org/3.1.1/gallery/index.html\n",
        "\n",
        "\n",
        "Example:\n",
        "- [how-to-surface-plot-3d-plot-from-dataframe](https://stackoverflow.com/questions/36589521/how-to-surface-plot-3d-plot-from-dataframe)\n",
        "- [plotting-pandas-crosstab-dataframe-into-3d-bar-chart](https://stackoverflow.com/questions/56336066/plotting-pandas-crosstab-dataframe-into-3d-bar-chart)\n",
        "- [matplotlib-3d-surface-from-a-rectangular-array-of-heights](https://stackoverflow.com/questions/11766536/matplotlib-3d-surface-from-a-rectangular-array-of-heights)\n",
        "- [surface-plots-in-matplotlib](https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffvdG1_VoIDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM5mJUJac0Jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3W9LBvdRc0Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFEureNMc0Pq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}