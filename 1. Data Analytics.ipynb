{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/1.%20Data%20Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MPe_mkDIJZM1"
   },
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "4mO6Wp3YBXYh"
   },
   "source": [
    ">[Table of contents](#scrollTo=MPe_mkDIJZM1)\n",
    "\n",
    ">[Sources](#scrollTo=BvORvDCvAdH6)\n",
    "\n",
    ">[1.EDA](#scrollTo=ltyLBhWQm2YF)\n",
    "\n",
    ">>[Lib/package](#scrollTo=ovlz0rlqPlz0)\n",
    "\n",
    ">>[Resume & Phân tích df](#scrollTo=pmXBVoIJDDjN)\n",
    "\n",
    ">>[Correlation](#scrollTo=DEhCkATJnX0l)\n",
    "\n",
    ">>[Statistical Checking](#scrollTo=TdqQPJ9zuPAh)\n",
    "\n",
    ">>[Bayesian Statistic](#scrollTo=foaLMVH2wNDH)\n",
    "\n",
    ">[2.Cleaning Data](#scrollTo=arothx_v_lC-)\n",
    "\n",
    ">>[Nan/Inf value](#scrollTo=FSzLvImTnBdI)\n",
    "\n",
    ">>[Outliers](#scrollTo=G9dkPpmLmkDE)\n",
    "\n",
    ">[3.Features Processing](#scrollTo=81RBd7sRk0fu)\n",
    "\n",
    ">>[str / NLP processing](#scrollTo=OkQAo4CboTLF)\n",
    "\n",
    ">>[merge/concat](#scrollTo=pEwO7OIEmZ-B)\n",
    "\n",
    ">>[group by](#scrollTo=GZ-VCSnElcgU)\n",
    "\n",
    ">>>[crosstab/ pivot_table](#scrollTo=hSX93l7md6QW)\n",
    "\n",
    ">>>[binning](#scrollTo=OcFpKNKUnEoX)\n",
    "\n",
    ">>>[lag/roll](#scrollTo=EhtHY720NEw_)\n",
    "\n",
    ">>[where/select/query / iloc/ix](#scrollTo=0ZwKNHDtmQ4h)\n",
    "\n",
    ">>>[sort](#scrollTo=yOhVJVEyPV7u)\n",
    "\n",
    ">>[sort a columns with custom sorter index](#scrollTo=yOhVJVEyPV7u)\n",
    "\n",
    ">>[Time](#scrollTo=yWm9sbjolESN)\n",
    "\n",
    ">[4.Visualization](#scrollTo=zJUeNk50VTOP)\n",
    "\n",
    ">>[Notes](#scrollTo=wuLwfvj3LvGi)\n",
    "\n",
    ">>[3D plot](#scrollTo=UEqPnlPhet0E)\n",
    "\n",
    ">[5.Rules Baed](#scrollTo=_QkWaspdKfaX)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "colab_type": "code",
    "id": "JNimUPIBigtD",
    "outputId": "9f9624ce-279e-4275-8b70-d705c63cc2e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://stackedit.io/app#\" width=\"1000\" height=\"600\"></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://stackedit.io/app#\" width=\"800\" height=\"600\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hmtsCrqtu-rI"
   },
   "source": [
    "- [**A Comprehensive Guide to Data Exploration**](https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/)\n",
    "- [Data Analytics Collection - Hasbrain](https://note.hasbrain.com/5c875913b40548001824aa67?target=path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ltyLBhWQm2YF"
   },
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovlz0rlqPlz0"
   },
   "source": [
    "## Lib/package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RlRv2BdPpwq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "pd.set_option('display.max_rows', 1000)  \n",
    "pd.set_option('display.max_columns', 100)  \n",
    "pd.options.display.float_format = '{:,.3f}'.format  \n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline  \n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "sns.set(font_scale=1)\n",
    "\n",
    "import gc  \n",
    "import time  \n",
    "import warnings  \n",
    "warnings.filterwarnings('ignore')  \n",
    "import string  \n",
    "import pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBeDkVS1QOKj"
   },
   "outputs": [],
   "source": [
    "def Diff(list1, list2):  \n",
    "return (list(list(set(list1)-set(list2)) + list(set(list2)-set(list1))))\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"--- {} mins --- \\n\".format(round((time.time() - start_time)/60,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FphbDZnHP7Oy"
   },
   "outputs": [],
   "source": [
    "dtype_dict = {'A': 'object',  \n",
    "              'B': 'int64',}  \n",
    "df = pd.read_csv('df.csv', dtype=dtype_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmXBVoIJDDjN"
   },
   "source": [
    "## Resume & Phân tích df\n",
    "Descriptive statistics are used to describe the basic features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data.\n",
    "- Phân tích về min max, mean, median, mode, std\n",
    "- Phân tích về loại của feature (continous/categorical, numeric/text)\n",
    "- Đếm giá trị unique ở features/ giá trị missing \n",
    "\n",
    "**Sources**:\n",
    "- [Giải thích các biến số cơ bản](https://towardsdatascience.com/understanding-descriptive-statistics-c9c2b0641291)\n",
    "- [Giải thích entropy](https://machinelearningmastery.com/what-is-information-entropy/)\n",
    "  - [1](https://stackoverflow.com/questions/26743201/interpreting-scipy-stats-entropy-values) về căn bản entropy của norm(1,0.5) với norm(1,0.5) =0. Còn entropy của norm(1,0.5) với norm(1.5,0.5) sẽ bằng >0\n",
    "  - ```python\n",
    "t1 = stats.norm(2.5, 0.1)\n",
    "t2 = stats.norm(2.5, 0.1)\n",
    "t3 = stats.norm(2.4, 0.1)\n",
    "t4 = stats.norm(2, 0.1)\n",
    "# domain to evaluate PDF on\n",
    "x = np.linspace(-5, 5, 100)\n",
    "print(stats.entropy(t1.pdf(x), t2.pdf(x)))\n",
    "print(stats.entropy(t1.pdf(x), t3.pdf(x)))\n",
    "print(stats.entropy(t1.pdf(x), t4.pdf(x)))\n",
    "  ```\n",
    "  - ```python\n",
    "stats.entropy(df[name].value_counts(normalize=True), base=2)\n",
    "  ``` \n",
    "  theo [scipy.stat. document](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.entropy.html), [ex2](https://stackoverflow.com/questions/57165569/scipy-stats-entropy-is-giving-a-different-result-to-entropy-formula), lệnh value_counts(normalize=True) trả về xác xuất xảy ra giá trị trong tất cả giá trị, tính entropy của xác suất xảy ra từng giá trị trong 1 cột theo Bernoulli trial with different p. entropy >= 1 ở trường hợp này nghĩa là\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UB4wn9OSlKw5"
   },
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "WOE_numeric_col = [x for x in df_WOE.select_dtypes(include=numerics).columns\n",
    "              if x not in label+ ag_id+ time\n",
    "              ]\n",
    "WOE_cat_col = [x for x in df_WOE.columns \n",
    "           if x not in WOE_numeric_col + label + ag_id + time]\n",
    "\n",
    "ratio_col = [x for x in WOE_numeric_col if 'ratio' in x]\n",
    "\n",
    "print(' WOE_numeric_col: ', WOE_numeric_col)\n",
    "print('\\n WOE_cat_col: ', WOE_cat_col)\n",
    "print('\\n ratio_col: ', ratio_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91jE-tpqCsjb"
   },
   "outputs": [],
   "source": [
    "# This function assits to expand Exploratory Data Analysis (EDA) is an open-ended \n",
    "#process where we calculate statistics and make figures to find trends, anomalies,\n",
    "#patterns, or relationships within the data. The goal of EDA is to learn what our\n",
    "#data can tell us. It generally starts out with a high level overview, \n",
    "#then narrows in to specific areas as we find intriguing areas of the data. \n",
    "\n",
    "def des_stat_analyze(df_input, get_sample=False):\n",
    "    # check number of rows, cols\n",
    "    no_rows = df_input.shape[0]\n",
    "    no_cols = df_input.shape[1]\n",
    "    print(\"No. observations:\", no_rows )\n",
    "    print(\"No. features:\", no_cols )\n",
    "  \n",
    "    # checking type of features\n",
    "    name = []\n",
    "    cols_type = []\n",
    "    for n,t in df_input.dtypes.iteritems():\n",
    "        name.append(n)\n",
    "        cols_type.append(t)\n",
    "\n",
    "    # checking distinction (unique values) of features\n",
    "    ls_unique = []\n",
    "    for cname in df_input.columns:\n",
    "        try:\n",
    "            nunique = df_input[cname].nunique()\n",
    "            pct_unique = nunique*100.0/ no_rows\n",
    "            ls_unique.append(\"{} ({:0.2f}%)\".format(nunique, pct_unique))\n",
    "        except:\n",
    "            ls_unique.append(\"{} ({:0.2f}%)\".format(0,0))\n",
    "            continue\n",
    "\n",
    "    # checking missing values of features\n",
    "    ls_miss = []\n",
    "    for cname in df_input.columns:\n",
    "        try:\n",
    "            nmiss = df_input[cname].isnull().sum()\n",
    "            pct_miss = nmiss*100.0/ no_rows\n",
    "            ls_miss.append(\"{} ({:0.2f}%)\".format(nmiss, pct_miss))\n",
    "        except:\n",
    "            ls_miss.append(\"{} ({:0.2f}%)\".format(0,0))\n",
    "            continue \n",
    "      \n",
    "    # checking zeros\n",
    "    ls_zeros = []\n",
    "    for cname in df_input.columns:\n",
    "        try:\n",
    "            nzeros = (df_input[cname] == 0).sum()\n",
    "            pct_zeros = nzeros * 100.0/ no_rows\n",
    "            ls_zeros.append(\"{} ({:0.2f}%)\".fornat(nzeros, pct_zeros))\n",
    "        except:\n",
    "            ls_zeros.append(\"{} ({:0.2f}%)\".format(0,0))\n",
    "            continue\n",
    "      \n",
    "    # checking negative values\n",
    "    ls_neg = []\n",
    "    for cname in df_input.columns:\n",
    "        try:\n",
    "            nneg = (df_input[cname].astype(\"float\")<0).sum()\n",
    "            pct_neg =nneg * 100.0 / no_rows\n",
    "            ls_neg.append(\"{} ({:0.2f}%)\".format(nneg, pct_neg))\n",
    "        except:\n",
    "            ls_neg.append(\"{} ({:0.2f}%)\".format(0,0))\n",
    "            continue\n",
    "      \n",
    "    # extracting the output\n",
    "    data = {\n",
    "      \"name\": name,\n",
    "      \"col_type\": cols_type,\n",
    "      \"n_unique\": ls_unique,\n",
    "      \"n_miss\": ls_miss,\n",
    "      \"n_zeros\":ls_zeros,\n",
    "      \"n_neg\":ls_neg      \n",
    "    }\n",
    "  \n",
    "    # statistical info\n",
    "    df_stats = df_input.describe().transpose()\n",
    "    ex_stats = pd.concat([df_input.median(),df_input.kurtosis(),df_input.skew()], axis = 1)\n",
    "    ex_stats = ex_stats.rename(columns={0:\"median\",1:\"kurtosis\",2:\"skew\"})\n",
    "    df_stats = df_stats.merge(ex_stats, left_index=True, right_index=True)\n",
    "    #ls_stats = []\n",
    "    for stat in df_stats.columns:\n",
    "        data[stat] = []\n",
    "        for cname in df_input.columns:\n",
    "            try:\n",
    "                data[stat].append(df_stats.loc[cname, stat])\n",
    "            except:\n",
    "                data[stat].append(\"NaN\")\n",
    "        \n",
    "    # take samples\n",
    "    if get_sample:\n",
    "        df_sample = df_input.sample(frac = .5).head().transpose()\n",
    "        df_sample.columns = [\"sample_{}\".format(i) for i in range(5)]\n",
    "\n",
    "    \n",
    "    # repair the output\n",
    "    col_ordered = [\"name\",\"col_type\",\"count\",\"n_unique\",\"n_miss\",\"n_zeros\",\"n_neg\",\n",
    "                \"25%\",\"50%\",\"75%\",\"max\",\"min\",\"mean\",\"median\",\"std\",\"kurtosis\",\"skew\"]\n",
    "    df_data = pd.DataFrame(data, columns = col_ordered).set_index(\"name\")\n",
    "    if get_sample:\n",
    "        df_data = pd.concat([df_data, df_sample], axis = 1)\n",
    "    df_data = df_data.reset_index().sort_values(by=['col_type'])  \n",
    "    # entropy\n",
    "    for col in name:\n",
    "        #print(col)\n",
    "        df_data.loc[df_data['name'] == col, 'Entropy'] = round(stats.entropy(df_input[col].value_counts(normalize=True), base=2),2) \n",
    "    \n",
    "\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8_Vn17UQPKQn"
   },
   "outputs": [],
   "source": [
    "def resumetable(df):\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n",
    "    summary = summary.reset_index()\n",
    "    summary['Name'] = summary['index']\n",
    "    summary = summary[['Name','dtypes']]\n",
    "    summary['Missing'] = df.isnull().sum().values    \n",
    "    summary['Uniques'] = df.nunique().values\n",
    "    summary['Max'] = df.max().values\n",
    "    summary['Min'] = df.min().values\n",
    "    summary['Mean'] = df.mean().values\n",
    "    summary['Var'] = df.std().values\n",
    "    summary['Q1'] = df.quantile(0.25).values\n",
    "    summary['Q3'] = df.quantile(0.75).values    \n",
    "    \n",
    "    \n",
    "    summary['First Value'] = df.loc[0].values\n",
    "    summary['Second Value'] = df.loc[1].values\n",
    "    summary['Third Value'] = df.loc[2].values\n",
    "#     summary['Sample 1'] = df.sample(1).values\n",
    "#     summary['Sample 2'] = df.sample(1).values\n",
    "#     summary['Sample 3'] = df.sample(1).values\n",
    "    \n",
    "\n",
    "    for name in summary['Name'].value_counts().index:\n",
    "        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n",
    "    summary.sort_values(by=['Name', 'dtypes'])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhDbpTYweqyx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tìm các bất ổn về missing value và các cột có bật ổn về số lượng giá trị\n",
    "'''\n",
    "many_null_cols = [col for col in df_train.columns if df_train[col].isnull().sum() / df_train.shape[0] > 0.9]\n",
    "many_null_cols_test = [col for col in df_test.columns if df_test[col].isnull().sum() / df_test.shape[0] > 0.9]\n",
    "\n",
    "\n",
    "big_top_value_cols = ([col for col in train.columns if \n",
    "                       df_train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9])\n",
    "big_top_value_cols_test = ([col for col in test.columns if \n",
    "                            df_test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9])\n",
    "one_value_cols = [col for col in df.columns if df[col].nunique() <= 1]\n",
    "one_value_cols_test = [col for col in df_test.columns if df_test[col].nunique() <= 1]\n",
    "one_value_cols == one_value_cols_test\n",
    "\n",
    "cols_to_drop = list(set(many_null_cols + many_null_cols_test + \n",
    "                        big_top_value_cols + big_top_value_cols_test + \n",
    "                        one_value_cols + one_value_cols_test))\n",
    "cols_to_drop.remove('isFraud')\n",
    "len(cols_to_drop)\n",
    "\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AQpCJG3ZTf9K"
   },
   "outputs": [],
   "source": [
    "def percent_groupby(df,col,label,sort_by_per=True):\n",
    "    '''\n",
    "    tìm percent của label theo groupby từng categories của 1 cột\n",
    "    '''\n",
    "    t1 = df.groupby(col).agg({\n",
    "        label:['sum','size']\n",
    "    }).reset_index()\n",
    "    t1.columns = [''.join(col) for col in t1.columns]\n",
    "    t1['label_percent'] = round(t1[label+'sum']/t1[label+'size'],2)\n",
    "    if sort_by_per:\n",
    "        return t1.sort_values(by=['label_percent'])    \n",
    "    else:\n",
    "        return t1.sort_values(by=col)    \n",
    "'''\n",
    "percent_groupby(df,['a','b'],'label')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3B5e1itQVi2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwM3amu4Tf1a"
   },
   "outputs": [],
   "source": [
    "def find_perLABEL(df,cols,label = 'LABEL'):\n",
    "    '''\n",
    "    Tính percent của LABEL == 1 trên \n",
    "    2 nhóm dữ liệu phụ thuộc vào null and notnull của 1 features\n",
    "    '''\n",
    "    result = pd.DataFrame(columns=['features','no_nan','per_nan','df_Nan_perLABEL','df_NotNan_perLABEL'])\n",
    "    for col in cols:\n",
    "        fillter = df[[col,label]][df[col].isnull()==True]\n",
    "        no_nan = len(fillter.index)\n",
    "        per_nan = round(len(fillter.index)/len(df.index),2)\n",
    "        dfnan_perLABEL = round(fillter[label].sum()/len(fillter.index),2)\n",
    "        fillter = df[[col,label]][df[col].isnull()==False]\n",
    "        dfnotnan_perLABEL = round(fillter[label].sum()/len(fillter.index),2)\n",
    "        \n",
    "        result = result.append({'features': col\n",
    "                                ,'no_nan':no_nan\n",
    "                                ,'per_nan':per_nan\n",
    "                                ,'df_Nan_perLABEL': dfnan_perLABEL\n",
    "                                , 'df_NotNan_perLABEL': dfnotnan_perLABEL\n",
    "                               },ignore_index=True)\n",
    "        \n",
    "    return result.sort_values(by=['per_nan'], ascending=False)\n",
    "\n",
    "# find_perLABEL(df,cols,label = 'LABEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DEhCkATJnX0l"
   },
   "source": [
    "## Correlation\n",
    "\n",
    "Xem correlation của các features với nhau: \n",
    "- 2 features có chỉ số correlation dương, có nghĩa khi A tăng thì B tăng (if A increase, B will increase)\n",
    "- 2 features có chỉ số correlation âm, có nghĩa khi A tăng thì B giảm (if A increase, B will decrease)\n",
    "- - 2 features có chỉ số correlation cao, có nghĩa là 2 features này có khả năng thay thế nhau trong model (replaces)\n",
    "\n",
    "Sources:\n",
    "-  [Covariance vs Correlation](https://forum.machinelearningcoban.com/t/covariance-va-correlation/767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CGrqIFBwDAUN"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "simple\n",
    "'''\n",
    "corr=df_model.corr()#[\"Survived\"]\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "sns.heatmap(corr, vmax=.8, linewidths=0.01,\n",
    "            square=True,annot=True,cmap='YlOrRd',linecolor=\"white\")\n",
    "plt.title('Correlation between features');\n",
    "\n",
    "'''\n",
    "function\n",
    "'''\n",
    "def plot_corr(df):\n",
    "    correlations = df.corr()\n",
    "\n",
    "    # Using seaborn package\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.zeros_like(correlations, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(260, 10, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlations, mask=mask, cmap=cmap, vmin = -1, vmax= 1 , center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.yticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TkVsIzFm7mr"
   },
   "outputs": [],
   "source": [
    "def find_topcorr_bycol(col):\n",
    "    df_corr = df.corr().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "    df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}\n",
    "                , inplace=True)     \n",
    "    return df_corr[df_corr[\"Feature 1\"]==col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rosahxWZdAEa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TdqQPJ9zuPAh"
   },
   "source": [
    "## Statistical Checking\n",
    "Các kỹ thuật khác trong việc kiểm tra về mặt thống kê\n",
    "\n",
    "Source:\n",
    "- [Statistical Learning oxh](https://ongxuanhong.wordpress.com/category/kien-thuc/statistical-inference/)\n",
    "- [Video về Statistic](https://www.youtube.com/user/drnguyenvtuan/videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "foaLMVH2wNDH"
   },
   "source": [
    "## Bayesian Statistic\n",
    "- [How Bayesian statistics convinced me to hit the gym](https://towardsdatascience.com/how-bayesian-statistics-convinced-me-to-hit-the-gym-fa737b0a7ac)\n",
    "- [Think you need to learn Bayesian Analysis? Read this first](https://peadarcoyle.com/2019/01/01/think-you-need-to-learn-bayesian-analysis-read-this-first/?fbclid=IwAR1KGEuQNfzzf0BexNampJsyZ3zNgcfzDOgNMsPvppjElAu4qOOPF8AwC7g)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xzrRN4mdAL1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arothx_v_lC-",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Cleaning Data\n",
    "\n",
    "Used in:\n",
    "  - Kaggle:[ Fraud Detection](https://www.kaggle.com/sonannguyenngoc/fraud-pca/edit/run/18581236) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FSzLvImTnBdI"
   },
   "source": [
    "## Nan/Inf value\n",
    "- fill na : https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7v9q8-S-nEBx"
   },
   "outputs": [],
   "source": [
    "# Tìm na/nan/null trong từng cột\n",
    "# Hiện thị: Số na, %na, type\n",
    "# GPreda, missing data\n",
    "def missing_data(data):\n",
    "    total = data.isnull().sum()\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100)\n",
    "    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    types = []\n",
    "    for col in data.columns:\n",
    "        dtype = str(data[col].dtype)\n",
    "        types.append(dtype)\n",
    "    tt['Types'] = types\n",
    "    return(np.transpose(tt))\n",
    "\n",
    "df['Count_miss'] = df[col].isnull().sum(axis=1)\n",
    "\n",
    "df['nulls1'] = df.isna().sum(axis=1)\n",
    "\n",
    "# đếm not null\n",
    "df['a'].notnull().sum()\n",
    "\n",
    "\n",
    "\n",
    "# đếm số lượng bằng 0 cho mỗi dòng trên toàn bộ data\n",
    "t1['count_zero'] = (t1 == 0).astype(int).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00zcRzYACpo6"
   },
   "outputs": [],
   "source": [
    "# tìm infinite\n",
    "np.isinf(df[ratio_col]).sum()\n",
    "\n",
    "# tìm dòng bị infinite\n",
    "t = np.isinf(df).sum().reset_index()\n",
    "t[t[0]>0]\n",
    "\n",
    "# replace infinite bằng null\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FjWMnkr_AO1E"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "1 số cách fill NA\n",
    "interpolate from categorical features\n",
    "'''\n",
    "\n",
    "df['MEMBER_MARITAL_STATUS'] = df['MEMBER_MARITAL_STATUS'].fillna('UNK')\n",
    "df['MEMBER_MARITAL_STATUS'] = df['MEMBER_MARITAL_STATUS'].replace({'S':'S-W-D','W':'S-W-D','D':'S-W-D'})\n",
    "\n",
    "\n",
    "##################################################################################\n",
    "'''\n",
    "FILL MISSING BẰNG GROUP BY\n",
    "'''\n",
    "\n",
    "df['MEMBER_ANNUAL_INCOME'] = df.groupby(['MEMBER_OCCUPATION_CD','MEMBER_MARITAL_STATUS','MEMBER_GENDER'])\n",
    "                              \\ ['MEMBER_ANNUAL_INCOME'].apply(lambda x: x.fillna(x.median()))  \n",
    "\n",
    "titanic_all.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\n",
    "\n",
    "\n",
    "titanic_all['Fare'] = titanic_all['Fare'].fillna(titanic_all.groupby(\n",
    "    ['Pclass', 'Parch', 'SibSp']).Fare.transform('mean'))\n",
    "\n",
    "df.groupby('name')['activity'].value_counts().unstack().fillna(0)\n",
    "\n",
    "##################################################################################\n",
    "'''\n",
    "fill missing cua AVERAGE INCOME bang trung binh cua 1 cột categorical OCCUPATION\n",
    "'''\n",
    "df['OCCUAPATION_concat_g1'] = np.where(df.OCCUAPATION_concat.isin(df.OCCUAPATION_concat.value_counts().index[0:20].tolist()),df.OCCUAPATION_concat,'other')\n",
    "df.AVERAGE_INCOME.fillna(df.groupby(['OCCUAPATION_concat_g1']).AVERAGE_INCOME.transform('mean'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pJbGg1lePffW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3Alus3s2GaA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AhgqCpzs2HVO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G9dkPpmLmkDE"
   },
   "source": [
    "## Outliers\n",
    "\n",
    "[estimation cdf - plot](https://www.jddata22.com/home//plotting-an-empirical-cdf-in-python)\n",
    "  - check outlier by std and mean\n",
    "  \n",
    "[boxplot & quanlite](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)\n",
    "  - check outlier by \n",
    "    - IQR: interquantile range = Q3 - Q1\n",
    "    - “maximum”: Q3 + 1.5*IQR\n",
    "    - “minimum”: Q1 -1.5*IQR\n",
    "\n",
    "z-score\n",
    "\n",
    "\n",
    "**News**\n",
    "- [Ways to Detect and Remove the Outliers](https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba)\n",
    "- [Three ways to detect outliers](http://colingorrie.github.io/outlier-detection.html)\n",
    "- [How to Use Statistics to Identify Outliers in Data](https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/)\n",
    "- [5 Ways to Find Outliers in Your Data](https://statisticsbyjim.com/basics/outliers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DReevHMvb0Qa"
   },
   "outputs": [],
   "source": [
    "def CalcOutliers(df_num): \n",
    "    '''\n",
    "    Find Outliers for continous cols\n",
    "    Tìm outliner dựa trên std, và mean\n",
    "\n",
    "    Sau khi phân tích quantile và mean, std\n",
    "    có thể biết outlier nằm ngoài vùng nào:\n",
    "    vd: cut = std * 3\n",
    "    '''\n",
    "    # calculating mean and std of the array\n",
    "    data_mean, data_std = np.mean(df_num), np.std(df_num)\n",
    "\n",
    "    # seting the cut line to both higher and lower values\n",
    "    # You can change this value\n",
    "    cut = data_std * 3\n",
    "\n",
    "    #Calculating the higher and lower cut values\n",
    "    lower, upper = data_mean - cut, data_mean + cut\n",
    "\n",
    "    # creating an array of lower, higher and total outlier values \n",
    "    outliers_lower = [x for x in df_num if x < lower]\n",
    "    outliers_higher = [x for x in df_num if x > upper]\n",
    "    outliers_total = [x for x in df_num if x < lower or x > upper]\n",
    "\n",
    "    # array without outlier values\n",
    "    outliers_removed = [x for x in df_num if x > lower and x < upper]\n",
    "    \n",
    "    print('Identified lowest outliers: %d' % len(outliers_lower)) # printing total number of values in lower cut of outliers\n",
    "    print('Identified upper outliers: %d' % len(outliers_higher)) # printing total number of values in higher cut of outliers\n",
    "    print('Total outlier observations: %d' % len(outliers_total)) # printing total number of values outliers of both sides\n",
    "    print('Non-outlier observations: %d' % len(outliers_removed)) # printing total number of non outlier values\n",
    "    print(\"Total percentual of Outliers: \", round((len(outliers_total) / len(outliers_removed) )*100, 4)) # Percentual of outliers in points\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAwK67oVoNiR"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "which defined as the proportion of the IQR past the low and high quartiles to extend the plot whiskers \n",
    "or interquartile range (IQR)\n",
    "therefore, maximum = Q3 + 1.5*IQR , min = Q1 - 1.5*IQ\n",
    "\n",
    "input = df[col] list của pd.DataFrame\n",
    "output = list [0,1] thể hiện giá trị nào là outlier\n",
    "\n",
    "'''\n",
    "\n",
    "def find_out_IQR_list(col) :\n",
    "    Q1 = col.quantile(0.25)\n",
    "    Q3 = col.quantile(0.75)\n",
    "    IQR = Q3 - Q1    \n",
    "    print('> {} : IQR range ({},{})'.format(col.name,(Q1 - 1.5 * IQR),(Q3 + 1.5 * IQR)))\n",
    "    \n",
    "    return np.where((col < (Q1 - 1.5 * IQR)) | (col > (Q3 + 1.5 * IQR)),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nOfFpvXue3We"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "81RBd7sRk0fu",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Features Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OkQAo4CboTLF"
   },
   "source": [
    "## str / NLP processing\n",
    " - https://stackoverflow.com/questions/37011734/pandas-dataframe-str-contains-and-operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVAQeauxoSxd"
   },
   "outputs": [],
   "source": [
    "def sum_str(x):\n",
    "    x = x.split(',')\n",
    "    l = []\n",
    "    for i in x:\n",
    "        if i != \"\":\n",
    "            l.append(int(i))\n",
    "    return sum(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GKzGdey_oS4k"
   },
   "outputs": [],
   "source": [
    "t = df[['APE_Net_SumLast3m','Gender','EducationCode','OccupationCode']]\n",
    "t['APE_Net_SumLast3m_Bins'] = pd.qcut(df.APE_Net_SumLast3m, q=10)\n",
    "t['OccupationCode_fillna'] =  t.groupby(['Gender','EducationCode','APE_Net_SumLast3m_Bins'])['OccupationCode'].apply(lambda x : x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMU1ZmXxp46x"
   },
   "outputs": [],
   "source": [
    "def to_lower_case(item):\n",
    "    return item.map(lambda x: x if type(x)!=str else x.lower())\n",
    "\n",
    "def to_upper_case(item):\n",
    "    return item.map(lambda x: x if type(x)!=str else x.upper())\n",
    "    \n",
    "''' example:\n",
    "train[\"province\"] = to_lower_case(train['province'])\n",
    "to_upper_case(pd.Series(['abc']))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "maKlgbsExUx5"
   },
   "outputs": [],
   "source": [
    "def replace_data(data,key,values):\n",
    "  for index, value in enumerate(data):\n",
    "    if key in str(value):\n",
    "      data[index] = values\n",
    "    elif not str(value):\n",
    "      data[index] = data[index]\n",
    "  return data\n",
    "replace_data(train_df[\"maCv\"],'công nhân','CN')\n",
    "replace_data(train_df[\"maCv\"],'nhân viên','NV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UqOeXGnNqcOA"
   },
   "outputs": [],
   "source": [
    " # sử dụng regex để tìm contains có nhiều condition\n",
    " t.COMPONENT_CODE.str.contains(r'(OPW[0-9])|(UCW[0-9])|(DSR[0-9])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hAR4OFzSqcYC"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "su dung fuzzywuzzy de tim 1 doan str trong str_list\n",
    "'''\n",
    "from fuzzywuzzy import fuzz  \n",
    "from fuzzywuzzy import process\n",
    "\n",
    "df_longdesc = df_addr_des[df_addr_des.new_addr.isnull()==False][['DESCITEM','new_addr']].rename(columns={'new_addr':'list_longdesc'})  \n",
    "df_longdesc.list_longdesc = df_longdesc.list_longdesc.str.replace(', Vietnam','')\n",
    "\n",
    "def find_longdesc(addr, df_longdesc, log=False):  \n",
    "  longdesc, percent, _ = process.extract(addr, df_longdesc.list_longdesc, limit=1, scorer=fuzz.ratio)[0]  \n",
    "  if log:  \n",
    "  print('{} -- {} ,{}, {}'.format(addr, longdesc, percent,  \n",
    "  df_longdesc[df_longdesc.list_longdesc == longdesc].DESCITEM.values[0]))  \n",
    "  return df_longdesc[df_longdesc.list_longdesc == longdesc].DESCITEM.values[0]\n",
    "\n",
    "\n",
    "list_shortdesc = list(df_addr_des.DESCITEM.values)\n",
    "def find_shortdesc(addr, list_shortdesc, log=False):  \n",
    "  shortdesc, percent = process.extract(addr, list_shortdesc, limit=1, scorer=fuzz.ratio)[0]  \n",
    "  if log:  \n",
    "  print('{} -- {} ,{}'.format(addr,shortdesc,percent))  \n",
    "  return shortdesc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytqAL29TqcfU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "patterns = {  \n",
    "'[àáảãạăắằẵặẳâầấậẫẩ]': 'a',  \n",
    "'[đ]': 'd',  \n",
    "'[èéẻẽẹêềếểễệ]': 'e',  \n",
    "'[ìíỉĩị]': 'i',  \n",
    "'[òóỏõọôồốổỗộơờớởỡợ]': 'o',  \n",
    "'[ùúủũụưừứửữự]': 'u',  \n",
    "'[ỳýỷỹỵ]': 'y'  \n",
    "}\n",
    "\n",
    "def vn_convert(text):  \n",
    "  \"\"\"  \n",
    "  Convert from 'Tieng Viet co dau' thanh 'Tieng Viet khong dau'  \n",
    "  text: input string to be converted  \n",
    "  Return: string converted  \n",
    "  \"\"\"  \n",
    "  output = text  \n",
    "  for regex, replace in patterns.items():  \n",
    "    output = re.sub(regex, replace, output)  \n",
    "    # deal with upper case  \n",
    "    output = re.sub(regex.upper(), replace.upper(), output)  \n",
    "  return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pEwO7OIEmZ-B"
   },
   "source": [
    "## merge/concat\n",
    "- https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4NlDZvymZUa"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df1,df2], axis = 0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ThYFF1smZlZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZ-VCSnElcgU"
   },
   "source": [
    "## group by\n",
    "- http://esantorella.com/2016/06/16/groupby/\n",
    "- https://towardsdatascience.com/pandas-tips-and-tricks-33bcc8a40bb9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZgKYqBJlbKh"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Groupby - Agg join str to str-array.\n",
    "# Groupby hợp một cột category thành 1 chuỗi category\n",
    "concat_str = (lambda x : ';'.join(x))\n",
    "concat_int = (lambda x: ';'.join(x.astype(str)))\n",
    "#\n",
    "df_gb = df.groupby([\"AGCode\"], as_index = True).agg({\n",
    "    \"AGLevel\": [concat_str]\n",
    "    , 'Cnt': [concat_int]\n",
    "    , \"YearMonth_Start\" : [concat_int,'min']\n",
    "  \n",
    "}).reset_index()\n",
    "df_gb.columns = ['_'.join(col) for col in df_gb.columns]\n",
    "df_gb.rename(columns={\"AGCode_\":\"AGCode\", \"AGLevel_join\": \"AGLevel_flow\"\n",
    "                      ,\"Cnt_<lambda>\": \"Cnt_flow\",\"YearMonth_Start_<lambda_0>\":\"YearMonth_flow\"\n",
    "                      , \"YearMonth_Start_min\":\"First_month\"\n",
    "                     }, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1TMsHlolboh"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Conditional groupby, các dạng custom agg của group \n",
    "#\n",
    "# custome agg function\n",
    "sum_unique = (lambda x : sum(x.unique()))\n",
    "mean_unique = (lambda x : np.mean(x.unique()))\n",
    "var_unique = (lambda x : np.var(x.unique()))\n",
    "mode = (lambda x: x.value_counts().index[0])\n",
    "quantile_mean = lambda x : x[(x>=x.quantile(0.25)) & (x<=x.quantile(0.75))].mean()\n",
    "join_unique = (lambda x: ','.join(x.unique().astype(str)))\n",
    "concat_str = (lambda x : ';'.join(x))\n",
    "\n",
    "# apply\n",
    "t = df[(df.PM_Promo_Time != \"Nan\") & (df.PM_Promo_Time <= 12)].groupby([\"PM_Promo_Time\"]).agg({\n",
    "    ,\"Gr_APE_Net_SumNext6m\": ['mean','median']\n",
    "     ,\"No_Active_Net_Last3m\": ['mean',quantile_mean]\n",
    "\n",
    "}).reset_index()#.sort_values(by=[('PM_Promo_Time', '')])\n",
    "\n",
    "t.columns = ['_'.join(col) for col in t.columns]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KtQDTlTl3nyb"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# Groupby sử dụng lambda, và các advantage techniques\n",
    "'''\n",
    "\n",
    "temp = temp.groupby(['Year','month','CHDRNUM','LIFCNUM']).apply(lambda x: sum(x[(x.LIFE == 1)&(x.COMPONENT_ID.isnull()==True)].SUM_ASSURED.fillna(0))).rename('SUM_SA').reset_index()\n",
    "#.apply(lambda x: sum(x[x.LA_INFO_ID == x.LA_INFO_ID.min()].SUM_ASSURED)) #.SUM_ASSURED.sum()\n",
    "df = df.merge(temp, on=['Year','month','CHDRNUM','LIFCNUM'])\n",
    "\n",
    "\n",
    "t = df.groupby(['c','d']).apply(lambda x: sum(x.a+x.b)).rename('e').reset_index()\n",
    "df.merge(t, on=['c','d'])\n",
    "\n",
    "# temp = temp.groupby(['Year','month','CHDRNUM','LIFCNUM']).apply(lambda x: sum(x[x.LA_INFO_ID == x.LA_INFO_ID.min()].SUM_ASSURED.fillna(0))).rename('SUM_SA').reset_index()\n",
    "# #.apply(lambda x: sum(x[x.LA_INFO_ID == x.LA_INFO_ID.min()].SUM_ASSURED)) #.SUM_ASSURED.sum()\n",
    "# df.merge(temp, on=['c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSaaP1ZY0wb5"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NHÓM n giá trị xuất hiện nhiều nhất trong trong CATCOL\n",
    "\n",
    "'''\n",
    "n = 20\n",
    "df['CATCOL_g1'] = np.where(df.CATCOL.isin(df.CATCOL.value_counts().index[0:n].tolist() \\\n",
    "                    ),df.CATCOL,'other')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asGs5x0A3pPc"
   },
   "source": [
    "**groupby - transform**\n",
    "\n",
    "---\n",
    "AFAIU I would change the lambda function as follows:\n",
    "```python\n",
    "df.groupby('Plate')['LogRatio'].transform(lambda s: s.loc[[True if v < s.quantile(q=0.8) and v > s.quantile(q=0.2) else False for v in s]].mean())\n",
    "```\n",
    "This the ```s.loc[]``` accepts an iterable with booleans in order to subset the LogRatio-Series\n",
    "\n",
    "In order to make it more readable, I'd go for the following solution:\n",
    "```python\n",
    "def quartile_subset(logratios,lower,upper):\n",
    "    # some comment to describe what you are doing\n",
    "    return logratios.loc[[True if v < logratios.quantile(q=upper) and v > logratios.quantile(q=lower) else False for v in logratios]]\n",
    "\n",
    "df.groupby('Plate')['LogRatio'].transform(lambda s: quartile_subset(s,0.2,0.8).mean())\n",
    "```\n",
    "\n",
    "---\n",
    "You can use GroupBy + transform with sum twice:\n",
    "```python\n",
    "df['e'] = df.groupby(['c', 'd'])[['a', 'b']].transform('sum').sum(1)\n",
    "\n",
    "print(df)\n",
    "   a  b  c  d   e\n",
    "0  1  1  q  z  12\n",
    "1  2  2  q  z  12\n",
    "2  3  3  q  z  12\n",
    "3  4  4  q  o   8\n",
    "4  5  5  w  o  22\n",
    "5  6  6  w  o  22\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7W3wclF0Myj9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSX93l7md6QW"
   },
   "source": [
    "### crosstab/ pivot_table\n",
    "- https://pbpython.com/pandas-crosstab.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8J7N4gVBRHcT"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "group by 1 cột phân loại theo số lượng của từng phân loại\n",
    "ví dụ: lấy top 10 phân loại có số lượng nhiều nhất của 1 biến phân loại\n",
    "'''\n",
    "col = 'LIFE'\n",
    "df_model[col+'_g1'] = np.where(df_model[col].isin(df_model[col].value_counts().index[0:10].tolist()),df_model[col],'other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QmKNG-FRHVj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcFpKNKUnEoX"
   },
   "source": [
    "### binning\n",
    "\n",
    "- https://pbpython.com/pandas-qcut-cut.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bcFdwO8Cm9wy"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "binning với custom label\n",
    "'''\n",
    "## ex1\n",
    "bins = [0, 3, 6,9, 12]\n",
    "labels = ['1', '2', '3', '4']\n",
    "t['PM_Promo_Time_bins'] = pd.cut(t['PM_Promo_Time'], bins, labels=labels)\n",
    "\n",
    "## ex2\n",
    "cut_labels_4 = ['below_normal', 'normal', 'above_normal', 'fat1','fat2','fat3']\n",
    "cut_bins = [0, 18.5, 24.9, 29.9, 34.9, 39.9, 10000]\n",
    "df_model['LA_BMI_bin'] = pd.cut(df_model['LA_BMI'], bins=cut_bins, labels=cut_labels_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LInlXcbEQ3m8"
   },
   "outputs": [],
   "source": [
    "## Binning 1 cot phan loai (qua nhieu phan loai >20)\n",
    "t = df[['OCCPCODE','LABEL']]\n",
    "col = 'OCCPCODE'\n",
    "t[col+'_g1'] = np.where(t[col].isin(t[col].value_counts().index[0:10].tolist()),t[col],'other')\n",
    "\n",
    "#>> percent_groupby(t,[col+'_g1'],'LABEL')\n",
    "\n",
    "\n",
    "\n",
    "## Binning 1 cot lien tuc\n",
    "col = 'AttendedAge'\n",
    "label = 'xsell'\n",
    "t = df[[col,label]]\n",
    "\n",
    "bins = [0,15,20,25,30,50,200]\n",
    "labels = [col + '_' + str(v) for v in [*range(0,len(bins)-1,1)]]\n",
    "\n",
    "t[col+'_bin1'] = pd.cut(t[col], bins, labels=labels)\n",
    "#>> percent_groupby(t,[col+'_bin1'],label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EhtHY720NEw_"
   },
   "source": [
    "### lag/roll/shift\n",
    "- https://pandas.pydata.org/pandas-docs/version/0.21.1/generated/pandas.DataFrame.rolling.html\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html\n",
    "- https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPqdQ76Zd9Z8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Tính tổng trong vòng 1 năm\n",
    "'''\n",
    "\n",
    "t = t.set_index('YearMonth_').groupby(['LIFCNUM_']).rolling(window=365, freq='D',min_periods=1).agg({\n",
    "    \"UAB_PeriodSum\":'sum'    \n",
    "}).reset_index()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_3Ircfld9TU"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "GROUPBY đếm số lần xuất hiện cộng dồn\n",
    "'''\n",
    "\n",
    "df['Num_in_POLICY'] = df.groupby(['LIFCNUM_']).cumcount()+1\n",
    "df['Num_in_COMPONENT'] = df.groupby(['LIFCNUM_']).Num_COMPONENT_CODE.apply(lambda x: x.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtuJAzIUHdou"
   },
   "outputs": [],
   "source": [
    "# shift bang group by\n",
    "df_BD['_lapse_shift1'] =(df_BD.sort_values(by=['CHDRNUM','BTDATE']\n",
    "                                           , ascending=True).groupby(['CHDRNUM'])['lapse'].shift(1))\n",
    "\n",
    "# tạo 1 cột đếm cumcount theo 1 group 1\n",
    "df_BD['_count_billtime'] = df_BD.sort_values(by=['CHDRNUM','BTDATE']\n",
    "                                             , ascending=True).groupby(['CHDRNUM'])['lapse'].cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NPY955ebHdHk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZwKNHDtmQ4h"
   },
   "source": [
    "## where/select/query / iloc/ix\n",
    "- https://kanoki.org/2020/01/21/pandas-dataframe-filter-with-multiple-conditions/\n",
    "- https://stackoverflow.com/questions/49228596/pandas-case-when-default-in-pandas\n",
    "- https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3ee3X1YmR_9"
   },
   "outputs": [],
   "source": [
    "df1['a'] = np.select(\n",
    "        [\n",
    "            df1.AGLevel_flow.str.contains('AG;PM'),  \n",
    "            df1.AGLevel_flow.isin((\"AG;PM;AG;PM;UM;PM\",\"AG;PM;AG;PM;UM;AG\"))\n",
    "            ,df1.AGLevel_flow == \"AG;UM\"             \n",
    "        ],\n",
    "        [\n",
    "           df1.YearMonth_flow.str.split(\";\").str[1]\n",
    "            ,1,2\n",
    "        ]\n",
    "    ,        default = (pd.to_datetime(df1.First_month, format='%Y%m') + pd.DateOffset(months=6)).dt.strftime('%Y%m')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QHdSOvWumSKx"
   },
   "outputs": [],
   "source": [
    "query_term = 1\n",
    "factor_1.query('(`Benefit term` == @query_term)').Factor_others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BZls0VxFtYa"
   },
   "outputs": [],
   "source": [
    "t['abcd'] = np.where(t['LA\\'s Decision'].isnull()==False,\n",
    "                             t['LA\\'s Decision']\n",
    "                             ,np.select([((t.FACTOR5=='YES-UW') | (t.FACTOR3=='YES') )],\n",
    "                                        ['refer UW']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DqqeXP7qPXxl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOhVJVEyPV7u"
   },
   "source": [
    "### sort\n",
    "\n",
    "## **sort a columns with custom sorter index**\n",
    "- https://stackoverflow.com/questions/23482668/sorting-by-a-custom-list-in-pandas\n",
    "- https://stackoverflow.com/questions/13838405/custom-sorting-in-pandas-dataframe\n",
    "- https://thispointer.com/pandas-sort-rows-or-columns-in-dataframe-based-on-values-using-dataframe-sort_values/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vAD8B3cIFtTY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWm9sbjolESN"
   },
   "source": [
    "## Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ayyixlm1lD3U"
   },
   "outputs": [],
   "source": [
    "df['Age'] = round((pd.to_datetime(df.Promo_YearMonth, format='%Y%m') - \n",
    "                   pd.to_datetime(df.DateOfBirth, format='%Y/%m/%d')) / np.timedelta64(1, 'Y'),0)\n",
    "\n",
    "df['New_InvoiceDate'] = pd.to_datetime(df.InvoiceDate, format='%d/%m/%Y %H:%M').dt.strftime('%Y%m')\n",
    "\n",
    "(pd.to_datetime(df_analyze.NewRe_1st_YearMonth_End.fillna(200001), format='%Y%m') \n",
    "                                              + pd.DateOffset(months=1)).dt.strftime('%Y%m').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqO8pJBblHFg"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "BINs of age\n",
    "'''\n",
    "\n",
    "df_model['LA_Age_bin2_LE'] = pd.qcut(df_model.ANBCCD, q=7, precision=0).astype('category').cat.codes\n",
    "bin_labels = []\n",
    "for string in sorted(pd.qcut(df_model.ANBCCD, q=7, precision=0).unique()):\n",
    "    x = str(string).replace('(', '').replace(']', '').replace(', ', '-').replace('.0', '')    \n",
    "    bin_labels.append(x)\n",
    "df_model['LA_Age_bin2'] = pd.qcut(df_model.ANBCCD, q=7, precision=0,labels=bin_labels).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJUeNk50VTOP",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Visualization\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuLwfvj3LvGi"
   },
   "source": [
    "## Notes\n",
    "- [machinelearningmastery - Data Visualization Methods in Python](https://machinelearningmastery.com/data-visualization-methods-in-python/)\n",
    "- [note 1](https://www.kaggle.com/sonannguyenngoc/titanic-survival-prediction-end-to-end-ml-pipeline?scriptVersionId=33324915#Visualizations-&-Analyze), [note 2]()\n",
    "- [matplotlib guide](https://www.kaggle.com/grroverpr/matplotlib-plotting-guide), [2](https://towardsdatascience.com/data-visualization-using-matplotlib-16f1aae5ce70), [top 50 matplotlib](https://nextjournal.com/sosiristseng/top-50-matplotlib-visualizations#categorical-plots)\n",
    "- [seaborn guide](https://www.kaggle.com/kralmachine/seaborn-tutorial-for-beginners), [seaborn 1](https://seaborn.pydata.org/tutorial.html)\n",
    "- [scatterplot](https://seaborn.pydata.org/generated/seaborn.scatterplot.html), [2](https://wellsr.com/python/seaborn-scatter-plot-with-sns-scatterplot/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7552VYghIj-g"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "simple plot\n",
    "'''\n",
    "\n",
    "# 1. barplot số lượng của 1 cột phân loại\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "g = sns.countplot('YearMonth_', hue='label', data= df_model, ax = ax)\n",
    "g.set_ylim([0, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXqUzgJPIVXk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "vẽ phân bố của label theo 3 features\n",
    "'''\n",
    "# scatter_plot_3cols(\"Age\",\"Money\",\"Gender\",\"label\",df_model)\n",
    "def scatter_plot_3cols(x,y,col,hue,data,zoom=[0.1,0.9]):\n",
    "    g=sns.FacetGrid(data, col=col, hue=hue\n",
    "                    , margin_titles=True,palette='tab10'\n",
    "                    ,size=6,aspect=1,height=4.5, col_wrap=3)\n",
    "    g=(g.map(plt.scatter, x, y, s=10, alpha = 0.66).add_legend())\n",
    "    #plt.tight_layout()\n",
    "    plt.ylim(np.quantile(data[y].values,zoom[0]), np.quantile(data[y].values,zoom[1]))\n",
    "    #plt.xlim(0, None)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "'''\n",
    "tính số xác suất của label trong từng biến x, và chia theo nhóm hue \n",
    "'''\n",
    "#percent_label_2catcols(\"x\",\"label\",\"hue\",df_model)\n",
    "def percent_label_2catcols(x,y,hue,data,sort_str=True):\n",
    "    if sort_str:\n",
    "        col_order = sorted(data[hue].unique().tolist(), key=len)\n",
    "    else:\n",
    "        col_order = sorted(data[hue].unique().tolist())\n",
    "    g = sns.factorplot(x=x, y=y, col=hue,\n",
    "                        data=data,# saturation=.5,\n",
    "                        kind=\"bar\"\n",
    "                       , order= sorted(data[x].unique().tolist())\n",
    "                       , col_order= col_order\n",
    "                       , ci=None, aspect=2.5, height=2.5,  palette='tab20',col_wrap=3)\n",
    "    (g.set_axis_labels(\"\", \"label Rate\")\n",
    "        #.set_xticklabels([\"Men\", \"Women\"])\n",
    "        #.set_titles(\"{col_name} {col_var}\")\n",
    "        .set(ylim=(0, 1))\n",
    "        .despine(left=True))  \n",
    "    plt.subplots_adjust(top=1.2)   \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "đếm số lượng của label trong từng biến x, và chia theo nhóm hue \n",
    "'''\n",
    "#percent_label_2catcols(\"Gender\",\"label\",\"Age_Bin\",df_model)\n",
    "\n",
    "def count_label_2catcols(x,y,hue,data,sort_str=True):\n",
    "    if sort_str:\n",
    "        col_order = sorted(data[hue].unique().tolist(), key=len)\n",
    "    else:\n",
    "        col_order = sorted(data[hue].unique().tolist())\n",
    "    g = sns.catplot(x, col=y, hue =hue,\n",
    "                    col_wrap=3,\n",
    "                    data=data\n",
    "                    , order= sorted(data[x].unique().tolist())\n",
    "                    #, order = data[x].value_counts().index\n",
    "                    , col_order= col_order\n",
    "                    , kind=\"count\", height=2.5, aspect=2.5, \n",
    "                    palette='tab20')\n",
    "    #g.add_legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QLva7-YEm7x-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "## Plot continous features\n",
    "to see:\n",
    "- distribution\n",
    "- quantile\n",
    "- outlier identified by IQR\n",
    "- distribution & quantile after remove outlier outside IQR\n",
    "\n",
    "Plotting the continous features :    \n",
    "1. A box plot (or box-and-whisker plot) shows the distribution of quantitative data \n",
    "in a way that facilitates comparisons between variables.\n",
    "2. Distribution graph :to check the linearity of the variables and look \n",
    "for skewness of features.\n",
    "\n",
    "\"\"\"\n",
    "def plot_continous(cols, df, target=None, info = False, corr = False, drop_3Qplot=False):\n",
    "    # Using boxplot to analyze the continous feature\n",
    "    if corr == True:\n",
    "      df_corr = df.corr().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\n",
    "      df_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}\n",
    "                , inplace=True)      \n",
    "\n",
    "    for col in cols:\n",
    "        print(f'\\n >Column name: {col}')\n",
    "        plt.figure(figsize=(15,8))\n",
    "        \n",
    "        if corr == True:\n",
    "          print(\"Top 5 positive correlated w/ featuers \\n{}\".format(\n",
    "              df_corr[(df_corr[\"Feature 1\"]==col) & (df_corr[\"Feature 2\"]!=col)].head()))\n",
    "          print(\"Top 5 negative correlated w/ featuers \\n{}\".format(\n",
    "              df_corr[(df_corr[\"Feature 1\"]==col) & (df_corr[\"Feature 2\"]!=col)].tail()))\n",
    "        if info == True:\n",
    "\n",
    "          print(\"max: {}, min: {}, mean: {}, std: {}, median: {}\".format(\n",
    "              df[col].max(),df[col].min(),\n",
    "              round(df[col].mean(),3),round(df[col].std(),3),\n",
    "              df[col].median()))\n",
    "          print(f\"Quantiles | {col}\\n\",df[col].quantile([.01, .025, .1, .25, .5, .75, .9, .975, .99]))\n",
    "          print(\"No unique: {}, No null: {}\".format(df[col].nunique(),df[col].isnull().sum()))\n",
    "          if df[col].nunique() < 10 :\n",
    "            print(f\"Value counts in {col} \\n\",format(df[col].value_counts()))\n",
    "\n",
    "        # plot  \n",
    "        plt.subplot(2, 2, 1)\n",
    "        if target == None:\n",
    "            fig = sns.boxplot(col, whis=1.5, data=df)\n",
    "            fig.legend()\n",
    "        else:\n",
    "            fig = sns.boxplot(x=target, y=col, whis=1.5, data=df)\n",
    "            fig.legend()\n",
    "        # which defined as the proportion of the IQR past the low and high quartiles to extend the plot whiskers \n",
    "        # or interquartile range (IQR)\n",
    "        # therefore, maximum = Q3 + 1.5*IQR , min = Q1 - 1.5*IQR\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1    \n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        fig = sns.distplot(df[col].dropna())#.hist(bins=20)\n",
    "        fig.set_ylabel('Volumn')\n",
    "        fig.set_xlabel(col)      \n",
    "        \n",
    "        print('No data out of range between Q1 : {} and Q3 : {} = {}' .format(Q1,Q3,((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()))\n",
    "\n",
    "        if drop_3Qplot==True:          \n",
    "          plt.subplot(2, 2, 3)\n",
    "          tmp = df[(df[col] >= Q1)&(df[col] <= Q3)]#[col]\n",
    "          if target == None:\n",
    "              fig = sns.boxplot(col, whis=1.5, data=tmp)\n",
    "              fig.legend()\n",
    "          else:\n",
    "              fig = sns.boxplot(x=target, y=col, whis=1.5, data=tmp)\n",
    "              fig.legend()\n",
    "          \n",
    "          plt.subplot(2, 2, 4)\n",
    "          fig = sns.distplot(tmp[col])#.dropna())#.hist(bins=20)\n",
    "          fig.set_ylabel('Volumn')\n",
    "          fig.set_xlabel(col)   \n",
    "\n",
    "        plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f72WTuvLn_aZ"
   },
   "outputs": [],
   "source": [
    "def plot_categorical(cols, target, df):\n",
    "  for col in cols:\n",
    "    if target == None:\n",
    "      sns.countplot(x=col, data=df[[col]] ,  palette=\"Reds_d\")      \n",
    "    else:\n",
    "      print('Column name: %s' %col)\n",
    "      sns.countplot(x=col, hue=target, data=df[[col,target]], palette=\"Reds_d\")\n",
    "      #sns.barplot(x=col, y=target, data=df[[col,target]]\n",
    "      #        , palette=\"Reds_d\", estimator = sum)\n",
    "      plt.legend()\n",
    "    plt.xticks(rotation=90)#-60    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEqPnlPhet0E"
   },
   "source": [
    "## 3D plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5D_h94Fe2SF"
   },
   "source": [
    "Sources:\n",
    "- https://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html\n",
    "- https://matplotlib.org/3.1.1/gallery/index.html\n",
    "\n",
    "\n",
    "Example:\n",
    "- [how-to-surface-plot-3d-plot-from-dataframe](https://stackoverflow.com/questions/36589521/how-to-surface-plot-3d-plot-from-dataframe)\n",
    "- [plotting-pandas-crosstab-dataframe-into-3d-bar-chart](https://stackoverflow.com/questions/56336066/plotting-pandas-crosstab-dataframe-into-3d-bar-chart)\n",
    "- [matplotlib-3d-surface-from-a-rectangular-array-of-heights](https://stackoverflow.com/questions/11766536/matplotlib-3d-surface-from-a-rectangular-array-of-heights)\n",
    "- [surface-plots-in-matplotlib](https://stackoverflow.com/questions/9170838/surface-plots-in-matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffvdG1_VoIDm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM5mJUJac0Jn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_QkWaspdKfaX"
   },
   "source": [
    "# Rules Baed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3W9LBvdRc0Mx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "define function\n",
    "'''\n",
    "def percent(x): # percent of 1, work only for binary (0,1)\n",
    "    return round(sum(x)/len(x),3)\n",
    "\n",
    "def cal_metric(pred_var,y_test,k):\n",
    "    result = pd.DataFrame(columns = ['Thress_hold','accuracy','precision','recall','roc_auc','f1_score_poslabel','f1_score_neglabel'\n",
    "                    ,'TP','TN','FN','FP'\n",
    "                    ,'case','reduce','per_reduce','wrong_in_reduce','per_correct_in_reduce','send','per_send_wrong'])\n",
    "    confu_matrix = confusion_matrix(pred_var,y_test, labels=[1,0])\n",
    "    result = pd.concat([result,pd.DataFrame([[\n",
    "        k, accuracy_score(pred_var,y_test)\n",
    "        ,precision_score(pred_var,y_test),recall_score(pred_var,y_test)\n",
    "        ,accuracy_score(pred_var,y_test),f1_score(pred_var,y_test)\n",
    "        ,f1_score(pred_var,y_test, pos_label=0)\n",
    "        ,confu_matrix[0,0],confu_matrix[0,1],confu_matrix[1,0],confu_matrix[1,1]  \n",
    "        ,len(pred_var),len(pred_var)-(confu_matrix[0,0]+confu_matrix[0,1]),1-round((confu_matrix[0,0]+confu_matrix[0,1])/len(pred_var),3)\n",
    "        ,confu_matrix[1,0], round(confu_matrix[1,1]/(confu_matrix[1,0]+confu_matrix[1,1]),3)\n",
    "        ,(confu_matrix[0,0]+confu_matrix[0,1]), round(confu_matrix[0,1]/(confu_matrix[0,0]+confu_matrix[0,1]),3)\n",
    "    ]]\n",
    "        ,columns = ['Thress_hold','accuracy','precision','recall','roc_auc','f1_score_poslabel','f1_score_neglabel'\n",
    "                     ,'TP','TN','FN','FP'\n",
    "                    ,'case','reduce','per_reduce','wrong_in_reduce','per_correct_in_reduce','send','per_send_wrong'\n",
    "                   ])])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DFEureNMc0Pq"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "test case xác suất label = 1 \n",
    "xác định trong YEARMONTH =< 202001\n",
    "test trong YEARMONTH > 202001\n",
    "FEATURES: 'Manually_UAB_PeriodSum_bin', 'Age_Bin1'\n",
    "'''\n",
    "matrix = df_model[df_model.YearMonth_<=202001].pivot_table(index=['UAB_BIN'], columns=['AGE_BIN'],\n",
    "                                                           values='label', aggfunc={percent}#,len}\n",
    "                                                           , fill_value=0)\n",
    "matrix.index = ['-1.0-832.0m', '832.0-1454.0m', '1898.0-2463.0m', '2463.0-3669.0m', '1454.0-1898.0m', '3669.0-12015.0m', '12015.0-94892190.0m']\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1C9gRu_-Kn6n"
   },
   "outputs": [],
   "source": [
    "df_filter['id'] = df_filter[id_col].astype(str).sum(axis=1).astype('category').cat.codes\n",
    "\n",
    "t1 = df_filter[df_filter['YearMonth_'] <= 202001]\n",
    "t1['filter_1'] = t1.groupby(id_col).label.transform('mean')\n",
    "# t1['id'] = t1[id_col].astype(str).sum(axis=1)\n",
    "t2 = df_filter[df_filter['YearMonth_'] > 202001]\n",
    "# t2['id'] = t2[id_col].astype(str).sum(axis=1)\n",
    "\n",
    "# runing dropout thresh-hold of RB and calculate results\n",
    "t = df_filter[df_filter['YearMonth_'] > 202001]\n",
    "df = pd.DataFrame() # create results df\n",
    "for k in np.arange(0,0.5,0.05): #[0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    #print('Thress hold :', k)\n",
    "    t1['predict'] = np.where(t1.filter_1 <= k , 0 ,1)\n",
    "    t2['predict'] = np.where(t2.id.isin(t1[t1.predict == 0].id.unique().tolist()),0,1)\n",
    "    df = pd.concat([df,cal_metric(t2.predict,t2.label,k)],axis=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GGa9MjP9K5SP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGyuX8lwKxCH"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "VOTING RULES BASED\n",
    "'''\n",
    "# selected cols\n",
    "col = [ 'Manually_UAB_PeriodSum_bin','OCCUAPATION_concat','LA_Age_bin2','Gender','LIFE' ]\n",
    "df_filter = df_model[['YearMonth_','label']+col].copy()\n",
    "# alter bin of categorical features\n",
    "df_filter['OCCUAPATION_concat_g1'] = np.where(df_filter.OCCUAPATION_concat.isin(df_filter.OCCUAPATION_concat.value_counts().index[0:10].tolist())\n",
    "                                              , df_filter.OCCUAPATION_concat, 'other')\n",
    "df_filter['LIFE_g1'] = np.where(df_filter.LIFE.isin(df_filter.LIFE.value_counts().index[0:1].tolist())\n",
    "                                              , df_filter.LIFE, 'other')\n",
    "\n",
    "# declare rules\n",
    "group_col = [['Manually_UAB_PeriodSum_bin','OCCUAPATION_concat_g1','LA_Age_bin2']\n",
    "             ,['Manually_UAB_PeriodSum_bin','Gender','LA_Age_bin2']\n",
    "             ,['Manually_UAB_PeriodSum_bin','LIFE_g1','LA_Age_bin2']\n",
    "            ]\n",
    "\n",
    "# runing loop for each rules\n",
    "for i,col in enumerate(group_col):\n",
    "    df_filter['id'+str(i)] = df_filter[col].astype(str).sum(axis=1).astype('category').cat.codes    \n",
    "\n",
    "t1 = df_filter[df_filter['YearMonth_'] <= 202001]\n",
    "t2 = df_filter[df_filter['YearMonth_'] > 202001]   \n",
    "\n",
    "for i,col in enumerate(group_col):    \n",
    "    t1['filter_'+str(i)] = t1.groupby(col).label.transform('mean')\n",
    "    # t1['id'] = t1[id_col].astype(str).sum(axis=1)\n",
    "    \n",
    "df_result = pd.DataFrame() # create results df\n",
    "for k in np.arange(0,0.5,0.05): #[0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    #print('Thress hold :', k)\n",
    "    for i,_ in enumerate(group_col):\n",
    "        t1['predict_'+str(i)] = np.where(t1['filter_'+str(i)] <= k , 0 ,1)\n",
    "        t2['predict_'+str(i)] = np.where(t2['id'+str(i)].isin(t1[t1['predict_'+str(i)] == 0]['id'+str(i)].unique().tolist()),0,1)\n",
    "    \n",
    "    t2['predict'] = t2[[col for col in t2.columns if 'predict_' in col]].sum(axis=1)\n",
    "    t2['predict'] = np.where(t2.predict==0,0,1)\n",
    "    df_result = pd.concat([df_result,cal_metric(t2.predict,t2.label,k)],axis=0)\n",
    "\n",
    "df_result    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EoNYA1DnLcux"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WzFvPys_F9KB"
   ],
   "include_colab_link": true,
   "name": "1. Data Analytics.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
