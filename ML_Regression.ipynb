{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Regression.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/ML_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubI7V1222TvJ",
        "colab_type": "toc"
      },
      "source": [
        ">[Preprocessing](#scrollTo=-2COeSc5ml1_)\n",
        "\n",
        ">>[Transformation Scaling/Standardize](#scrollTo=PcX1fpPrrg-_)\n",
        "\n",
        ">>[Encoding](#scrollTo=qeLaVzFW0Se6)\n",
        "\n",
        ">>>[Onehot Encode](#scrollTo=HULldo6Zm6l1)\n",
        "\n",
        ">>>[Label Encode](#scrollTo=Hz-vTHED0Vbc)\n",
        "\n",
        ">>>[Reduce memory](#scrollTo=WzFvPys_F9KB)\n",
        "\n",
        ">>>[Reduce dimensional checking](#scrollTo=a_a8z7m7uec_)\n",
        "\n",
        ">>[Feature Generation](#scrollTo=ffkEEAsbC4EW)\n",
        "\n",
        ">>[Feature Selection](#scrollTo=LfdUhklbDAQa)\n",
        "\n",
        ">[MATH REVIEW](#scrollTo=OxKUSl9g2BWM)\n",
        "\n",
        ">[Regression Algorithms](#scrollTo=8FIjZqKbt8P8)\n",
        "\n",
        ">>[Linear Regression](#scrollTo=dpc1c9ZsP2i6)\n",
        "\n",
        ">>[Perceptron Learning Algorithm (PLA)](#scrollTo=eX6sM5ldJu0-)\n",
        "\n",
        ">>[Logistic Regression](#scrollTo=bwn5AC4UPgJE)\n",
        "\n",
        ">>[Softmax Regression](#scrollTo=IDHLmr9DPmak)\n",
        "\n",
        ">>[SVM](#scrollTo=4Kz3zCJNjApN)\n",
        "\n",
        ">[2.Instance-based (memory-based)](#scrollTo=_j1ygYkdutFv)\n",
        "\n",
        ">>[K-nearest neighbors (KNN)](#scrollTo=m1MEZV-Js4Yq)\n",
        "\n",
        ">[3.Clustering Algorithms](#scrollTo=np0HDTN556XV)\n",
        "\n",
        ">>[K-means clustering](#scrollTo=8OJRGMNB5xXU)\n",
        "\n",
        ">[4.Reduce Dimension](#scrollTo=W8gx540rhd0Q)\n",
        "\n",
        ">>>[Singular Value Decomposition (SVD)](#scrollTo=ZF025S_Lgd07)\n",
        "\n",
        ">>[Principal Component Analysis](#scrollTo=Xc9w3obZjBTW)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxKUSl9g2BWM",
        "colab_type": "text"
      },
      "source": [
        "# [MATH REVIEW](https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/4.Math.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa5U6PpSn2EI",
        "colab_type": "text"
      },
      "source": [
        "## General Overfitting in ML\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kmw_AwxPUPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQSvvg90sowf",
        "colab_type": "text"
      },
      "source": [
        "Overfitting là khi mô hình quá fit với tập train hay chỉ mô tả tốt training set, dẫn đến không có tính tổng quá hóa và dẫn đến predict sai.\n",
        "\n",
        "Training error/ Test error:\n",
        "\n",
        "![alt text](https://cdn.mathpix.com/snip/images/Z3C1amdGFbXC1H-d3OvgS3H8nK-J-2KwvgI11OBrnps.original.fullsize.png)\n",
        "\n",
        "## Validation\n",
        "Trích ra 1 tập validate trên tập train, trước khi test với tập test\n",
        "Dựa vào validation error chúng ta có thể tuning parameter:\n",
        "- model complexity (quardaric regression)\n",
        "- regularization terms (l1 , l2)\n",
        "## Cross-validation\n",
        "Chia ra thành nhiều fold, gọi là k-fold cross-validation\n",
        "Chia training set ra k-fold validation set, chúng ta fit trên từng fold và test trên phần còn lại (k-1 fold). Lặp lại việc fit k lần.\n",
        "Error của mô hình sẽ bằng trung bình cộng của các fold.\n",
        "> Chọn mô hình tốt nhất sẽ là mô hình có error thấp nhất và std nhỏ nhất (thay đổi)\n",
        "\n",
        "### Early stopping\n",
        "Set early stopping sao cho thuật toán sẽ dừng khi training error vẫn có xu hướng giảm nhưng validation error có xu hướng tăng (tránh overfit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiD6NVsSrPoA",
        "colab_type": "text"
      },
      "source": [
        "## Regularization\n",
        "Một cách cơ bản, là thay đổi model đi 1 chút, chấp nhận hy sinh độ chính xác trong training set, nhưng giảm độ phức tạp của model, giúp tránh overfitting trong khi vẫn giữ được tính tổng quát của nó\n",
        "\n",
        "Regularization Algorithms:\n",
        "\n",
        "  - Ridge Regression\n",
        "  - Least Absolute Shrinkage and Selection Operator (LASSO) - $l_1$\n",
        "  - Least-Angle Regression (LARS) - $l_2$\n",
        "  - Elastic Net\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYnyNVagDtk8",
        "colab_type": "text"
      },
      "source": [
        "### Thêm hạng số $l_1$ hoặc $l_2$\n",
        "![alt text](https://cdn.mathpix.com/snip/images/xNlrgE29C5jNwpTeOfYzy2YH7u6E83T2qf6O5jGXntM.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YT6SeV_4peW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2COeSc5ml1_",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcX1fpPrrg-_",
        "colab_type": "text"
      },
      "source": [
        "## Transformation Scaling/Standardize\n",
        "News:\n",
        "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
        "\n",
        "\n",
        "Căn bản:\n",
        "- [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n",
        "![alt text](https://miro.medium.com/max/4800/1*z-C9ANBC4rjsk-ZK4wzijg.png)\n",
        "- [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
        "\n",
        "\n",
        "- Đối với từng model nên xét thêm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bn4UO8yW5hMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWrridnMkIwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "nottrans_num_col = ['AGSegCode', 'MAPASegCode',\n",
        "                    'Leader_AGSegCode', 'Leader_MAPASegCode']\n",
        "                    \n",
        "scale_col = [c for c in WOE_numeric_col if c not in nottrans_num_col]\n",
        "print('scale cols' ,scale_col)\n",
        "robust_scale = RobustScaler().fit(round(df_WOE[scale_col],2))\n",
        "df_WOE[scale_col] = robust_scale.transform(df_WOE[scale_col])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeLaVzFW0Se6",
        "colab_type": "text"
      },
      "source": [
        "## Encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HULldo6Zm6l1",
        "colab_type": "text"
      },
      "source": [
        "### Onehot Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cfS1ArWkI61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummies_col = []\n",
        "for c in ['Leader_RegionCode', 'RegionCode', 'ContactProvince_Code']:\n",
        "    print('> Get dummies with prefix {}'.format(c+'_'))\n",
        "    df_dummies = pd.get_dummies(df[c], prefix = c ) \n",
        "    dummies_col = dummies_col +(df_dummies.columns.to_list())\n",
        "    df_WOE = pd.concat( [df_WOE, df_dummies] , axis = 1)\n",
        "    \n",
        "print('dummies col: ',dummies_col)\n",
        "WOE_dummies_col = dummies_col"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz-vTHED0Vbc",
        "colab_type": "text"
      },
      "source": [
        "### Label Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOaNKw9Q0WS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_WOE = df_WOE.replace({\n",
        "             'Leader_AGLevel' : {'PM':1,'UM':2,'BM':3}\n",
        "           })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB3mZzSk0Waf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in ['Leader_RegionCode', 'RegionCode', 'ContactProvince_Code','EducationCode']:\n",
        "    print('> ',col)\n",
        "    df_WOE[col] = df_WOE[col].astype('category').cat.codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS4OLaR35bbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzFvPys_F9KB",
        "colab_type": "text"
      },
      "source": [
        "### Reduce memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUyadfjFF_kb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.        \n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype\n",
        "        \n",
        "        if col_type != object:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "        else:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgZr6swygYbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvTRFClqgYjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_a8z7m7uec_",
        "colab_type": "text"
      },
      "source": [
        "### Reduce dimensional checking\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDc3N94Mj8K-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#scaling only on tranform data\n",
        "df_V = df_V[c_V_trans] \n",
        "df_V_test = df_V_test[c_V_trans]\n",
        "\n",
        "#standardize scaled\n",
        "scaler = StandardScaler().fit(df_V)\n",
        "df_V_scaled = scaler.transform(df_V)\n",
        "print('Scaled mean df_V',df_V_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_scaled[:,0].std()) \n",
        "scaler = StandardScaler().fit(df_V_test)\n",
        "df_V_test_scaled = scaler.transform(df_V_test)\n",
        "print('Scaled mean df_V',df_V_test_scaled[:,0].mean())  # zero (or very close)\n",
        "print('Scaled std df_V',df_V_test_scaled[:,0].std()) \n",
        "\n",
        "# plot cumulative explained variance\n",
        "# pca = PCA().fit(df_V_scaled)\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# #plt.xlim(0,7,1)\n",
        "# plt.xlabel('Number of components')\n",
        "# plt.ylabel('Cumulative explained variance')\n",
        "\n",
        "\n",
        "# Setup Principal component analysis\n",
        "pca = PCA(n_components=125) \n",
        "#pca = PCA(n_components=0.96)  #v2: n_components=0.95\n",
        "df_V_pca = pca.fit_transform(df_V_scaled)\n",
        "np.save('df_V_pca_v3.npy',df_V_pca)\n",
        "#df_V_pca.to_csv('df_V_pca', sep='\\t')\n",
        "df_V_test_pca = pca.fit_transform(df_V_test_scaled)\n",
        "np.save('df_V_test_pca_v3.npy',df_V_test_pca)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljSftUKgvtyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import io\n",
        "import plotly.offline as py#visualization\n",
        "py.init_notebook_mode(connected=True)#visualization\n",
        "import plotly.graph_objs as go#visualization\n",
        "import plotly.tools as tls#visualization\n",
        "import plotly.figure_factory as ff#visualization\n",
        "\n",
        "'''\n",
        "Scatter plot giữa 3 cột bất kỳ trong dữ liệu, được hue = TARGET\n",
        "cho cái nhìn về dữ liệu, xem liệu nó có khả năng phân tách không\n",
        "'''\n",
        "\n",
        "trace1 = go.Scatter3d(x = churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = churn[\"ANNUAL_FEES\"],\n",
        "                      z = churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      mode = \"markers\",\n",
        "                      name = \"Churn customers\",\n",
        "                      text = \"Id : \" + churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      marker = dict(size = 1,color = \"red\")\n",
        "                     )\n",
        "trace2 = go.Scatter3d(x = not_churn[\"MEMBER_ANNUAL_INCOME\"],\n",
        "                      y = not_churn[\"ANNUAL_FEES\"],\n",
        "                      z = not_churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
        "                      name = \"Non churn customers\",\n",
        "                      text = \"Id : \" + not_churn[\"MEMBERSHIP_NUMBER\"],\n",
        "                      mode = \"markers\",\n",
        "                      marker = dict(size = 1,color= \"green\")\n",
        "                     )\n",
        "\n",
        "layout = go.Layout(dict(title = \"Monthly charges,total charges & tenure in customer attrition\",\n",
        "                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n",
        "                                                   center=dict(x=0, y=0, z=0),\n",
        "                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n",
        "                                     xaxis  = dict(title = \"annual incomes\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'),\n",
        "                                     yaxis  = dict(title = \"annual fees\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  ),\n",
        "                                     zaxis  = dict(title = \"term years\",\n",
        "                                                   gridcolor='rgb(255, 255, 255)',\n",
        "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
        "                                                   showbackground=True,\n",
        "                                                   backgroundcolor='rgb(230, 230,230)'\n",
        "                                                  )\n",
        "                                    ),\n",
        "                        height = 700,\n",
        "                       )\n",
        "                  )\n",
        "                  \n",
        "\n",
        "data = [trace1,trace2]\n",
        "fig  = go.Figure(data = data,layout = layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQI7G83cDx2l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components = 2)\n",
        "\n",
        "X = tmp[[i for i in tmp.columns if i not in Id_col + target_col]]\n",
        "Y = tmp[target_col + Id_col]\n",
        "\n",
        "principal_components = pca.fit_transform(X)\n",
        "pca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\n",
        "pca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\n",
        "pca_data[\"CHURN\"] = pca_data[\"CHURN\"].replace({1:\"CANCELLED\",0:\"INFORCE\"})\n",
        "\n",
        "def pca_scatter(target,color) :\n",
        "    tracer = go.Scatter(x = pca_data[pca_data[\"CHURN\"] == target][\"PC1\"] ,\n",
        "                        y = pca_data[pca_data[\"CHURN\"] == target][\"PC2\"],\n",
        "                        name = target,mode = \"markers\",\n",
        "                        marker = dict(color = color,\n",
        "                                      line = dict(width = .5),\n",
        "                                      symbol =  \"diamond-open\"),\n",
        "                        text = (\"Customer Id : \" + \n",
        "                                pca_data[pca_data[\"CHURN\"] == target]['MEMBERSHIP_NUMBER'])\n",
        "                       )\n",
        "    return tracer\n",
        "\n",
        "layout = go.Layout(dict(title = \"Visualising data with principal components\",\n",
        "                        plot_bgcolor  = \"rgb(243,243,243)\",\n",
        "                        paper_bgcolor = \"rgb(243,243,243)\",\n",
        "                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 1\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
        "                                     title = \"principal component 2\",\n",
        "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
        "                        height = 600\n",
        "                       )\n",
        "                  )\n",
        "trace1 = pca_scatter(\"CANCELLED\",'red')\n",
        "trace2 = pca_scatter(\"INFORCE\",'royalblue')\n",
        "data = [trace2,trace1]\n",
        "fig = go.Figure(data=data,layout=layout)\n",
        "py.iplot(fig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmZwNeiLgSMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffkEEAsbC4EW",
        "colab_type": "text"
      },
      "source": [
        "## Feature Generation\n",
        "\n",
        "[Simple FE](https://machinelearningcoban.com/general/2017/02/06/featureengineering/)\n",
        "-    Trực tiếp lấy raw data\n",
        "-    Bag-of-words\n",
        "  -    Bag-of-Words trong Computer Vision\n",
        "-    Feature Scaling and Normalization\n",
        "  -        Rescaling\n",
        "  -        Standardization\n",
        "  -        Scaling to unit length\n",
        "\n",
        "  - Feature Generation: ở bước này tập trung các kĩ thuật để tạo ra feature: xử lí nlp, image, binning, scaling, grouping, aggregate.\n",
        "  - Kết hợp với Modelling để tìm ra nhóm feature tốt và tập trung mạnh vào đó\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfdUhklbDAQa",
        "colab_type": "text"
      },
      "source": [
        "## Feature Selection\n",
        "- Feature Selection: ở bước này tập trung các kĩ thuật chọn feature để tốt ưu mô hình\n",
        "  - Chọn được feature tốt và tối ưu được bộ nhớ\n",
        "  - Chọn được model parameters tốt nhất phù hợp với bộ features tốt nhất\n",
        "  - Kết hợp với các kĩ thuật chia dataset (CV, leave-one)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb6TIKC408SK",
        "colab_type": "text"
      },
      "source": [
        "- Tiêu chí đánh giá **metric of feature**\n",
        "  - AUC: thể hiện khả năng dự đoán\n",
        "  - Correlation: kiểm tra độ tương quan giữa feature với target hoặc với các feature quan trọng khác\n",
        "  - Converage: kiểm tra null, null nhiều thì ít thông tin\n",
        "  - Weighted of evidence, and information value:\n",
        "    - $\\ln \\left(\\frac{P(\\text {Good})}{P(\\text {Bad})}\\right)$\n",
        "    - \\begin{array}{l}{\\text { Information value }} \\\\ {\\qquad \\sum(P(G o o d)-P(B a d)) * \\ln \\left(\\frac{P(G o d)}{P(B a d)}\\right)}\\end{array}\n",
        "\n",
        "\n",
        "\n",
        "[Brute Force Approach](https://www.kdnuggets.com/2017/11/rapidminer-basic-concepts-feature-selection.html)\n",
        "Cách tiếp cận bằng các chạy thử thất cả các feature combination và so sánh trên các metrics of feature\n",
        "\n",
        "[Feature Selection](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) \n",
        "- Forwarding: ta bắt đầu với tập feature rỗng, sau đó lần lượt add thêm feature vào tập này. Nếu thấy performance của model tăng ta sẽ tiếp tục quá trình này, ngược lại sẽ dừng lại.\n",
        "- Backwarding: ta bắt đầu với toàn bộ tập feature, sau đó lần lượt remove từng feature khỏi tập này. Nếu thấy performance của model tăng hoặc giảm không quá nhiều, ta sẽ tiếp tục quá trình này, ngược lại nếu performance bị drop quá mạnh sẽ dừng lại.\n",
        "- Hybridge: kết hợp cả 2 hướng trên\n",
        "\n",
        "\n",
        "Research:\n",
        "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
        "- [2](https://www.kaggle.com/sz8416/6-ways-for-feature-selection) example selectkbes-RFE in kaggle\n",
        "- [3](https://www.kaggle.com/dkim1992/feature-selection-ranking) chua doc\n",
        "- [4](https://machinelearningmastery.com/feature-selection-machine-learning-python/) tong quat\n",
        "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "- [5](https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/) chua doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-8cO9PJ3ryk",
        "colab_type": "text"
      },
      "source": [
        "### WOE & IV\n",
        "- https://www.one-tab.com/page/0V6bYIX4ShSQmuW4bpD7Zg\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avH4UCL53u9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas.core.algorithms as algos\n",
        "from pandas import Series\n",
        "import scipy.stats.stats as stats\n",
        "import re\n",
        "import traceback\n",
        "import string\n",
        "\n",
        "max_bin = 20\n",
        "force_bin = 3\n",
        "\n",
        "# define a binning function\n",
        "def mono_bin(Y, X, n = max_bin):\n",
        "    \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
        "    r = 0\n",
        "    while np.abs(r) < 1:\n",
        "        try:\n",
        "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
        "            d2 = d1.groupby('Bucket', as_index=True)\n",
        "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
        "            n = n - 1 \n",
        "        except Exception as e:\n",
        "            n = n - 1\n",
        "\n",
        "    if len(d2) == 1:\n",
        "        n = force_bin         \n",
        "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
        "        if len(np.unique(bins)) == 2:\n",
        "            bins = np.insert(bins, 0, 1)\n",
        "            bins[1] = bins[1]-(bins[1]/2)\n",
        "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
        "        d2 = d1.groupby('Bucket', as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"MIN_VALUE\"] = d2.min().X\n",
        "    d3[\"MAX_VALUE\"] = d2.max().X\n",
        "    d3[\"COUNT\"] = d2.count().Y\n",
        "    d3[\"EVENT\"] = d2.sum().Y\n",
        "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
        "    d3=d3.reset_index(drop=True)\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def char_bin(Y, X):\n",
        "        \n",
        "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
        "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
        "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
        "    df2 = notmiss.groupby('X',as_index=True)\n",
        "    \n",
        "    d3 = pd.DataFrame({},index=[])\n",
        "    d3[\"COUNT\"] = df2.count().Y\n",
        "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
        "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
        "    d3[\"EVENT\"] = df2.sum().Y\n",
        "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
        "    \n",
        "    if len(justmiss.index) > 0:\n",
        "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
        "        d4[\"MAX_VALUE\"] = np.nan\n",
        "        d4[\"COUNT\"] = justmiss.count().Y\n",
        "        d4[\"EVENT\"] = justmiss.sum().Y\n",
        "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
        "        d3 = d3.append(d4,ignore_index=True)\n",
        "    \n",
        "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
        "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
        "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
        "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
        "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
        "    d3[\"VAR_NAME\"] = \"VAR\"\n",
        "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
        "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
        "    d3.IV = d3.IV.sum()\n",
        "    d3 = d3.reset_index(drop=True)\n",
        "    \n",
        "    return(d3)\n",
        "\n",
        "def data_vars(df1, target):\n",
        "    \n",
        "    stack = traceback.extract_stack()\n",
        "    filename, lineno, function_name, code = stack[-2]\n",
        "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
        "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
        "    \n",
        "    x = df1.dtypes.index\n",
        "    count = -1\n",
        "    \n",
        "    for i in x:\n",
        "        if i.upper() not in (final.upper()):\n",
        "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
        "                conv = mono_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i\n",
        "                count = count + 1\n",
        "            else:\n",
        "                conv = char_bin(target, df1[i])\n",
        "                conv[\"VAR_NAME\"] = i            \n",
        "                count = count + 1\n",
        "                \n",
        "            if count == 0:\n",
        "                iv_df = conv\n",
        "            else:\n",
        "                iv_df = iv_df.append(conv,ignore_index=True)\n",
        "    \n",
        "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
        "    iv = iv.reset_index()\n",
        "    return(iv_df,iv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KQqwlOh02BZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from tqdm import tqdm\n",
        "woe = pd.DataFrame()\n",
        "for col in WOE_cat_col:\n",
        "    l = []\n",
        "    t_final_iv = final_iv[final_iv.VAR_NAME == col]\n",
        "    print('> ',col)\n",
        "    print(' no of bins: ',t_final_iv.shape[0])\n",
        "    if t_final_iv.shape[0] <= 10:\n",
        "        print(t_final_iv[['VAR_NAME','MIN_VALUE']])\n",
        "    else:\n",
        "        print(t_final_iv.sample(5)[['VAR_NAME','MIN_VALUE']])\n",
        "    print('\\n')\n",
        "    for x in df_WOE[col]:\n",
        "        l.append(t_final_iv[t_final_iv.MIN_VALUE == x].WOE.values[0])\n",
        "    woe[col+'_woe'] = l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQp7xf0C5Rp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for col in WOE_numeric_col:\n",
        "    l = []\n",
        "    t_final_iv = final_iv[final_iv.VAR_NAME == col]\n",
        "    print('> ',col)\n",
        "    print(' no of bins: ',t_final_iv.shape[0])\n",
        "    if t_final_iv.shape[0] <= 10:\n",
        "        print(t_final_iv[['VAR_NAME','MIN_VALUE','MAX_VALUE']])\n",
        "    else:\n",
        "        print(t_final_iv.sample(5)[['VAR_NAME','MIN_VALUE','MAX_VALUE']])\n",
        "    print('\\n')\n",
        "    for x in df_WOE[col]:\n",
        "        l.append(t_final_iv[(t_final_iv.MIN_VALUE <= x)& (t_final_iv.MAX_VALUE >= x)].WOE.values[0])\n",
        "    woe[col+'_woe'] = l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oGlijJm5RxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmnU0NRff_ck",
        "colab_type": "text"
      },
      "source": [
        "### **RFE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7hYFfmv7cbk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "[theory](https://www.kaggle.com/nroman/recursive-feature-elimination)\n",
        "\n",
        "Trong mini course này, tôi sẽ áp dụng hướng “backwarding”. Các bước thực hiện như sau:\n",
        "\n",
        "- Đặt: n là số lần lặp feature selection, k là số feature sẽ drop ở mỗi lần lặp, p là AUC sau mỗi lần train\n",
        "Train model với XGboost\n",
        "- Lấy kết quả feature important sắp xếp giảm dần và loại ra k feature có giá trị thấp nhất\n",
        "- Lưu lại performance hiện tại để so sánh với performance tiếp theo.\n",
        "- Nếu thấp hơn ngưỡng p sẽ dừng\n",
        "- Tiếp tục quá trình selection\n",
        "\n",
        "Tuỳ theo số lượng feature và cài đặt hyper-parameter của model thì thời gian sẽ nhanh chậm khác nhau.\n",
        "\n",
        "Additional:\n",
        "- Genetic algorithm for feature selection\n",
        "\n",
        "Source:\n",
        "- [1](https://ongxuanhong.wordpress.com/2019/04/17/data-science-mini-course/#more-15645)\n",
        "- [2](https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15) doc them\n",
        "\n",
        "---\n",
        "**Boruta**\n",
        "[1](http://danielhomola.com/2015/05/08/borutapy-an-all-relevant-feature-selection-method/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UIowwhg2A3s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "lr_best = LogisticRegressionCV\n",
        "#lr_best = LogisticRegression(C=7.7, penalty='l2' ,class_weight='balanced')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2roeUhZt6FzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_selection import RFECV\n",
        "# Create the RFE object and compute a cross-validated score.\n",
        "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
        "rfecv = RFECV(estimator=lr_best, step=1, cv=10, scoring='roc_auc')\n",
        "rfecv.fit(X_train, y_train)\n",
        "\n",
        "best_features = X_train.columns[rfecv.support_].tolist()\n",
        "\n",
        "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
        "print('Selected features: %s' % best_features)\n",
        "\n",
        "\n",
        "# Plot number of features VS. cross-validation scores\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
        "plt.plot(np.arange(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfMtLkkO7fxr",
        "colab_type": "text"
      },
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mzGc0BE7xWq",
        "colab_type": "text"
      },
      "source": [
        "### Under-sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeblIals7lTa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# under-sampling 7:3\n",
        "sample = train[train.label_2==1].shape[0]/3*7\n",
        "train_US73=pd.concat([train[train.label_2!=1].sample(int(sample)),train[train.label_2==1]], axis = 0, ignore_index=True)\n",
        "print('sample shape',train_US73.shape)\n",
        "\n",
        "# under-sampling 5:5\n",
        "# sample = train[train.label_2==1].shape[0]/5*5\n",
        "# train_US55=pd.concat([train[train.label_2!=1].sample(int(sample)),train[train.label_2==1]], axis = 0, ignore_index=True\n",
        "# print('sample shape',train_US73.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHICQmTi7nqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = train[dummies_col+numeric_col]\n",
        "X_train = train_US73[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_train = train_US73['label_2']\n",
        "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_test = test['label_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4il7y5CI8O8D",
        "colab_type": "text"
      },
      "source": [
        "### Over-sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ioqMRx98Syz",
        "colab_type": "text"
      },
      "source": [
        "#### RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49htNR8a7n1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.base import BaseSampler\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, f1_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdEAQb4H8XfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = train[dummies_col+numeric_col]\n",
        "X_train = train[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_train = train['label_2']\n",
        "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_test = test['label_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EI2Up4po8Xit",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_smote = LogisticRegression()\n",
        "\n",
        "pipe = make_pipeline(RandomOverSampler(sampling_strategy=1, random_state=0), model_smote)\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwL1Z1yO8Xlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_smote_predictions = pipe.predict_proba(X_test)\n",
        "model_smote_pred_label = pipe.predict(X_test) \n",
        "model_smote_roc_score = roc_auc_score( y_test, model_smote_predictions[:,1])\n",
        "model_smote_f1_score = f1_score(y_test, model_smote_pred_label)\n",
        "print('random forest roc score on test: ', model_smote_roc_score)\n",
        "print('random forest f1 score on test: ', model_smote_f1_score)\n",
        "\n",
        "confu_matrix = confusion_matrix(y_test, model_smote_pred_label) \n",
        "sns.heatmap(confu_matrix , annot=True, fmt='d')\n",
        "print(classification_report(y_test, model_smote_pred_label) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A8WLw5B8dnc",
        "colab_type": "text"
      },
      "source": [
        "#### SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSansvBy8fIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X = train[dummies_col+numeric_col]\n",
        "X_train = train[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_train = train['label_2']\n",
        "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
        "y_test = test['label_2']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1aaJroW8fQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC)\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.base import BaseSampler\n",
        "\n",
        "rf_clf = LogisticRegression()\n",
        "\n",
        "smotes = {0 : 'SMOTE',\n",
        "          1 : 'BorderlineSMOTE',\n",
        "          2 : 'SVMSMOTE',\n",
        "          3 : 'ADASYN'}\n",
        "\n",
        "\n",
        "for i, sampler in enumerate((SMOTE(sampling_strategy = 1, random_state=0),\n",
        "                BorderlineSMOTE(sampling_strategy = 1, random_state=0, kind='borderline-1'),\n",
        "                SVMSMOTE(sampling_strategy = 1, random_state=0),\n",
        "                ADASYN(sampling_strategy = 1, random_state=0))):\n",
        "    pipe_line = make_pipeline(sampler, rf_clf)\n",
        "    pipe_line.fit(X_train, y_train)\n",
        "    rf_predictions = pipe_line.predict_proba(X_test)\n",
        "    rf_pred_label = pipe_line.predict(X_test) \n",
        "    rf_roc_score = roc_auc_score(y_test, rf_predictions[:,1])\n",
        "    rf_f1_score = f1_score(y_test, rf_pred_label)\n",
        "    print('------------------------------------------------')\n",
        "    print('SMOTE method: ', smotes[i])\n",
        "    print('random forest roc score on test: ', rf_roc_score)\n",
        "    print('random forest f1 score on test: ', rf_f1_score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzF3WgLN8fTt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q46qxVdk8fWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FIjZqKbt8P8",
        "colab_type": "text"
      },
      "source": [
        "# 1.Regression Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpc1c9ZsP2i6",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNc4fwyjgnCC",
        "colab_type": "text"
      },
      "source": [
        "Lost function: MSE\n",
        "Giải đạo hàm: \n",
        "![alt text](https://cdn.mathpix.com/snip/images/FK2Cy8U9_QURngdhSj25RQTfGExgynPNUPK0pWZTz08.original.fullsize.png)\n",
        "![alt text](https://cdn.mathpix.com/snip/images/C9R4s0LujO1M6IRNd6SfTkvsrkVTPzcy7J2-PUeuE1s.original.fullsize.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHEbcIoshU4X",
        "colab_type": "text"
      },
      "source": [
        "**Discuss & Code**\n",
        "**Hạn chế:**\n",
        "- Linear Regression là nó rất nhạy cảm với nhiễu (sensitive to noise)\n",
        "- Hạn chế thứ 2 là nó không biểu diễn được các mô hình phcứ tạp. Chúng ta biết rằng LR vẫn có thể áp dụng khi relationship giữa input và outcome là không tuyến tính, nhưng rất khó để tìm được các đặc trựng (các distribution) của từng điểm $x$\n",
        "- Trong trường hợp ma trận $A=X X^{T}$ không khả nghich:  Ta biến đổi $A=X X^{T}+\\lambda I$ với $\\lambda$ là 1 số dương rất nhỏ và ma trận đơn vị $I$. Giải nghiệm ta được,  $\\mathbf{w}=\\left(\\mathbf{X X}^{T}+\\lambda \\mathbf{I}\\right)^{-1} \\mathbf{X} \\mathbf{y}$  - còn gọi là **Ridge regression**: giúp phương trình đạo hàm có nghiệm duy nhất và tránh overfittting.\n",
        "- Không thể giải phương trình đạo hàm: Gradient descent\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl5V2OcCP6gL",
        "colab_type": "code",
        "outputId": "ec3619cf-4ffb-4679-91c0-fa1fdfa67b4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "    \n",
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src='https://nbviewer.jupyter.org/github/tiepvupsu/tiepvupsu.github.io/blob/master/assets/LR/LR.ipynb?flush_cache=true',\n",
        "       width=800, height=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"600\"\n",
              "            src=\"https://nbviewer.jupyter.org/github/tiepvupsu/tiepvupsu.github.io/blob/master/assets/LR/LR.ipynb?flush_cache=true\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fcb964af518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX6sM5ldJu0-",
        "colab_type": "text"
      },
      "source": [
        "## Perceptron Learning Algorithm (PLA)\n",
        "Bài toán Perceptron được phát biểu như sau: Cho hai class được gán nhãn, hãy tìm một đường phẳng sao cho toàn bộ các điểm thuộc class 1 nằm về 1 phía, toàn bộ các điểm thuộc class 2 nằm về phía còn lại của đường phẳng đó. Với giả định rằng tồn tại một đường phẳng như thế.\n",
        "\n",
        "Nếu tồn tại một đường phẳng phân chia hai class thì ta gọi hai class đó là linearly separable. Các thuật toán classification tạo ra các boundary là các đường phẳng được gọi chung là Linear Classifier.\n",
        "\n",
        "Source:\n",
        "[1](https://machinelearningcoban.com/2017/01/21/perceptron/),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS20_sOtJ_L1",
        "colab_type": "text"
      },
      "source": [
        ">**Thuật toán PLA**\n",
        "1. Chọn ngẫu nhiên một vector hệ số $w$ với các phần tử gần 0.\n",
        "2. Duyệt ngẫu nhiên qua từng điểm dữ liệu $x_i$:\n",
        "Nếu xi\n",
        "  *   Nễu $x_i$ được phân lớp đúng, tức $sgn(w^Tx_i)=y_i$, chúng ta không làm gì\n",
        "  *   Nếu $x_i$ bị misclassifed, cập nhật $w = w + y_i x_i$\n",
        "3. Kiểm tra xem có bao nhiêu điểm bị misclassifed. Nếu không còn điểm nào, dừng thuật toán. Nếu còn, quay lại bước 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnCFo1TyLj7l",
        "colab_type": "text"
      },
      "source": [
        "**Discuss & code**\n",
        "**Hạn chế**:\n",
        "* PLA có thể cho vô số nghiệm khác nhau: rõ ràng rằng, nếu hai class là linearly separable thì có vô số đường thằng phân cách 2 class đó.\n",
        "* PLA đòi hỏi dữ liệu linearly separable: Hai class trong ví dụ dưới đây tương đối linearly separable. Mỗi class có 1 điểm coi như nhiễu nằm trong khu vực các điểm của class kia. PLA sẽ không làm việc trong trường hợp này vì luôn luôn có ít nhất 2 điểm bị misclassified.\n",
        "\n",
        "**Cải thiện**\n",
        "* Pocket Algorithm\n",
        "Một cách tự nhiên, nếu có một vài nhiễu, ta sẽ đi tìm một đường thẳng phân chia hai class sao cho có ít điểm bị misclassified nhất. Việc này có thể được thực hiện thông qua PLA với một chút thay đổi nhỏ như sau:\n",
        "  1. Giới hạn số lượng vòng lặp của PLA.\n",
        "  2. Mỗi lần cập nhật nghiệm $w$ mới, ta đếm xem có bao nhiêu điểm bị misclassified. Nếu là lần đầu tiên, giữ lại nghiệm này trong pocket (túi quần). Nếu không, so sánh số điểm misclassified này với số điểm misclassified của nghiệm trong pocket, nếu nhỏ hơn thì lôi nghiệm cũ ra, đặt nghiệm mới này vào.\n",
        "\n",
        "Thuật toán này giống với thuật toán tìm phần tử nhỏ nhất trong 1 mảng.\n",
        "\n",
        "**Code**\n",
        "```python\n",
        "def h(w, x):    \n",
        "    return np.sign(np.dot(w.T, x))\n",
        "```\n",
        "\n",
        "[1](https://nbviewer.jupyter.org/github/tiepvupsu/tiepvupsu.github.io/blob/master/assets/pla/perceptron.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6Q6HL9RJt1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwn5AC4UPgJE",
        "colab_type": "text"
      },
      "source": [
        "## Logistic Regression\n",
        "Loss function: maximum likelihood\n",
        "\n",
        "Lấy logarit tự nhiên (cơ số e) của likelihood function biến phép nhân thành phép cộng và để tránh việc số quá nhỏ. Sau đó lấy ngược dấu để được một hàm và coi nó là hàm mất mát. Lúc này bài toán tìm giá trị lớn nhất (maximum likelihood) trở thành bài toán tìm giá trị nhỏ nhất của hàm mất mát (hàm này còn được gọi là negative log likelihood):\n",
        "\n",
        "\n",
        "$$\n",
        "J\\left(\\mathbf{w} ; \\mathbf{x}_{i}, y_{i}\\right)=-\\left(y_{i} \\log z_{i}+\\left(1-y_{i}\\right) \\log \\left(1-z_{i}\\right)\\right)\\\\\n",
        "\\frac{\\partial J\\left(\\mathbf{w} ; \\mathbf{x}_{i}, y_{i}\\right)}{\\partial \\mathbf{w}}=\\left(z_{i}-y_{i}\\right) \\mathbf{x}_{i}\n",
        "$$\n",
        "\n",
        "\n",
        "Source: [1](https://machinelearningcoban.com/2017/01/27/logisticregression/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inC4c2QlQAq_",
        "colab_type": "text"
      },
      "source": [
        "**Discuss & code**\n",
        "\n",
        "- Một điểm cộng cho Logistic Regression so với PLA là nó không cần có giả thiết dữ liệu hai class là linearly separable. Tuy nhiên, boundary tìm được vẫn có dạng tuyến tính. Vậy nên mô hình này chỉ phù hợp với loại dữ liệu mà hai class là gần với linearly separable. \n",
        "- Một kiểu dữ liệu mà Logistic Regression không làm việc được là dữ liệu mà một class chứa các điểm nằm trong 1 vòng tròn, class kia chứa các điểm bên ngoài đường tròn đó. Kiểu dữ liệu này được gọi là phi tuyến (non-linear). Sau một vài bài nữa, tôi sẽ giới thiệu với các bạn các mô hình khác phù hợp hơn với loại dữ liệu này hơn.\n",
        "- Một hạn chế nữa của Logistic Regression là nó yêu cầu các điểm dữ liệu được tạo ra một cách độc lập với nhau. Trên thực tế, các điểm dữ liệu có thể bị ảnh hưởng bởi nhau. Ví dụ: có một nhóm ôn tập với nhau trong 4 giờ, cả nhóm đều thi đỗ (giả sử các bạn này học rất tập trung), nhưng có một sinh viên học một mình cũng trong 4 giờ thì xác suất thi đỗ thấp hơn. Mặc dù vậy, để cho đơn giản, khi xây dựng mô hình, người ta vẫn thường giả sử các điểm dữ liệu là độc lập với nhau.\n",
        "\n",
        "\n",
        "**Code**\n",
        "```python\n",
        "def sigmoid(s):\n",
        "    return 1/(1 + np.exp(-s))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Lx-sy76bNX",
        "colab_type": "text"
      },
      "source": [
        "### sample code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEvR8amvOdKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss, roc_auc_score, f1_score\n",
        "import warnings\n",
        "# warnings.filterwarnings(action='once')\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOq7P_KN6gEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PDdCe5U1Chp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr_best = LogisticRegression(C=7.7, penalty='l2' ,class_weight='balanced')\n",
        "#lr_best = LogisticRegressionCV(cv=5, Cs=7, penalty='l2' ,class_weight='balanced')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IeHdRdT1CaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create regularization penalty space\n",
        "penalty = ['l1', 'l2']\n",
        "\n",
        "# Create regularization hyperparameter space\n",
        "C = np.logspace(0, 4, 10)\n",
        "\n",
        "# Create hyperparameter options\n",
        "hyperparameters = dict(C=C, penalty=penalty)\n",
        "\n",
        "logistic = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# Create grid search using 5-fold cross validation\n",
        "clf = GridSearchCV(logistic, hyperparameters, cv=5, verbose=0)\n",
        "# Fit grid search\n",
        "best_model = clf.fit(X_train, y_train)\n",
        "\n",
        "# View best hyperparameters\n",
        "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
        "print('Best C:', best_model.best_estimator_.get_params()['C'])\n",
        "c_best = best_model.best_estimator_.get_params()['C']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_ZlU2QxPkZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "X_train = X_train[best_features]\n",
        "X_test = X_test[best_features]\n",
        "\n",
        "# check classification scores of logistic regression\n",
        "#logreg = LogisticRegression()\n",
        "logreg = LogisticRegressionCV(cv=5, Cs=7, penalty='l2' ,class_weight='balanced')\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
        "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
        "print('Train/Test split results:')\n",
        "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
        "print(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\n",
        "print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n",
        "\n",
        "idx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n",
        "\n",
        "confu_matrix = pd.DataFrame(\n",
        "    confusion_matrix(y_test, y_pred, labels=[0, 1]), \n",
        "    index=['true:0', 'true:1'], \n",
        "    columns=['pred:0', 'pred:1']\n",
        "            )\n",
        "print('> confusion matrix: \\n',confu_matrix)\n",
        "print('> classification report: \\n',classification_report(y_test, y_pred) )\n",
        "\n",
        "# confu_matrix = confusion_matrix(y_test, y_pred) \n",
        "# sns.heatmap(confu_matrix , annot=True, fmt='.0f')\n",
        "# print(classification_report(y_test, y_pred) )\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\n",
        "plt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\n",
        "plt.ylabel('True Positive Rate (recall)', fontsize=14)\n",
        "plt.title('Receiver operating characteristic (ROC) curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n",
        "      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n",
        "      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJM7p4Qu6xtV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "train cutoff ratio cua model\n",
        "'''\n",
        "#Logistics Regression:\n",
        "pred_prob=logreg.predict_proba(X_train)\n",
        "search_range = np.arange(0,1,0.05)\n",
        "result = pd.DataFrame(columns = ['value','precision','recall','roc_auc','f1_score'])\n",
        "for k in search_range:\n",
        "    pred_var = [1 if i > k else 0 for i in pred_prob[:,1]]\n",
        "    result = pd.concat([result,pd.DataFrame([[k,precision_score(pred_var,y_train),recall_score(pred_var,y_train),accuracy_score(pred_var,y_train),f1_score(pred_var,y_train)]]\n",
        "                                            ,columns = ['value','precision','recall','roc_auc','f1_score'])])\n",
        "    \n",
        "result = result.reset_index()\n",
        "result.loc[result['f1_score'].reset_index().idxmax()].value\n",
        "# print('Optimal cut-off Value = ' + str(k))\n",
        "def plot_act_ratio(df,x,k,y):\n",
        "    plt.figure(figsize =(20,10))\n",
        "    sns.lineplot(x=x, y=y,data=df,sort=True)\n",
        "    sns.lineplot(x=x, y=k,data=df,sort=True)\n",
        "    plt.show()\n",
        "plot_act_ratio(result,result.value.to_list(),result.precision.to_list(),result.f1_score)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCGO-WQf6xxZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "over-sampling trên logistic regression cross valiadation\n",
        "'''\n",
        "\n",
        "rf_clf = LogisticRegressionCV(cv=5, Cs=8, penalty='l2' ,class_weight='balanced')\n",
        "\n",
        "smotes = {0 : 'SMOTE',\n",
        "          1 : 'BorderlineSMOTE',\n",
        "          2 : 'SVMSMOTE',\n",
        "          3 : 'ADASYN'}\n",
        "\n",
        "\n",
        "for i, sampler in enumerate((SMOTE(sampling_strategy = 1, random_state=0),\n",
        "                BorderlineSMOTE(sampling_strategy = 1, random_state=0, kind='borderline-1'),\n",
        "                SVMSMOTE(sampling_strategy = 1, random_state=0),\n",
        "                ADASYN(sampling_strategy = 1, random_state=0))):\n",
        "    pipe_line = make_pipeline(sampler, rf_clf)\n",
        "    pipe_line.fit(X_train, y_train.label_1)\n",
        "    rf_predictions = pipe_line.predict_proba(X_test)\n",
        "    rf_pred_label = pipe_line.predict(X_test) \n",
        "    rf_roc_score = roc_auc_score(y_test, rf_predictions[:,1])\n",
        "    rf_f1_score = f1_score(y_test, rf_pred_label)\n",
        "    print('------------------------------------------------')\n",
        "    print('SMOTE method: ', smotes[i])\n",
        "    print('random forest roc score on test: ', rf_roc_score)\n",
        "    print('random forest f1 score on test: ', rf_f1_score)\n",
        "    cmtx = pd.DataFrame(\n",
        "    confusion_matrix(y_test, rf_pred_label, labels=[0, 1]), \n",
        "    index=['true:0', 'true:1'], \n",
        "    columns=['pred:0', 'pred:1']\n",
        "            )\n",
        "    print('confusion matrix: \\n', cmtx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_jBWMbC6x0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4C_v3AS7GWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhlcHGFT7GlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDHLmr9DPmak",
        "colab_type": "text"
      },
      "source": [
        "## Softmax Regression\n",
        "Loss function: xây dựng bởi entropy\n",
        "$$J(\\mathbf{W} ; \\mathbf{X}, \\mathbf{Y})=-\\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{j i} \\log \\left(a_{j i}\\right)=-\\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{j i} \\log \\left(\\frac{\\exp \\left(\\mathbf{w}_{j}^{T} \\mathbf{x}_{i}\\right)}{\\sum_{k=1}^{C} \\exp \\left(\\mathbf{w}_{k}^{T} \\mathbf{x}_{i}\\right)}\\right)\\\\\n",
        "\\frac{\\partial J(\\mathbf{W})}{\\partial \\mathbf{W}}=\\sum_{i=1}^{N} \\mathbf{x}_{i} \\mathbf{e}_{i}^{T}=\\mathbf{X} \\mathbf{E}^{T}\\\\\n",
        "where,\\\\\n",
        "\\mathbf{E}=\\mathbf{A}-\\mathbf{Y}\\\\\n",
        "a_{i}=\\frac{\\exp \\left(z_{i}\\right)}{\\sum_{j=1}^{C} \\exp \\left(z_{j}\\right)}, \\quad \\forall i=1,2, \\ldots, C\n",
        "$$\n",
        "\n",
        "Khi $C=2$, Softmax Regression và Logistic Regression là giống nhau\n",
        "\n",
        "Source: \n",
        "[1](https://machinelearningcoban.com/2017/02/17/softmax/),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaWaipRwWWcF",
        "colab_type": "text"
      },
      "source": [
        "**Code**\n",
        "```python\n",
        "import numpy as np \n",
        "def softmax(Z):\n",
        "    \"\"\"\n",
        "    Compute softmax values for each sets of scores in V,   each column of V is a set of score.    \n",
        "    \"\"\"\n",
        "    e_Z = np.exp(Z)\n",
        "    A = e_Z / e_Z.sum(axis = 0)\n",
        "    return A\n",
        "\n",
        "def softmax_stable(Z):\n",
        "    \"\"\"\n",
        "    Compute softmax values for each sets of scores in Z.\n",
        "    each column of Z is a set of score.    \n",
        "    \"\"\"\n",
        "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
        "    A = e_Z / e_Z.sum(axis = 0)\n",
        "    return A\n",
        " \n",
        "  \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kz3zCJNjApN",
        "colab_type": "text"
      },
      "source": [
        "## SVM\n",
        "Support vector machine\n",
        "* Với bài toán binary classification mà 2 classes là linearly separable, có vô số các siêu mặt phẳng giúp phân biệt hai classes, tức mặt phân cách. Với mỗi mặt phân cách, ta có một classifier. Khoảng cách gần nhất từ 1 điểm dữ liệu tới mặt phân cách ấy được gọi là margin của classifier đó.\n",
        "* Việc này có thể được tổng quát lên không gian nhiều chiều: Khoảng cách từ một điểm (vector) có toạ độ $\\mathbf{x_0}$ tới siêu mặt phẳng (hyperplane) có phương trình $\\mathbf{w}^{T} \\mathbf{x}+b=0$ được xác định bởi: \n",
        "$$\\frac{\\left|\\mathbf{w}^{T} \\mathbf{x}_{0}+b\\right|}{\\|\\mathbf{w}\\|_{2}}$$\n",
        "    với $\\|\\mathbf{w}\\|_{2}=\\sqrt{\\sum_{i=1}^{d} w_{i}^{2}}$ , d là số chiều không gian\n",
        "* Support Vector Machine là bài toán đi tìm mặt phân cách sao cho margin tìm được là lớn nhất, đồng nghĩa với việc các điểm dữ liệu an toàn nhất so với mặt phân cách.\n",
        "* Bài toán tối ưu trong SVM là một bài toán lồi với hàm mục tiêu là stricly convex, nghiệm của bài toán này là duy nhất. Hơn nữa, bài toán tối ưu đó là một Quadratic Programming (QP).\n",
        "* Mặc dù có thể trực tiếp giải SVM qua bài toán tối ưu gốc này, thông thường người ta thường giải bài toán đối ngẫu (strong duality) . Bài toán đối ngẫu cũng là một QP nhưng nghiệm là sparse nên có những phương pháp giải hiệu quả hơn.\n",
        "\n",
        "Một số sự tương tự giữa SVM và NN\n",
        "![alt text](https://cdn.mathpix.com/snip/images/XFeNXnSwVrber9Q6PWSnjm7nb8WLC8TxkavH6nCKpQs.original.fullsize.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Source\n",
        "[1](https://machinelearningcoban.com/2017/04/09/smv/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhSsF_0teLnU",
        "colab_type": "text"
      },
      "source": [
        "**[Soft Margin SVM](https://machinelearningcoban.com/2017/04/13/softmarginsmv/#-tom-tat-va-thao-luan)**\n",
        "* SVM thuần (Hard Margin SVM) hoạt động không hiệu quả khi có nhiễu ở gần biên hoặc thậm chí khi dữ liệu giữa hai lớp gần linearly separable. Soft Margin SVM có thể giúp khắc phục điểm này.\n",
        "* Trong Soft Margin SVM, chúng ta chấp nhận lỗi xảy ra ở một vài điểm dữ liệu. Lỗi này được xác định bằng khoảng cách từ điểm đó tới đường biên tương ứng. Bài toán tối ưu sẽ tối thiểu lỗi này bằng cách sử dụng thêm các biến được gọi là slack varaibles.\n",
        "\n",
        "**Kernel SVM**\n",
        "* Nếu dữ liệu của hai lớp là không phân biệt tuyến tính (nonlinearly separable), chúng ta có thể tìm cách biến đổi dữ liệu sang một không gian mới sao cho trong không gian mới ấy, dữ liệu của hai lớp là phân biệt tuyến tính hoặc gần phân biệt tuyến tính.\n",
        "* Việc tính toán trực tiếp hàm $\\Phi()$ đôi khi phức tạp và tốn nhiều bộ nhớ. Thay vào đó, ta có thể sử dụng kernel trick. Trong cách tiếp cận này, ta chỉ cần tính tích vô hướng của hai vector bất kỳ trong không gian mới:$k(\\mathbf{x}, \\mathbf{z})=\\Phi(\\mathbf{x})^{T} \\Phi(\\mathbf{z})$\n",
        "\n",
        "**Multi-class SVM**\n",
        "* Giống như Softmax Regression, Multi-class SVM vẫn được coi là một bộ phân lớp tuyến tính vì đường phân chia giữa các class là các đường tuyến tính.\n",
        "* Kernel SVM cũng hoạt động khá tốt, nhưng việc tính toán ma trận kernel có thể tốn nhiều thời gian và bộ nhớ. Hơn nữa, việc mở rộng nó ra cho bài toán multi-class classification thường không hiệu quả bằng Multi-class SVM. \n",
        "* Một ưu điểm nữa của Multi-class SVM là nó có thể được tối ưu bằng (Stochastic) Gradient Descnet, tức là nó phù hợp với các bài toán large-scale. Việc boundary giữa các class là tuyến tính có thể được giải quyết bằng cách kết hợp nó với Deep Neurel Networks. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvMJsDrP1ua9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j1ygYkdutFv",
        "colab_type": "text"
      },
      "source": [
        "# 2.Instance-based (memory-based)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1MEZV-Js4Yq",
        "colab_type": "text"
      },
      "source": [
        "## K-nearest neighbors (KNN)\n",
        "Không học mà chỉ nhớ một cách máy móc toàn bộ dữ liệu. KKN tìm đầu ra bằng thông tin của K điểm gần nhất.\n",
        "\n",
        "Input dữ liệu đầu vào và số lượng  nhóm (K), với 1 điểm tính toán khoảng cách từ 1 điểm dữ liệu nhỏ nhất tới tất cả của dữ liệu đầu vào N điểm, và chọn ra K khoảng cách nhỏ nhất.\n",
        "\n",
        "**2 vấn đề chính:**\n",
        "- vấn đề về cách đo các điểm, thì các điểm thường được thể hiện bằng 1 vector đặc trưng, nên $l2-norm$ hay Euclid là cách đo khoảng cách thường được sử dụng\n",
        "- vấn đề về tính toán, khi tập huấn luyện lớn và vector đặc trựng lớn $X \\in R^{d*N}$. KNN sẽ phải tính toán khoảng cách từ 1 điểm dữ liệu nhỏ nhất tới tất cả N điểm, và chọn ra K khoảng cách nhỏ nhất (khối lượng tính toán lớn)\n",
        "\n",
        "Xem thêm trang 103-104/ML cơ bản, để biết các optimize cách tính: *1. Khoảng cách từ 1 điểm tới từng điểm trong 1 tập hợp, 2. khoảng cách giữa từng cặp điểm trong 2 tập hợp*. Đối với GPU, xem thêm [faiss](https://github.com/facebookresearch/faiss)\n",
        "\n",
        "Source: \n",
        "[1](https://machinelearningcoban.com/2017/01/08/knn/),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg_5JeHX3pfX",
        "colab_type": "text"
      },
      "source": [
        "** Discuss & Code**\n",
        "Ưu điểm của KNN:\n",
        " - Độ phức tạp tính toán của quá trình training là bằng 0.\n",
        " - Việc dự đoán kết quả của dữ liệu mới rất đơn giản.\n",
        " - Không cần giả sử gì về phân phối của các class.\n",
        "\n",
        "Nhược điểm của KNN\n",
        " - KNN rất nhạy cảm với nhiễu khi K nhỏ.\n",
        " - Như đã nói, KNN là một thuật toán mà mọi tính toán đều nằm ở khâu test. Trong đó việc tính khoảng cách tới từng điểm dữ liệu trong training set sẽ tốn rất nhiều thời gian, đặc biệt là với các cơ sở dữ liệu có số chiều lớn và có nhiều điểm dữ liệu. Với K càng lớn thì độ phức tạp cũng sẽ tăng lên. Ngoài ra, việc lưu toàn bộ dữ liệu trong bộ nhớ cũng ảnh hưởng tới hiệu năng của KNN.\n",
        "\n",
        "Tăng tốc cho KNN\n",
        "\n",
        "Ngoài việc tính toán khoảng cách từ một điểm test data đến tất cả các điểm trong traing set (Brute Force), có một số thuật toán khác giúp tăng tốc việc tìm kiếm này. Bạn đọc có thẻ tìm kiếm thêm với hai từ khóa: K-D Tree và Ball Tree. Tôi xin dành phần này cho độc giả tự tìm hiểu, và sẽ quay lại nếu có dịp. Chúng ta vẫn còn những thuật toán quan trọng hơn khác cần nhiều sự quan tâm hơn.\n",
        "\n",
        "**Code**\n",
        "\n",
        "Sử dụng thư viện `sklearn.neighbors.KNeighborsClassifier()`\n",
        "\n",
        "Mặc định `weights = 'uniform'`, nếu k = 10 thì vãi trò của mỗi điểm là như nhau để xét cho điểm hiện tại. \n",
        "- Để cải thiện chúng ta có thể đánh giá theo khoảng cách `weights = 'distance'` (càng lân cận càng tin tưởng)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TBaXY_Mt2nt",
        "colab_type": "code",
        "outputId": "45102472-21b6-4921-e0f4-6bd676e500a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        }
      },
      "source": [
        "# KNN    \n",
        "from IPython.display import IFrame\n",
        "IFrame(src='https://nbviewer.jupyter.org/github/tiepvupsu/ebookML_src/blob/master/src/knn/KNN.ipynb',\n",
        "       width=800, height=600)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"600\"\n",
              "            src=\"https://nbviewer.jupyter.org/github/tiepvupsu/ebookML_src/blob/master/src/knn/KNN.ipynb\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fcb964af748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np0HDTN556XV",
        "colab_type": "text"
      },
      "source": [
        "# 3.Clustering Algorithms\n",
        "\n",
        "\n",
        "*   K-means clustering\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OJRGMNB5xXU",
        "colab_type": "text"
      },
      "source": [
        "## K-means clustering\n",
        "- unsupervised learning\n",
        "\n",
        "Input là dữ liệu đầu vào và số lượng nhóm ($K$), tìm nhóm cho từng điểm dữ liệu và có $K$ nhóm\n",
        "\n",
        "Source:\n",
        "[1](https://machinelearningcoban.com/2017/01/01/kmeans/),\n",
        "[2](https://machinelearningcoban.com/2017/01/04/kmeans2/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWXkmnIm8Cow",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://cdn.mathpix.com/snip/images/PT2D3fLs4a_7od0kWH8kWo1EPA41yKK6GI-HV0OBQ_4.original.fullsize.png)\n",
        "\n",
        "**Tối ưu hàm mất mát**:\n",
        "- Cố định M, tìm Y: nghĩa là giả sử các centroid, hay tìm các label vector để hàm mất mát nhỏ nhất.\n",
        "- Cố định Y, tìm M: giả sử đã tìm được cluster cho từng điểm, hãy tìm centroid mới cho mỗi cluster đề hàm mất mát đạt giá trị nhỏ nhất\n",
        "\n",
        "\n",
        ">**Thuật toán k-means clustering**:\n",
        "- Input:Ma trận dữ liệu $X \\in R^{d*N}$\n",
        "- Output: ma trận centroid $M \\in R^{d*K}$ và ma trận label $Y \\in R^{N*K}$\n",
        ">\n",
        ">> 1.   Chọn K điểm bất kì làm trong training set là centroid ban đầu\n",
        "2.   Phân mỗi điểm dữ liệu vào cluster có centroid gần nó nhất\n",
        "3. Nếu việc phân nhóm dữ liệu vào từng cluster ở bước 2 không thay đổi so với vong lặp trước thì -> dừng thuật toán\n",
        "4. Cập nhật centroid cho từng cluster: bằng cách lấy trung bình cộng của tất cả các điểm dữ liệu đã được gán vào cluster đó sau bước 2\n",
        "5. Back -> step 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hlSFmgkEy_p",
        "colab_type": "text"
      },
      "source": [
        "**Discuss & code**\n",
        "Hạn chế\n",
        "*    Chúng ta cần biết số lượng cluster cần clustering\n",
        "*   Nghiệm cuối cùng phụ thuộc vào các centers được khởi tạo ban đầu\n",
        "*    Các cluster cần có só lượng điểm gần bằng nhau\n",
        "*    Các cluster cần có dạng hình tròn\n",
        "*    Khi một cluster nằm phía trong 1 cluster khác\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAKVxJ5V6Cjd",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8gx540rhd0Q",
        "colab_type": "text"
      },
      "source": [
        "# 4.Reduce Dimension\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF025S_Lgd07",
        "colab_type": "text"
      },
      "source": [
        "### Singular Value Decomposition (SVD)\n",
        "![alt text](https://cdn.mathpix.com/snip/images/JXD8GZE0T-iEvom84u0TFQl4QXzjm6cM2fC0n92kgFk.original.fullsize.png)\n",
        "\n",
        "Chúng ta có thể đơn giản hóa giữ liệu bằng cách lựa chọn giá trị $k$ rank\n",
        "Best rank là có thể được tính bằng xấp xỉ  Frobenius norm và norm 2.\n",
        "\n",
        "Frobenius norm và norm 2 là hai norms được sử dụng nhiều nhất trong ma trận. Như vậy, xét trên cả hai norm này, Truncated SVD đều cho xấp xỉ tốt nhất. Vì vậy Truncated SVD còn được gọi là Best low-rank Approximation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc9w3obZjBTW",
        "colab_type": "text"
      },
      "source": [
        "## Principal Component Analysis\n",
        "\n",
        "Cách đơn giản nhất để giảm chiều dữ liệu từ $D$ về$K<D$ là chỉ giữ lại $K$ phần tử quan trọng nhất. Tuy nhiên, việc làm này chắc chắn chưa phải tốt nhất vì chúng ta chưa biết xác định thành phần nào là quan trọng hơn. Hoặc trong trường hợp xấu nhất, lượng thông tin mà mỗi thành phần mang là như nhau, bỏ đi thành phần nào cũng dẫn đến việc mất một lượng thông tin lớn.\n",
        "\n",
        "Tuy nhiên, nếu chúng ta có thể biểu diễn các vector dữ liệu ban đầu trong một hệ cơ sở mới mà trong hệ cơ sở mới đó, tầm quan trọng giữa các thành phần là khác nhau rõ rệt, thì chúng ta có thể bỏ qua những thành phần ít quan trọng nhất.\n",
        "\n",
        "---\n",
        "\n",
        "**Thuật toán PCA** \n",
        "\n",
        "> ![alt text](https://cdn.mathpix.com/snip/images/aNj8bx_bo8CPSnN5AN3jPHiThzuei2ioOB27eJr6Aqk.original.fullsize.png)"
      ]
    }
  ]
}