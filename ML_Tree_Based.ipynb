{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Tree Based.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/ML_Tree_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXna41cWVA9d",
        "colab_type": "toc"
      },
      "source": [
        ">[Decision Tree](#scrollTo=noBJeC_EK5qB)\n",
        "\n",
        ">>[CART](#scrollTo=awesPEQf-Ade)\n",
        "\n",
        ">>>[Công thức Goodness of slit](#scrollTo=awesPEQf-Ade)\n",
        "\n",
        ">>>[GINI](#scrollTo=awesPEQf-Ade)\n",
        "\n",
        ">>>[Chi-Square](#scrollTo=awesPEQf-Ade)\n",
        "\n",
        ">>>[Reduction in Variance](#scrollTo=iOUGGkW5e68V)\n",
        "\n",
        ">>[Iterative Dichotomiser 3 (ID3)](#scrollTo=RI06iOw99-Um)\n",
        "\n",
        ">>[C4.5/ C5.0](#scrollTo=RI06iOw99-Um)\n",
        "\n",
        ">>[Overfitting and Underfitting in simple tree based](#scrollTo=YAFQq3RiOsmg)\n",
        "\n",
        ">>>[So sánh CART vs ID3/C.45](#scrollTo=YAFQq3RiOsmg)\n",
        "\n",
        ">[Random Forest](#scrollTo=o77WoH98dYfG)\n",
        "\n",
        ">>>[Ý tưởng:](#scrollTo=8edR2-G0F7J4)\n",
        "\n",
        ">>>[Features importance](#scrollTo=jmnOhZESZUXQ)\n",
        "\n",
        ">>[Para Tuning](#scrollTo=OCmgxgTyvD3R)\n",
        "\n",
        ">>[Code](#scrollTo=ym-_dbrR4FoA)\n",
        "\n",
        ">[Boosting model](#scrollTo=zUJz36UqK5pr)\n",
        "\n",
        ">>>[Ada Boost](#scrollTo=Bxjoz6gBv8-D)\n",
        "\n",
        ">>>[Gradient Boosting Model (GBM)](#scrollTo=0mBr8fl6NIni)\n",
        "\n",
        ">>>[XGB](#scrollTo=FVuGEej0hDIk)\n",
        "\n",
        ">>>[LightGB](#scrollTo=Y50lpKa9-nLW)\n",
        "\n",
        ">>>[CatBoost](#scrollTo=qvL0DkpX7UcS)\n",
        "\n",
        ">[Sample Code](#scrollTo=58emi3haTbms)\n",
        "\n",
        ">>>[Model tổng hợp](#scrollTo=l2lAzxrgg_xL)\n",
        "\n",
        ">[Tunning Parameters](#scrollTo=Ln9RSYLwM4ZE)\n",
        "\n",
        ">>[Bayesian Optimization](#scrollTo=QxqhoXbjNrX7)\n",
        "\n",
        ">>[RandomSearchCV](#scrollTo=0msNr1tVCgBV)\n",
        "\n",
        ">[Cross Validation/ Ensemble](#scrollTo=vEZFRbUXpUY5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "noBJeC_EK5qB"
      },
      "source": [
        "# Decision Tree\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awesPEQf-Ade",
        "colab_type": "text"
      },
      "source": [
        "## CART\n",
        "- Sử dụng các phương pháp phân nhánh như (split stategies): Gini index or Goodness of split \n",
        "\n",
        "### 1. Công thức **Goodness of slit** \n",
        "-> dựa trên ý tưởng của việc tim mức độ đồng nhất \"purity\" ở mỗi node\n",
        "$$\\phi(s|t)=2P_LP_R\\sum_{j}^{\\# of classes}(abs(P(j|t_L)-P(j|t_R)))$$\n",
        "với $s$ là số lượng observations (số lượng điểm dữ liệu sẽ thay đổi sau mỗi lần phân nhánh), t là cách thức phân nhánh\n",
        "- Ý tưởng:\n",
        "  - Với mỗi feature/cột phân thành trái và phải theo cách chọn: (bên trái feature_1 = x ) và (bên phải feature_1 != x), tính **Goodness of slit** cho từng giá trị phân loại (giá trị unique) của tất cả dữ liệu.\n",
        "  - Chọn giá trị có **Goodness of slit** cao nhất làm root note, và tiếp tục tính lại và chọn cho đến khi hết từng giá trị unique của dữ liệu\n",
        "  - Chọn giá trị phân nhánh, chon và dừng ở phần giá trị PURE (chỉ bằng 0 hoặc 1), tiếp tục tính lại và phân nhánh ở phần có lẫn lộn\n",
        "  - Tốt: Chi tiết cho từng giá trị của dữ liệu, đạt độ chính xác cao\n",
        "  - Hạn chế: Quá chi tiết, chỉ phù hợp với dữ liệu mang tính phân loại (các giá trị của unique values/ số lượng giá trị phân loại nhỏ)\n",
        "\n",
        "\n",
        "### 2. GINI\n",
        "$$GINI(t) = 1- \\sum_j^{\\#ofclasses}(p(j|t))^2 \\\\\n",
        "GINI_{split}=\\sum^K_{j=i}\\frac{n_i}{n}GINI(i)\n",
        "$$\n",
        "với K là 1 hệ số phân chia, bằng với số unique trong biến phân loại, thông thường $K = (2*n)^{1/3}$\n",
        "- Ý tương:\n",
        "  - Với mỗi feature/cột, nếu feature cos giá trị unique thấp thì áp dụng trực tiếp tính GINI_split, nếu feature có giá trị unique cao (ví dụ các biến liên tục, tuổi, lương..) sử dụng hệ số K để phân chia r tính GINI_split\n",
        "  - Chọn cột có GINI_split thấp nhất để làm root note, và tiếp tục cho các lần sau.\n",
        "  - Chọn giá trị phân nhánh, chon và dừng ở phần giá trị PURE (chỉ bằng 0 hoặc 1), tiếp tục tính lại và phân nhánh ở phần có lẫn lộn\n",
        "  - Tốt: áp dụng được cho cả biến phân loại (định tính/ categorical feature) và biến định lượng (liên tục / continous)\n",
        "\n",
        "### 3. Chi-Square\n",
        "\n",
        "Thuật toán tìm ý nghĩa thống kê giữa sự khác nhau của node con và node cha. Chi-square được tính bằng cách tính tổng bình phương độ lệch chuẩn giữa các quan sát và kì vọng tần suất của biến phụ thuộc\n",
        "\n",
        "\n",
        "Sources:\n",
        "- [CART-goodness of split-1](https://bigdatauni.com/vi/tin-tuc/thuat-toan-cay-quyet-dinh-classification-regression-tree-cart-p-1.html)\n",
        "- [GINI vidu](https://bigdatauni.com/vi/tin-tuc/thuat-toan-cay-quyet-dinh-p-2-cart-gini-index.html?fbclid=IwAR3Zqc2ju1l5mqS0pzt_WYEXbuB1JRW-K7Pvrg7UNhVwGdOHasFVDii0-_8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUGGkW5e68V",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "### Reduction in Variance\n",
        "\n",
        "Cả ba thuật toán trên đều áp dụng cho Categorical Decision Tree. Reduction in Varicance là thuật toán sử dụng cho Regression Decision Tree. Thuật toán sử dụng phương sai để chọn việc phân nhánh. Phân nhánh nào có phương sai nhỏ hơn thì sẽ được chọn. Công thức tính như sau"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI06iOw99-Um",
        "colab_type": "text"
      },
      "source": [
        "##  Iterative Dichotomiser 3 (ID3) \n",
        "- Sử dụng: Entropy function and Information Gain để kiểm tra\n",
        "\n",
        "\n",
        "ID3 là một thuật toán decision tree được áp dụng cho các bài toán classification mà tất cả các thuộc tính đều ở dạng categorical.\n",
        "**Ý tưởng:** Trong ID3, chúng ta cần xác định thứ tự của thuộc tính cần được xem xét tại mỗi bước. Ở mỗi bước chúng ta chọn thuộc tính tốt nhất để chia *child node* .\n",
        "\n",
        "*Trước hết, thế nào là một phép phân chia tốt?* Bằng trực giác, một phép phân chia là tốt nhất:\n",
        "- Nếu dữ liệu trong mỗi child node hoàn toàn thuộc vào một class–khi đó child node này có thể được coi là một leaf node, tức ta không cần phân chia thêm nữa.\n",
        "- Nếu dữ liệu trong các child node vẫn lẫn vào nhau theo tỉ lệ lớn, ta coi rằng phép phân chia đó chưa thực sự tốt.\n",
        "\n",
        "Từ nhận xét này, ta cần có một hàm số đo độ tinh khiết (purity), hoặc độ vẩn đục (impurity) của một phép phân chia. Hàm số này sẽ cho giá trị thấp nhất nếu dữ liệu trong mỗi child node nằm trong cùng một class (tinh khiết nhất), và cho giá trị cao nếu mỗi child node có chứa dữ liệu thuộc nhiều class khác nhau.\n",
        "\n",
        "**Công thức Entropy & IG**\n",
        "$$Entropy(t) = -\\sum_j^{\\#ofclasses}P(j|t)log_2P(j|t)$$\n",
        "Ý nghĩa, giá trị Entropy càng nhỏ thì tức node hay tập con chứa nhiều đối tượng dữ liệu có cùng thuộc tính j bất kỳ $P(j|t)$ sẽ lớn.\n",
        "\n",
        "**Information Gain** công thức để xác định trong số các cách thức phân nhánh thì cách nào đem lại được nhiều thông tin nhất.\n",
        "$$IG_{split}=Entropy(S)-(\\sum_{i=1}^K \\frac{n_i}{n}Entropy(i))$$\n",
        "với, $Entropy(S)$ là entropy của biến mục tiêu (target feature)\n",
        "- Chọn feature/cột có IG cao nhất để phân nhánh, phân nhánh cho tất cả giá trị unique trong cột, nếu phần nào chưa PURE thì tiếp tục tính và phân chia.\n",
        "- IG có hạn chế lớn, trong việc xử lí các biến có tính phân loại cao (ID, các mô tả chi tiết), không thể phân nhánh cho tất cả giá trị dẫn đến overfitting. Do dó IG có khuynh hướng \"bias\" thiên vị về các feature có nhiều unique values/dữ liệu khác nhau.\n",
        "\n",
        "\n",
        "## C4.5/ C5.0\n",
        "Sử dụng thêm công thức Gain ratio dể khác phục khuyết điểm của IG ở ID3\n",
        "**Công thức Gain ratio**\n",
        "$$\n",
        "\\text {GainRATIO}_{\\text {split }}=\\frac{IG_{\\text {split }}}{\\text {SplitINFO}=-\\sum_{i=1}^{k} \\frac{n_{i}}{n} \\log _{2} \\frac{n_{i}}{n}}\n",
        "$$\n",
        "$\\text{SplitINFO}$ chính là thông tin có được về số lượng đối ượng dữ liệu trong mỗi nhánh\n",
        "\n",
        "\n",
        "**Source**\n",
        "[tree based model](https://forum.machinelearningcoban.com/t/tree-based-modeling-phan-2/2320),\n",
        "[id3](https://machinelearningcoban.com/2018/01/14/id3/),\n",
        "[id3 ví dụ](https://viblo.asia/p/cay-quyet-dinh-decision-tree-RnB5pXWJ5PG),\n",
        "[id3,gain ratio](https://bigdatauni.com/vi/tin-tuc/thuat-toan-cay-quyet-dinh-p-3-c4-5-entropy.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAFQq3RiOsmg",
        "colab_type": "text"
      },
      "source": [
        "## Overfitting and Underfitting in simple tree based\n",
        "Đối với các bài toán CART nếu phân nhánh quá nhiều (cấy quá deep) sẽ dẫn đến overfitting. Thi sẽ có cách Pruning Tree để giảm bớt chiều sâu nhưng vẫn đảm bảo model không bị underfitting.\n",
        "\n",
        "### So sánh CART vs ID3/C.45\n",
        "- Khác với CART, ID3/C.45 không bị giới hạn bởi số nhánh có thể phân trong mỗi lần (ở CART split chỉ = 2), ID3/C.45 có thể phân nhiều nhánh ở mỗi tầng\n",
        "- CART sử dụng Goodness of slipt or GINI index, còn ID3/C.45 sử dụng công thức Entropy với IG, hoặc Gain ratio dể phân tách\n",
        "\n",
        "**Source**\n",
        "- [Đọc để biết điểm mạnh/yếu của thuật toán descision tree, và cách khắc phục cơ bản](https://bigdatauni.com/vi/tin-tuc/thuat-toan-cay-quyet-dinh-p-4-uu-khuyet-diem-stopping-pruning-method.html)\n",
        "- [đọc thêm](https://bigdatauni.com/vi/tin-tuc/thuat-toan-cay-quyet-dinh-p-5-regression-tree-va-decision-rules.html)\n",
        "\n",
        "[**A Complete Tutorial on Tree Based Modeling from Scratch**](https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/#five)\n",
        "1.  What is a Decision Tree? How does it work?\n",
        "2. Regression Trees vs Classification Trees\n",
        "3.  How does a tree decide where to split?\n",
        "4.  What are the key parameters of model building and how can we avoid over-fitting in decision trees?\n",
        "5. Are tree based models better than linear models?\n",
        "> “If I can use logistic regression for classification problems and linear regression for regression problems, why is there a need to use trees”? Many of us have this question. And, this is a valid one too.\n",
        "Actually, you can use any algorithm. It is dependent on the type of problem you are solving. Let’s look at some key factors which will help you to decide which algorithm to use:\n",
        "> - If the relationship between dependent & independent variable is well approximated by a linear model, linear regression will outperform tree based model.\n",
        "> - If there is a high non-linearity & complex relationship between dependent & independent variables, a tree model will outperform a classical regression method.\n",
        "> - If you need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression!\n",
        "6.  Working with Decision Trees in R and Python\n",
        "7.  What are the ensemble methods of trees based model?\n",
        ">Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models.\n",
        "8.  What is Bagging? How does it work?\n",
        ">Bagging is a technique used to reduce the variance of our predictions by combining the result of multiple classifiers modeled on different sub-samples of the same data set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o77WoH98dYfG",
        "colab_type": "text"
      },
      "source": [
        "#  Random Forest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8edR2-G0F7J4",
        "colab_type": "text"
      },
      "source": [
        "9.What is Random Forest ? How does it work?\n",
        "\n",
        "Random Forest is considered to be a panacea of all data science problems. On a funny note, when you can’t think of any algorithm (irrespective of situation), use random forest!\n",
        "\n",
        "Random Forest is a versatile machine learning method capable of performing both regression and classification tasks. **It also undertakes dimensional reduction methods, treats missing values, outlier values and other essential steps of data exploration, and does a fairly good job.** It is a type of ensemble learning method, where a group of weak models combine to form a powerful model.\n",
        "\n",
        "### **Ý tưởng:**\n",
        "To classify a new object based on attributes, each tree gives a classification and we say the tree “votes” for that class. The forest chooses the classification having the most votes (over all the trees in the forest) and in case of regression, it takes the average of outputs by different trees.\n",
        "It works in the following manner. Each tree is planted & grown as follows:\n",
        "- Assume number of cases in the training set is N. Then, sample of these N cases is taken at random but with replacement. This sample will be the training set for growing the tree.\n",
        "- If there are M input variables, a number $m< M$ is specified such that at each node, m variables are selected at random out of the M. The best split on these m is used to split the node. The value of m is held constant while we grow the forest.\n",
        "- Each tree is grown to the largest extent possible and  there is no pruning.\n",
        "- Predict new data by aggregating the predictions of the ntree trees (i.e., majority votes for classification, average for regression).\n",
        "\n",
        "Sources:\n",
        "- [RF Example](https://www.analyticsvidhya.com/blog/2014/06/introduction-random-forest-simplified/)\n",
        "- [Comparing a CART model to Random Forest (Part 1)](https://www.analyticsvidhya.com/blog/2014/06/comparing-cart-random-forest-1/)[, (Part 2)](https://www.analyticsvidhya.com/blog/2014/06/comparing-random-forest-simple-cart-model/)\n",
        "- [Code mẫu](https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html)\n",
        "- [Code mẫu 2](https://towardsdatascience.com/random-forests-and-decision-trees-from-scratch-in-python-3e4fa5ae4249)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmnOhZESZUXQ",
        "colab_type": "text"
      },
      "source": [
        "### Features importance\n",
        "- [Beware Default Random Forest Importances](https://explained.ai/rf-importance/index.html)\n",
        "- [The Mathematics of Decision Trees, Random Forest and Feature Importance](https://towardsdatascience.com/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCmgxgTyvD3R",
        "colab_type": "text"
      },
      "source": [
        "## Para Tuning\n",
        "- [In Depth: Parameter tuning for Random Forest](https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d)\n",
        "- [Hyperparameter Tuning the Random Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym-_dbrR4FoA",
        "colab_type": "text"
      },
      "source": [
        "## Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG6d839GCYyW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufk6-2Ib4Ds6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2 , random_state=24)\n",
        "\n",
        "rf = RandomForestClassifier(\n",
        "    random_state=42,\n",
        "    n_estimators= 1000,\n",
        "    min_samples_split= 20,\n",
        "    min_samples_leaf= 10,\n",
        "    max_features='sqrt',\n",
        "    max_depth= 30,\n",
        "    bootstrap= True,\n",
        "    class_weight={0: 0.2, 1: 0.8}\n",
        ")\n",
        "\n",
        "rf.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rbN1fwW4DeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "PLOT RANDOM FOREST TREE\n",
        "'''\n",
        "# Import tools needed for visualization\n",
        "from sklearn.tree import export_graphviz\n",
        "import pydot# Pull out one tree from the forest\n",
        "\n",
        "tree = rf.estimators_[6]# Export the image to a dot file\n",
        "export_graphviz(tree, out_file = 'tree.dot', feature_names = x_train.columns.to_list(),class_names=True, rounded = True,filled = True, precision = 1)# Use dot file to create a graph\n",
        "(graph, ) = pydot.graph_from_dot_file('tree.dot')# Write graph to a png file\n",
        "graph.write_png('tree.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxaX0-m34sY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zUJz36UqK5pr"
      },
      "source": [
        "# Boosting model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njZAgt1Yr1T0",
        "colab_type": "text"
      },
      "source": [
        "10.  What is Boosting ? How does it work?\n",
        "To convert weak learner to strong learner, we’ll combine the prediction of each weak learner using methods like:\n",
        "- Using average/ weighted average\n",
        "- Considering prediction has higher vote\n",
        "\n",
        "For example:  Above, we have defined 5 weak learners. Out of these 5, 3 are voted as ‘SPAM’ and 2 are voted as ‘Not a SPAM’. In this case, by default, we’ll consider an email as SPAM because we have higher(3) vote for ‘SPAM’.\n",
        "\n",
        "Source\n",
        "- [StatQuest - Explain](https://www.youtube.com/watch?v=3CC4N4z3GJc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHcEd_nXr2G0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxjoz6gBv8-D",
        "colab_type": "text"
      },
      "source": [
        "### Ada Boost\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0jvsGPrwN7",
        "colab_type": "text"
      },
      "source": [
        "Mostly use in image processing and face detection problem. At every stage, it gives more weight to those points which are misclassified.\n",
        "- Ý tưởng:\n",
        "  - Adaboost là 1 phương pháp kết hợp, có thể xem là k phải treebased, vì depth = 1 (chỉ đơn giản giống như 1 Linear regreesion,or SVM) gọi là week-learner, tạo 1 phương trình phân cách các mục tiêu. Sau đó lặp lại vòng lặp, nhưng ở các vòng sau thì **các điểm bị sai được đặt trọng số cao hơn**, cuối cùng ensemble các week-learner lại để tạo ra 1 strong-learner phân tách tốt hơn.\n",
        "\n",
        "\n",
        "You can tune the parameters to optimize the performance of algorithms, I’ve mentioned below the key parameters for tuning:\n",
        "- **n_estimators**: It controls the number of weak learners.\n",
        "- **learning_rate**: Controls the contribution of weak learners in the final combination. There is a trade-off between learning_rate and n_estimators.\n",
        "- **base_estimators**: It helps to specify different ML algorithms.\n",
        "\n",
        "Source\n",
        "- [Ada Boost - oxh](https://ongxuanhong.wordpress.com/2015/09/22/adaboost-hoi-gi-dap-nay/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mBr8fl6NIni",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Boosting Model (GBM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoqqc_OUrrYG",
        "colab_type": "text"
      },
      "source": [
        "Gradient Boosting is similar to AdaBoost in that they both use an ensemble of decision trees to predict a target label. However, unlike AdaBoost, the Gradient Boost trees have a depth larger than 1.\n",
        "\n",
        "11.  Which is more powerful: GBM or Xgboost?\n",
        ">  \n",
        "  1. Regularization:\n",
        "    - Standard GBM implementation has no regularization like XGBoost, therefore it also helps to reduce overfitting.\n",
        "    - In fact, XGBoost is also known as ‘regularized boosting‘ technique.\n",
        "  2. Parallel Processing:\n",
        "    - XGBoost implements parallel processing and is blazingly faster as compared to GBM.\n",
        "    - But hang on, we know that boosting is sequential process so how can it be parallelized? We know that each tree can be built only after the previous one, so what stops us from making a tree using all cores? I hope you get where I’m coming from. Check this link out to explore further.\n",
        "    - XGBoost also supports implementation on Hadoop.\n",
        "  3. High Flexibility\n",
        "    - XGBoost allow users to define custom optimization objectives and evaluation criteria.\n",
        "    - This adds a whole new dimension to the model and there is no limit to what we can do.\n",
        "  4. Handling Missing Values\n",
        "    - XGBoost has an in-built routine to handle missing values.\n",
        "    - User is required to supply a different value than other observations and pass that as a parameter. XGBoost tries different things as it encounters a missing value on each node and learns which path to take for missing values in future.\n",
        "  5. Tree Pruning:\n",
        "    - A GBM would stop splitting a node when it encounters a negative loss in the split. Thus it is more of a greedy algorithm.\n",
        "    - XGBoost on the other hand make splits upto the max_depth specified and then start pruning the tree backwards and remove splits beyond which there is no positive gain.\n",
        "    - Another advantage is that sometimes a split of negative loss say -2 may be followed by a split of positive loss +10. GBM would stop as it encounters -2. But XGBoost will go deeper and it will see a combined effect of +8 of the split and keep both.\n",
        "  6. Built-in Cross-Validation\n",
        "    - XGBoost allows user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n",
        "    - This is unlike GBM where we have to run a grid-search and only a limited values can be tested.\n",
        "  7. Continue on Existing Model\n",
        "    - User can start training an XGBoost model from its last iteration of previous run. This can be of significant advantage in certain specific applications.\n",
        "    - GBM implementation of sklearn also has this feature so they are even on this point.\n",
        "\n",
        "\n",
        "12.  Working with GBM in R and Python\n",
        "13.  Working with Xgboost in R and Python\n",
        "14.  Where to Practice ?\n",
        "\n",
        "\n",
        "**Sources**:\n",
        "- [GBM Tuning model](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n",
        "- [Làm thế nào để boosting tree hoạt động paralell](http://zhanpengfang.github.io/418home.html)\n",
        "- [Simple explain GBM](https://towardsdatascience.com/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4)\n",
        "- [How to explain gradient boosting](https://explained.ai/gradient-boosting/index.html)\n",
        "- [Gradient Boosting from scratch](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)\n",
        "- [Demystifying Maths of Gradient Boosting](https://towardsdatascience.com/demystifying-maths-of-gradient-boosting-bd5715e82b7c)\n",
        "- [Math behind GBM and XGBoost](https://medium.com/@vikeshsingh37/math-behind-gbm-and-xgboost-d00e8536b7de)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVuGEej0hDIk",
        "colab_type": "text"
      },
      "source": [
        "### XGB\n",
        "- https://www.one-tab.com/page/jlQjjQw7TAin0MmuXZkj8Q\n",
        "\n",
        "\n",
        "Sources\n",
        "- [explain 1](https://ongxuanhong.wordpress.com/2017/12/21/xgboost-thuat-toan-gianh-chien-thang-tai-nhieu-cuoc-thi-kaggle/)\n",
        "- [XGBoost Mathematics Explained](https://towardsdatascience.com/xgboost-mathematics-explained-58262530904a)\n",
        "- [Tune XGB](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)\n",
        "- [Compare XGB vs GBM](https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eADQQvTw4GDT",
        "colab_type": "text"
      },
      "source": [
        "More code in kaggle:\n",
        "- [M5 LOFO Importance on GPU via Rapids/Xgboost](https://www.kaggle.com/aerdem4/m5-lofo-importance-on-gpu-via-rapids-xgboost)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc7M3LX04WCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y50lpKa9-nLW",
        "colab_type": "text"
      },
      "source": [
        "### LightGB \n",
        "[**lightgbm.readthedocs**](https://lightgbm.readthedocs.io/en/latest/Parameters.html?highlight=bagging_seed#learning-control-parameters)\n",
        "\n",
        "Sources:\n",
        "- [Tune model Example-1](https://www.avanwyk.com/an-overview-of-lightgbm/#lightgbmparameters)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiJ1TwopfGw6",
        "colab_type": "text"
      },
      "source": [
        "**Notes:**\n",
        "- n_estimators = num_boost_round, [link](https://stackoverflow.com/questions/46943674/how-to-get-predictions-with-xgboost-and-xgboost-using-scikit-learn-wrapper-to-ma)\n",
        "- [speed up lighgbm training](https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/148273)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aOIb3MYm3aT",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**More training from Kaggle**:\n",
        "- [lightgbm-timeries split CV](https://hub.packtpub.com/cross-validation-strategies-for-time-series-forecasting-tutorial/)\n",
        "- [lightgbm-hyperopt](https://www.kaggle.com/vladgriguta/lightgbm-hyperopt/data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5mM7LYYm2jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvL0DkpX7UcS",
        "colab_type": "text"
      },
      "source": [
        "### CatBoost\n",
        "- [explain Catboost](https://catboost.ai/docs/search/?query=max_ctr_complexity) \n",
        "- [understand Catboost](https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2)\n",
        "- [Introduction to gradient boosting on decision trees with Catboost](https://towardsdatascience.com/introduction-to-gradient-boosting-on-decision-trees-with-catboost-d511a9ccbd14)\n",
        "\n",
        "**Comparation**\n",
        "- [gradient-boosting-decision-trees-xgboost-vs-lightgbm-and-catboost](https://medium.com/kaggle-nyc/gradient-boosting-decision-trees-xgboost-vs-lightgbm-and-catboost-72df6979e0bb)\n",
        "- [Compare catboost-vs-light-gbm-vs-xgboost](https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db)\n",
        "\n",
        "**Sources**:\n",
        "- [Catboost Tunning Parameter](https://towardsdatascience.com/https-medium-com-talperetz24-mastering-the-new-generation-of-gradient-boosting-db04062a7ea2)\n",
        "- [Ques 1](https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-tree-based-models/), [Ques 2](https://www.analyticsvidhya.com/blog/2016/12/detailed-solutions-for-skilltest-tree-based-algorithms/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4tYKVAV4XSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58emi3haTbms",
        "colab_type": "text"
      },
      "source": [
        "# 5. Sample Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2lAzxrgg_xL",
        "colab_type": "text"
      },
      "source": [
        "### Model tổng hợp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HV0gNKWf6ao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df_train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID','Date'], axis=1)\n",
        "y = df_train.sort_values('TransactionDT')['isFraud']\n",
        "#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n",
        "X_test = df_test.drop(['TransactionDT', 'TransactionID','Date'], axis=1)\n",
        "#del train\n",
        "test = df_test[[\"TransactionDT\", 'TransactionID']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVwqihfhgOu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# by https://www.kaggle.com/dimartinot\n",
        "def clean_inf_nan(df):\n",
        "    return df.replace([np.inf, -np.inf], np.nan)   \n",
        "\n",
        "# Cleaning infinite values to NaN\n",
        "X = clean_inf_nan(X)\n",
        "X_test = clean_inf_nan(X_test )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UUEfRc6gO24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_fold = 5\n",
        "folds = TimeSeriesSplit(n_splits=n_fold)\n",
        "folds = KFold(n_splits=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V3fmDT8vLR-3",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "import gc\n",
        "from numba import jit\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "from itertools import product\n",
        "\n",
        "import altair as alt\n",
        "from altair.vega import v5\n",
        "from IPython.display import HTML\n",
        "\n",
        "# using ideas from this kernel: https://www.kaggle.com/notslush/altair-visualization-2018-stackoverflow-survey\n",
        "def prepare_altair():\n",
        "    \"\"\"\n",
        "    Helper function to prepare altair for working.\n",
        "    \"\"\"\n",
        "\n",
        "    vega_url = 'https://cdn.jsdelivr.net/npm/vega@' + v5.SCHEMA_VERSION\n",
        "    vega_lib_url = 'https://cdn.jsdelivr.net/npm/vega-lib'\n",
        "    vega_lite_url = 'https://cdn.jsdelivr.net/npm/vega-lite@' + alt.SCHEMA_VERSION\n",
        "    vega_embed_url = 'https://cdn.jsdelivr.net/npm/vega-embed@3'\n",
        "    noext = \"?noext\"\n",
        "    \n",
        "    paths = {\n",
        "        'vega': vega_url + noext,\n",
        "        'vega-lib': vega_lib_url + noext,\n",
        "        'vega-lite': vega_lite_url + noext,\n",
        "        'vega-embed': vega_embed_url + noext\n",
        "    }\n",
        "    \n",
        "    workaround = f\"\"\"    requirejs.config({{\n",
        "        baseUrl: 'https://cdn.jsdelivr.net/npm/',\n",
        "        paths: {paths}\n",
        "    }});\n",
        "    \"\"\"\n",
        "    \n",
        "    return workaround\n",
        "    \n",
        "\n",
        "def add_autoincrement(render_func):\n",
        "    # Keep track of unique <div/> IDs\n",
        "    cache = {}\n",
        "    def wrapped(chart, id=\"vega-chart\", autoincrement=True):\n",
        "        if autoincrement:\n",
        "            if id in cache:\n",
        "                counter = 1 + cache[id]\n",
        "                cache[id] = counter\n",
        "            else:\n",
        "                cache[id] = 0\n",
        "            actual_id = id if cache[id] == 0 else id + '-' + str(cache[id])\n",
        "        else:\n",
        "            if id not in cache:\n",
        "                cache[id] = 0\n",
        "            actual_id = id\n",
        "        return render_func(chart, id=actual_id)\n",
        "    # Cache will stay outside and \n",
        "    return wrapped\n",
        "           \n",
        "\n",
        "@add_autoincrement\n",
        "def render(chart, id=\"vega-chart\"):\n",
        "    \"\"\"\n",
        "    Helper function to plot altair visualizations.\n",
        "    \"\"\"\n",
        "    chart_str = \"\"\"\n",
        "    <div id=\"{id}\"></div><script>\n",
        "    require([\"vega-embed\"], function(vg_embed) {{\n",
        "        const spec = {chart};     \n",
        "        vg_embed(\"#{id}\", spec, {{defaultStyle: true}}).catch(console.warn);\n",
        "        console.log(\"anything?\");\n",
        "    }});\n",
        "    console.log(\"really...anything?\");\n",
        "    </script>\n",
        "    \"\"\"\n",
        "    return HTML(\n",
        "        chart_str.format(\n",
        "            id=id,\n",
        "            chart=json.dumps(chart) if isinstance(chart, dict) else chart.to_json(indent=None)\n",
        "        )\n",
        "    )\n",
        "    \n",
        "\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "            else:\n",
        "                c_prec = df[col].apply(lambda x: np.finfo(x).precision).max()\n",
        "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max and c_prec == np.finfo(np.float32).precision:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "    \n",
        "\n",
        "@jit\n",
        "def fast_auc(y_true, y_prob):\n",
        "    \"\"\"\n",
        "    fast roc_auc computation: https://www.kaggle.com/c/microsoft-malware-prediction/discussion/76013\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_true = y_true[np.argsort(y_prob)]\n",
        "    nfalse = 0\n",
        "    auc = 0\n",
        "    n = len(y_true)\n",
        "    for i in range(n):\n",
        "        y_i = y_true[i]\n",
        "        nfalse += (1 - y_i)\n",
        "        auc += y_i * nfalse\n",
        "    auc /= (nfalse * (n - nfalse))\n",
        "    return auc\n",
        "\n",
        "\n",
        "def eval_auc(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Fast auc eval function for lgb.\n",
        "    \"\"\"\n",
        "    return 'auc', fast_auc(y_true, y_pred), True\n",
        "\n",
        "\n",
        "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
        "    \"\"\"\n",
        "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
        "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
        "    \"\"\"\n",
        "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
        "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
        "    \n",
        "\n",
        "def train_model_regression(X, X_test, y, params, folds=None, model_type='lgb', eval_metric='mae', columns=None, plot_feature_importance=False, model=None,\n",
        "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3):\n",
        "    \"\"\"\n",
        "    A function to train a variety of regression models.\n",
        "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
        "    \n",
        "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
        "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
        "    :params: y - target\n",
        "    :params: folds - folds to split data\n",
        "    :params: model_type - type of model to use\n",
        "    :params: eval_metric - metric to use\n",
        "    :params: columns - columns to use. If None - use all columns\n",
        "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
        "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
        "    \n",
        "    \"\"\"\n",
        "    columns = X.columns if columns is None else columns\n",
        "    X_test = X_test[columns]\n",
        "    splits = folds.split(X) if splits is None else splits\n",
        "    n_splits = folds.n_splits if splits is None else n_folds\n",
        "    \n",
        "    # to set up scoring parameters\n",
        "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
        "                        'catboost_metric_name': 'MAE',\n",
        "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
        "                    'group_mae': {'lgb_metric_name': 'mae',\n",
        "                        'catboost_metric_name': 'MAE',\n",
        "                        'scoring_function': group_mean_log_mae},\n",
        "                    'mse': {'lgb_metric_name': 'mse',\n",
        "                        'catboost_metric_name': 'MSE',\n",
        "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
        "                    }\n",
        "\n",
        "    \n",
        "    result_dict = {}\n",
        "    \n",
        "    # out-of-fold predictions on train data\n",
        "    oof = np.zeros(len(X))\n",
        "    \n",
        "    # averaged predictions on train data\n",
        "    prediction = np.zeros(len(X_test))\n",
        "    \n",
        "    # list of scores on folds\n",
        "    scores = []\n",
        "    feature_importance = pd.DataFrame()\n",
        "    \n",
        "    # split and train on folds\n",
        "    for fold_n, (train_index, valid_index) in enumerate(splits):\n",
        "        if verbose:\n",
        "            print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "        if type(X) == np.ndarray:\n",
        "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
        "            y_train, y_valid = y[train_index], y[valid_index]\n",
        "        else:\n",
        "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
        "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "            \n",
        "        if model_type == 'lgb':\n",
        "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
        "            model.fit(X_train, y_train, \n",
        "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
        "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
        "            \n",
        "            y_pred_valid = model.predict(X_valid)\n",
        "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
        "            \n",
        "        if model_type == 'xgb':\n",
        "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
        "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
        "\n",
        "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
        "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
        "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
        "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
        "        \n",
        "        if model_type == 'sklearn':\n",
        "            model = model\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
        "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
        "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
        "            print('')\n",
        "            \n",
        "            y_pred = model.predict(X_test).reshape(-1,)\n",
        "        \n",
        "        if model_type == 'cat':\n",
        "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
        "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
        "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
        "\n",
        "            y_pred_valid = model.predict(X_valid)\n",
        "            y_pred = model.predict(X_test)\n",
        "        \n",
        "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
        "        if eval_metric != 'group_mae':\n",
        "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
        "        else:\n",
        "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
        "\n",
        "        prediction += y_pred    \n",
        "        \n",
        "        if model_type == 'lgb' and plot_feature_importance:\n",
        "            # feature importance\n",
        "            fold_importance = pd.DataFrame()\n",
        "            fold_importance[\"feature\"] = columns\n",
        "            fold_importance[\"importance\"] = model.feature_importances_\n",
        "            fold_importance[\"fold\"] = fold_n + 1\n",
        "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
        "\n",
        "    prediction /= n_splits\n",
        "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
        "    \n",
        "    result_dict['oof'] = oof\n",
        "    result_dict['prediction'] = prediction\n",
        "    result_dict['scores'] = scores\n",
        "    \n",
        "    if model_type == 'lgb':\n",
        "        if plot_feature_importance:\n",
        "            feature_importance[\"importance\"] /= n_splits\n",
        "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
        "                by=\"importance\", ascending=False)[:50].index\n",
        "\n",
        "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
        "\n",
        "            plt.figure(figsize=(16, 12));\n",
        "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
        "            plt.title('LGB Features (avg over folds)');\n",
        "            \n",
        "            result_dict['feature_importance'] = feature_importance\n",
        "        \n",
        "    return result_dict\n",
        "    \n",
        "\n",
        "\n",
        "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
        "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000, splits=None, n_folds=3, averaging='usual', n_jobs=-1):\n",
        "    \"\"\"\n",
        "    A function to train a variety of classification models.\n",
        "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
        "    \n",
        "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
        "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
        "    :params: y - target\n",
        "    :params: folds - folds to split data\n",
        "    :params: model_type - type of model to use\n",
        "    :params: eval_metric - metric to use\n",
        "    :params: columns - columns to use. If None - use all columns\n",
        "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
        "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
        "    \n",
        "    \"\"\"\n",
        "    columns = X.columns if columns is None else columns\n",
        "    n_splits = folds.n_splits if splits is None else n_folds\n",
        "    X_test = X_test[columns]\n",
        "    \n",
        "    # to set up scoring parameters\n",
        "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
        "                        'catboost_metric_name': 'AUC',\n",
        "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
        "                    }\n",
        "    \n",
        "    result_dict = {}\n",
        "    if averaging == 'usual':\n",
        "        # out-of-fold predictions on train data\n",
        "        oof = np.zeros((len(X), 1))\n",
        "\n",
        "        # averaged predictions on train data\n",
        "        prediction = np.zeros((len(X_test), 1))\n",
        "        \n",
        "    elif averaging == 'rank':\n",
        "        # out-of-fold predictions on train data\n",
        "        oof = np.zeros((len(X), 1))\n",
        "\n",
        "        # averaged predictions on train data\n",
        "        prediction = np.zeros((len(X_test), 1))\n",
        "\n",
        "    \n",
        "    # list of scores on folds\n",
        "    scores = []\n",
        "    feature_importance = pd.DataFrame()\n",
        "    \n",
        "    # split and train on folds\n",
        "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
        "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
        "        if type(X) == np.ndarray:\n",
        "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
        "            y_train, y_valid = y[train_index], y[valid_index]\n",
        "        else:\n",
        "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
        "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
        "            \n",
        "        if model_type == 'lgb':\n",
        "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = n_jobs)\n",
        "            model.fit(X_train, y_train, \n",
        "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
        "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
        "            \n",
        "            y_pred_valid = model.predict_proba(X_valid)[:, 1]\n",
        "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)[:, 1]\n",
        "            \n",
        "        if model_type == 'xgb':\n",
        "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
        "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
        "\n",
        "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
        "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
        "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
        "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
        "        \n",
        "        if model_type == 'sklearn':\n",
        "            model = model\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
        "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
        "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
        "            print('')\n",
        "            \n",
        "            y_pred = model.predict_proba(X_test)\n",
        "        \n",
        "        if model_type == 'cat':\n",
        "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
        "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
        "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
        "\n",
        "            y_pred_valid = model.predict(X_valid)\n",
        "            y_pred = model.predict(X_test)\n",
        "        \n",
        "        if averaging == 'usual':\n",
        "            \n",
        "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
        "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
        "            \n",
        "            prediction += y_pred.reshape(-1, 1)\n",
        "\n",
        "        elif averaging == 'rank':\n",
        "                                  \n",
        "            oof[valid_index] = y_pred_valid.reshape(-1, 1)\n",
        "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
        "                                  \n",
        "            prediction += pd.Series(y_pred).rank().values.reshape(-1, 1)        \n",
        "        \n",
        "        if model_type == 'lgb' and plot_feature_importance:\n",
        "            # feature importance\n",
        "            fold_importance = pd.DataFrame()\n",
        "            fold_importance[\"feature\"] = columns\n",
        "            fold_importance[\"importance\"] = model.feature_importances_\n",
        "            fold_importance[\"fold\"] = fold_n + 1\n",
        "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
        "\n",
        "    prediction /= n_splits\n",
        "    \n",
        "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
        "    \n",
        "    result_dict['oof'] = oof\n",
        "    result_dict['prediction'] = prediction\n",
        "    result_dict['scores'] = scores\n",
        "    \n",
        "    if model_type == 'lgb':\n",
        "        if plot_feature_importance:\n",
        "            feature_importance[\"importance\"] /= n_splits\n",
        "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
        "                by=\"importance\", ascending=False)[:50].index\n",
        "\n",
        "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
        "\n",
        "            plt.figure(figsize=(16, 12));\n",
        "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
        "            plt.title('LGB Features (avg over folds)');\n",
        "            \n",
        "            result_dict['feature_importance'] = feature_importance\n",
        "            result_dict['top_columns'] = cols\n",
        "        \n",
        "    return result_dict\n",
        "\n",
        "# setting up altair\n",
        "workaround = prepare_altair()\n",
        "HTML(\"\".join((\n",
        "    \"<script>\",\n",
        "    workaround,\n",
        "    \"</script>\",\n",
        ")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOqhISb3gY5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "params = {'num_leaves': 256,\n",
        "          'min_child_samples': 79,\n",
        "          'objective': 'binary',\n",
        "          'max_depth': 13,\n",
        "          'learning_rate': 0.03,\n",
        "          \"boosting_type\": \"gbdt\",\n",
        "          \"subsample_freq\": 3,\n",
        "          \"subsample\": 0.9,\n",
        "          \"bagging_seed\": 11,\n",
        "          \"metric\": 'auc',\n",
        "          \"verbosity\": -1,\n",
        "          'reg_alpha': 0.3,\n",
        "          'reg_lambda': 0.3,\n",
        "          'colsample_bytree': 0.9,\n",
        "          #'categorical_feature': cat_cols\n",
        "         }\n",
        "result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, \n",
        "                                             model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n",
        "                                             verbose=500, early_stopping_rounds=200, n_estimators=1000, \n",
        "                                             averaging='usual', n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQkfbPoRgZEy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub['isFraud'] = result_dict_lgb['prediction']\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "\n",
        "\n",
        "sub.head()\n",
        "\n",
        "# pd.DataFrame(result_dict_lgb['oof']).to_csv('lgb_oof.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln9RSYLwM4ZE",
        "colab_type": "text"
      },
      "source": [
        "# Tunning Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7kx6hOusBz1",
        "colab_type": "text"
      },
      "source": [
        "- [Hyperparameter optimization in Python](https://towardsdatascience.com/hyperparameter-optimization-in-python-part-0-introduction-c4b66791614b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxqhoXbjNrX7",
        "colab_type": "text"
      },
      "source": [
        "## Bayesian Optimization\n",
        "https://www.kaggle.com/danil328/ligthgbm-with-bayesian-optimization\n",
        "\n",
        "https://www.kaggle.com/clair14/tutorial-bayesian-optimization\n",
        "\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "http://philipperemy.github.io/visualization/\n",
        "\n",
        "https://github.com/fmfn/BayesianOptimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0msNr1tVCgBV",
        "colab_type": "text"
      },
      "source": [
        "## RandomSearchCV\n",
        "- [source1](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n",
        "- [scoring_metric](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter), [custom metric](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DN4lGn3NuDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap,\n",
        "               'class_weight': ['balanced',{1:0.3,0:0.7},{1:0.4,0:0.6}]                  \n",
        "              }\n",
        "random_grid\n",
        "\n",
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestClassifier(criterion='entropy')\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(x_train, y_train)\n",
        "\n",
        "print(rf_random.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRX3pKGXjZqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEZFRbUXpUY5",
        "colab_type": "text"
      },
      "source": [
        "# Cross Validation/ Ensemble\n",
        "- [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/?fbclid=IwAR1xJfInYadNmthmkOQEkiJlTXHk2v0PYZqDtYYNOUEVRnFEijXvHInOm84)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNjcOiJ2J3SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "ENSEMBLE WRITE DOWN FOR RF\n",
        "'''\n",
        "\n",
        "class Create_ensemble(object):\n",
        "    def __init__(self, n_splits, base_models):\n",
        "        self.n_splits = n_splits\n",
        "        self.base_models = base_models\n",
        "\n",
        "    def predict(self, X, y, T):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        T = np.array(T)\n",
        "        no_class = len(np.unique(y))\n",
        "\n",
        "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \n",
        "                                     random_state = random_state).split(X, y))\n",
        "\n",
        "        train_proba = np.zeros((X.shape[0], no_class))\n",
        "        test_proba = np.zeros((T.shape[0], no_class))\n",
        "        \n",
        "        train_pred = np.zeros((X.shape[0], len(self.base_models)))\n",
        "        test_pred = np.zeros((T.shape[0], len(self.base_models)* self.n_splits))\n",
        "        f1_scores = np.zeros((len(self.base_models), self.n_splits))\n",
        "        recall_scores = np.zeros((len(self.base_models), self.n_splits))\n",
        "        \n",
        "        test_col = 0\n",
        "        for i, clf in enumerate(self.base_models):\n",
        "            \n",
        "            for j, (train_idx, valid_idx) in enumerate(folds):\n",
        "                \n",
        "                X_train = X[train_idx]\n",
        "                Y_train = y[train_idx]\n",
        "                X_valid = X[valid_idx]\n",
        "                Y_valid = y[valid_idx]\n",
        "                \n",
        "                clf.fit(X_train, Y_train)\n",
        "                \n",
        "                valid_pred = clf.predict(X_valid)\n",
        "                recall  = recall_score(Y_valid, valid_pred, average='macro')\n",
        "                f1 = f1_score(Y_valid, valid_pred, average='macro')\n",
        "                \n",
        "                recall_scores[i][j] = recall\n",
        "                f1_scores[i][j] = f1\n",
        "                \n",
        "                train_pred[valid_idx, i] = valid_pred\n",
        "                test_pred[:, test_col] = clf.predict(T)\n",
        "                test_col += 1\n",
        "                \n",
        "                ## Probabilities\n",
        "                valid_proba = clf.predict_proba(X_valid)\n",
        "                train_proba[valid_idx, :] = valid_proba\n",
        "                test_proba  += clf.predict_proba(T)\n",
        "                \n",
        "                print( \"Model- {} and CV- {} recall: {}, f1_score: {}\".format(i, j, recall, f1))\n",
        "                \n",
        "            test_proba /= self.n_splits\n",
        "            \n",
        "        return train_proba, test_proba, train_pred, test_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBDAKf7FSVwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flBfYc0pSVoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj7os5IuSVdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}