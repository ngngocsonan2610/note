{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ngngocsonan2610/note/blob/master/1.PreProcessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "toc",
    "id": "3188bu2pVCw2"
   },
   "source": [
    ">[Preprocessing](#scrollTo=-2COeSc5ml1_)\n",
    "\n",
    ">[Transformation Scaling/Standardize](#scrollTo=PcX1fpPrrg-_)\n",
    "\n",
    ">[Encoding](#scrollTo=qeLaVzFW0Se6)\n",
    "\n",
    ">>>[Onehot Encode](#scrollTo=HULldo6Zm6l1)\n",
    "\n",
    ">>>[Label Encode](#scrollTo=Hz-vTHED0Vbc)\n",
    "\n",
    ">>>[Reduce memory](#scrollTo=WzFvPys_F9KB)\n",
    "\n",
    ">>>[Reduce dimensional checking](#scrollTo=a_a8z7m7uec_)\n",
    "\n",
    ">>>>[PCA](#scrollTo=TQSNaBpvtFjT)\n",
    "\n",
    ">>>>[T-sne](#scrollTo=kUzEGvostxdh)\n",
    "\n",
    ">[Features Engineer](#scrollTo=ffkEEAsbC4EW)\n",
    "\n",
    ">[Feature Selection](#scrollTo=LfdUhklbDAQa)\n",
    "\n",
    ">>>[High correlation](#scrollTo=k9prSardKWXm)\n",
    "\n",
    ">>>[WOE & IV](#scrollTo=t-8cO9PJ3ryk)\n",
    "\n",
    ">>>[RFE](#scrollTo=cmnU0NRff_ck)\n",
    "\n",
    ">[Data Type Handling](#scrollTo=JfMtLkkO7fxr)\n",
    "\n",
    ">>>[Under-sampling](#scrollTo=5mzGc0BE7xWq)\n",
    "\n",
    ">>>[Over-sampling](#scrollTo=4il7y5CI8O8D)\n",
    "\n",
    ">>>>[RandomOverSampler](#scrollTo=2ioqMRx98Syz)\n",
    "\n",
    ">>>>[SMOTE](#scrollTo=8A8WLw5B8dnc)\n",
    "\n",
    ">[Validation](#scrollTo=cJ7_EPQZIm0I)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PcX1fpPrrg-_",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Transformation Scaling/Standardize\n",
    "News:\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "\n",
    "Căn bản:\n",
    "- [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)\n",
    "![alt text](https://miro.medium.com/max/4800/1*z-C9ANBC4rjsk-ZK4wzijg.png)\n",
    "- [Why, How and When to Scale your Features](https://medium.com/greyatom/why-how-and-when-to-scale-your-features-4b30ab09db5e)\n",
    "\n",
    "\n",
    "- Đối với từng model nên xét thêm\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bn4UO8yW5hMS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWrridnMkIwF"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "nottrans_num_col = ['AGSegCode', 'MAPASegCode',\n",
    "                    'Leader_AGSegCode', 'Leader_MAPASegCode']\n",
    "                    \n",
    "scale_col = [c for c in WOE_numeric_col if c not in nottrans_num_col]\n",
    "print('scale cols' ,scale_col)\n",
    "robust_scale = RobustScaler().fit(round(df_WOE[scale_col],2))\n",
    "df_WOE[scale_col] = robust_scale.transform(df_WOE[scale_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qeLaVzFW0Se6"
   },
   "source": [
    "# Encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HULldo6Zm6l1"
   },
   "source": [
    "## Onehot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0cfS1ArWkI61"
   },
   "outputs": [],
   "source": [
    "onehot_col = ['a', 'b', 'c']\n",
    "\n",
    "def onehot_encode(df,onehot_col):\n",
    "    dummies_col=list()\n",
    "    for c in onehot_col:\n",
    "        print('> Get dummies with prefix {}'.format(c+'_'))\n",
    "        df_dummies = pd.get_dummies(df[c], prefix = c ) \n",
    "        dummies_col = dummies_col +(df_dummies.columns.to_list())\n",
    "        df = pd.concat( [df, df_dummies] , axis = 1)\n",
    "    print('dummies col: ',dummies_col)\n",
    "    return df,dummies_col\n",
    "\n",
    "\n",
    "temp,new_onehot_col = onehot_encode(temp,onehot_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71Mnyx61FiB4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hz-vTHED0Vbc"
   },
   "source": [
    "## Label Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DOaNKw9Q0WS7"
   },
   "outputs": [],
   "source": [
    "df = df.replace({\n",
    "             'Leader_AGLevel' : {'PM':1,'UM':2,'BM':3}\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IB3mZzSk0Waf"
   },
   "outputs": [],
   "source": [
    "numeric_col = ['Leader_RegionCode', 'RegionCode', 'ContactProvince_Code','EducationCode']\n",
    "for col in numeric_col:\n",
    "    print('> ',col)\n",
    "    df[col] = df[col].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hS4OLaR35bbk"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "BINs of numeric features\n",
    "'''\n",
    "# AVERAGE_INCOME\n",
    "df_model['AVERAGE_INCOME_bin_LE'] = pd.qcut(df_model.AVERAGE_INCOME, q=7, precision=0).astype('category').cat.codes\n",
    "bin_labels = []\n",
    "for string in sorted(pd.qcut(round(df_model.AVERAGE_INCOME/10**6).astype('int'), q=7, precision=0).unique()):\n",
    "    x = str(string).replace(r'(', '').replace(r']', '').replace(r', ', '-').replace(r'\\.0', '')\n",
    "    x = x +'m'\n",
    "    bin_labels.append(x)\n",
    "df_model['AVERAGE_INCOME_bin'] = pd.qcut(df_model.AVERAGE_INCOME, q=7, precision=0,labels=bin_labels).astype('str')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature_engine.categorical_encoders \n",
    "CountFrequencyCategoricalEncoder\n",
    "- https://feature-engine.readthedocs.io/en/latest/encoders/CountFrequencyCategoricalEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    train_df[col] = train_df[col].astype(str)\n",
    "    test_df[col] = test_df[col].astype(str)\n",
    "    \n",
    "from feature_engine import categorical_encoders as ce\n",
    "\n",
    "# Label-Encoding \n",
    "lbl = LabelEncoder()\n",
    "for col in cat_cols:\n",
    "    assert train_df[col].dtypes == test_df[col].dtypes\n",
    "    \n",
    "    encoder = ce.CountFrequencyCategoricalEncoder(encoding_method='frequency')\n",
    "    \n",
    "    train_df[col] = train_df[col].fillna('None')\n",
    "    test_df[col] = test_df[col].fillna('None')\n",
    "    # Only take the common values in Train/Test-set\n",
    "    common_vals = list(set(train_df[col]).intersection(set(test_df[col])))\n",
    "    \n",
    "    # Take if vals appeared both 5 times\n",
    "    common_vals = set(train_df[col].value_counts()[train_df[col].value_counts()> 20].index)\\\n",
    "                        .intersection(test_df[col].value_counts()[test_df[col].value_counts()>20].index)\n",
    "    \n",
    "    # Replace not-common values with \"Missing\" or np.NaN      \n",
    "    train_df.loc[~train_df[col].isin(common_vals), col] = 'Missing'\n",
    "    test_df.loc[~test_df[col].isin(common_vals), col] = 'Missing'\n",
    "\n",
    "    # Implement LE\n",
    "    lbl.fit(train_df[col].tolist() + test_df[col].tolist())\n",
    "    train_df[col] = lbl.transform(train_df[col])\n",
    "    test_df[col] = lbl.transform(test_df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## đổi tên\n",
    "fe_cols = cat_cols\n",
    "for col in fe_cols:\n",
    "    temp_df = pd.concat([train_df[col], test_df[col]], axis=0)\n",
    "    d_fe = temp_df.value_counts().to_dict()\n",
    "    train_df[col+'_fe'] = train_df[col].apply(lambda x: d_fe.get(x))\n",
    "    test_df[col+'_fe'] = test_df[col].apply(lambda x: d_fe.get(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3no0SyveFy5c"
   },
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "t = pd.concat([train, test]).reset_index(drop=True)\n",
    "count_enc = ce.CountEncoder().fit_transform(t[cat_features])\n",
    "tt = t.join(count_enc.add_suffix(\"_count\"))\n",
    "\n",
    "# phân train-test\n",
    "f2_train = tt.loc[tt.index < train.shape[0]]\n",
    "f2_test = tt.loc[tt.index >= train.shape[0]]\n",
    "\n",
    "columns = sorted(set(f2_train.columns).intersection(f2_test.columns))\n",
    "print(len(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzFvPys_F9KB"
   },
   "source": [
    "## Reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jUyadfjFF_kb"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgZr6swygYbp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvTRFClqgYjk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_a8z7m7uec_",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Reduce dimensional checking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQSNaBpvtFjT"
   },
   "source": [
    "#### PCA\n",
    "- [PCA in python & spark](https://dataplatform.cloud.ibm.com/analytics/notebooks/v2/02716668-5f99-40b1-b4a1-1a820b6e5c55/view?access_token=50797d4e6ec323c16603c628b41e1e3f203221faa3e826a218724558ed57de26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qDc3N94Mj8K-"
   },
   "outputs": [],
   "source": [
    "#scaling only on tranform data\n",
    "df_V = df_V[c_V_trans] \n",
    "df_V_test = df_V_test[c_V_trans]\n",
    "\n",
    "#standardize scaled\n",
    "scaler = StandardScaler().fit(df_V)\n",
    "df_V_scaled = scaler.transform(df_V)\n",
    "print('Scaled mean df_V',df_V_scaled[:,0].mean())  # zero (or very close)\n",
    "print('Scaled std df_V',df_V_scaled[:,0].std()) \n",
    "scaler = StandardScaler().fit(df_V_test)\n",
    "df_V_test_scaled = scaler.transform(df_V_test)\n",
    "print('Scaled mean df_V',df_V_test_scaled[:,0].mean())  # zero (or very close)\n",
    "print('Scaled std df_V',df_V_test_scaled[:,0].std()) \n",
    "\n",
    "# plot cumulative explained variance\n",
    "# pca = PCA().fit(df_V_scaled)\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# #plt.xlim(0,7,1)\n",
    "# plt.xlabel('Number of components')\n",
    "# plt.ylabel('Cumulative explained variance')\n",
    "\n",
    "\n",
    "# Setup Principal component analysis\n",
    "pca = PCA(n_components=125) \n",
    "#pca = PCA(n_components=0.96)  #v2: n_components=0.95\n",
    "df_V_pca = pca.fit_transform(df_V_scaled)\n",
    "np.save('df_V_pca_v3.npy',df_V_pca)\n",
    "#df_V_pca.to_csv('df_V_pca', sep='\\t')\n",
    "df_V_test_pca = pca.fit_transform(df_V_test_scaled)\n",
    "np.save('df_V_test_pca_v3.npy',df_V_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ljSftUKgvtyG"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import io\n",
    "import plotly.offline as py#visualization\n",
    "py.init_notebook_mode(connected=True)#visualization\n",
    "import plotly.graph_objs as go#visualization\n",
    "import plotly.tools as tls#visualization\n",
    "import plotly.figure_factory as ff#visualization\n",
    "\n",
    "'''\n",
    "Scatter plot giữa 3 cột bất kỳ trong dữ liệu, được hue = TARGET\n",
    "cho cái nhìn về dữ liệu, xem liệu nó có khả năng phân tách không\n",
    "'''\n",
    "\n",
    "trace1 = go.Scatter3d(x = churn[\"MEMBER_ANNUAL_INCOME\"],\n",
    "                      y = churn[\"ANNUAL_FEES\"],\n",
    "                      z = churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
    "                      mode = \"markers\",\n",
    "                      name = \"Churn customers\",\n",
    "                      text = \"Id : \" + churn[\"MEMBERSHIP_NUMBER\"],\n",
    "                      marker = dict(size = 1,color = \"red\")\n",
    "                     )\n",
    "trace2 = go.Scatter3d(x = not_churn[\"MEMBER_ANNUAL_INCOME\"],\n",
    "                      y = not_churn[\"ANNUAL_FEES\"],\n",
    "                      z = not_churn[\"MEMBERSHIP_TERM_YEARS\"],\n",
    "                      name = \"Non churn customers\",\n",
    "                      text = \"Id : \" + not_churn[\"MEMBERSHIP_NUMBER\"],\n",
    "                      mode = \"markers\",\n",
    "                      marker = dict(size = 1,color= \"green\")\n",
    "                     )\n",
    "\n",
    "layout = go.Layout(dict(title = \"Monthly charges,total charges & tenure in customer attrition\",\n",
    "                        scene = dict(camera = dict(up=dict(x= 0 , y=0, z=0),\n",
    "                                                   center=dict(x=0, y=0, z=0),\n",
    "                                                   eye=dict(x=1.25, y=1.25, z=1.25)),\n",
    "                                     xaxis  = dict(title = \"annual incomes\",\n",
    "                                                   gridcolor='rgb(255, 255, 255)',\n",
    "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
    "                                                   showbackground=True,\n",
    "                                                   backgroundcolor='rgb(230, 230,230)'),\n",
    "                                     yaxis  = dict(title = \"annual fees\",\n",
    "                                                   gridcolor='rgb(255, 255, 255)',\n",
    "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
    "                                                   showbackground=True,\n",
    "                                                   backgroundcolor='rgb(230, 230,230)'\n",
    "                                                  ),\n",
    "                                     zaxis  = dict(title = \"term years\",\n",
    "                                                   gridcolor='rgb(255, 255, 255)',\n",
    "                                                   zerolinecolor='rgb(255, 255, 255)',\n",
    "                                                   showbackground=True,\n",
    "                                                   backgroundcolor='rgb(230, 230,230)'\n",
    "                                                  )\n",
    "                                    ),\n",
    "                        height = 700,\n",
    "                       )\n",
    "                  )\n",
    "                  \n",
    "\n",
    "data = [trace1,trace2]\n",
    "fig  = go.Figure(data = data,layout = layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQI7G83cDx2l"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "X = tmp[[i for i in tmp.columns if i not in Id_col + target_col]]\n",
    "Y = tmp[target_col + Id_col]\n",
    "\n",
    "principal_components = pca.fit_transform(X)\n",
    "pca_data = pd.DataFrame(principal_components,columns = [\"PC1\",\"PC2\"])\n",
    "pca_data = pca_data.merge(Y,left_index=True,right_index=True,how=\"left\")\n",
    "pca_data[\"CHURN\"] = pca_data[\"CHURN\"].replace({1:\"CANCELLED\",0:\"INFORCE\"})\n",
    "\n",
    "def pca_scatter(target,color) :\n",
    "    tracer = go.Scatter(x = pca_data[pca_data[\"CHURN\"] == target][\"PC1\"] ,\n",
    "                        y = pca_data[pca_data[\"CHURN\"] == target][\"PC2\"],\n",
    "                        name = target,mode = \"markers\",\n",
    "                        marker = dict(color = color,\n",
    "                                      line = dict(width = .5),\n",
    "                                      symbol =  \"diamond-open\"),\n",
    "                        text = (\"Customer Id : \" + \n",
    "                                pca_data[pca_data[\"CHURN\"] == target]['MEMBERSHIP_NUMBER'])\n",
    "                       )\n",
    "    return tracer\n",
    "\n",
    "layout = go.Layout(dict(title = \"Visualising data with principal components\",\n",
    "                        plot_bgcolor  = \"rgb(243,243,243)\",\n",
    "                        paper_bgcolor = \"rgb(243,243,243)\",\n",
    "                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
    "                                     title = \"principal component 1\",\n",
    "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
    "                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n",
    "                                     title = \"principal component 2\",\n",
    "                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n",
    "                        height = 600\n",
    "                       )\n",
    "                  )\n",
    "trace1 = pca_scatter(\"CANCELLED\",'red')\n",
    "trace2 = pca_scatter(\"INFORCE\",'royalblue')\n",
    "data = [trace2,trace1]\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WmZwNeiLgSMP"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "class PCA:\n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X = X - self.mean\n",
    "        cov = np.cov(X.T)\n",
    "\n",
    "        evalue, evector = np.linalg.eig(cov)\n",
    "\n",
    "        eigenvectors = evector.T\n",
    "        idxs = np.argsort(evalue)[::-1]\n",
    "\n",
    "        evalue = evalue[idxs]\n",
    "        evector = evector[idxs]\n",
    "        self.components = evector[0:self.n_components]\n",
    "\n",
    "    def transform(self, X):\n",
    "        #project data\n",
    "        X = X - self.mean\n",
    "        return(np.dot(X, self.components.T))\n",
    "\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq2uGZRytViB"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler        \n",
    "scale_col = ['A', 'B', 'C']\n",
    "print('scale cols: ' ,scale_col)\n",
    "robust_scale = RobustScaler().fit(round(data[scale_col],2))\n",
    "data[scale_col] = robust_scale.transform(data[scale_col])\n",
    "\n",
    "X = data[col]\n",
    "y = data['label']\n",
    "\n",
    "pca = PCA(2)\n",
    "pca.fit(X)\n",
    "X_projected = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1_23svdSthHY"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "2d plot\n",
    "'''\n",
    "x1 = X_projected[:,0]\n",
    "x2 = X_projected[:,1]\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(x1,x2,c=y,edgecolor='none',alpha=0.8,cmap=plt.cm.get_cmap('viridis',2),s=6)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar()\n",
    "plt.margins(0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_vbSYW6ttn1Q"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "3d plot\n",
    "'''\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "X = data[col]\n",
    "y = data['label']\n",
    "\n",
    "pca = PCA(3)\n",
    "pca.fit(X)\n",
    "X_projected = pca.transform(X)\n",
    "\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=X_projected[:,0], \n",
    "    ys=X_projected[:,1], \n",
    "    zs=X_projected[:,2], \n",
    "    c=y, \n",
    "    cmap=plt.cm.get_cmap('viridis',2)#'tab10'\n",
    "    ,s=6\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-two')\n",
    "ax.set_zlabel('pca-three')\n",
    "ax.margins(0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUzEGvostxdh"
   },
   "source": [
    "#### T-sne\n",
    "- [VisualizingDatausingt-SNE](http://jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLEMgmEptNnR"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guRKlf86tN95"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler        \n",
    "scale_col = ['A', 'B', 'C']\n",
    "print('scale cols: ' ,scale_col)\n",
    "robust_scale = RobustScaler().fit(round(data[scale_col],2))\n",
    "data[scale_col] = robust_scale.transform(data[scale_col])\n",
    "\n",
    "X = data[col]\n",
    "y = data['label']\n",
    "\n",
    "'''\n",
    "reduce to 2d & plot\n",
    "'''\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(X)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "data['tsne-2d-one'] = tsne_results[:,0]\n",
    "data['tsne-2d-two'] = tsne_results[:,1]\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "\n",
    "sns.scatterplot(\n",
    "    x=tsne_results[:,0], y= tsne_results[:,1],\n",
    "    hue=data['label'],\n",
    "    palette=sns.color_palette(\"hls\", 2),\n",
    "    #data=data,\n",
    "    legend=\"full\",\n",
    "    alpha=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MG-wFG8t_1R"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "reduce to 3d & plot\n",
    "'''\n",
    "X = data[col]\n",
    "y = data['label']\n",
    "\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_results = tsne.fit_transform(X)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "ax = plt.figure(figsize=(16,10)).gca(projection='3d')\n",
    "ax.scatter(\n",
    "    xs=tsne_results[:,0], \n",
    "    ys=tsne_results[:,1], \n",
    "    zs=tsne_results[:,2], \n",
    "    c=data['label'], \n",
    "    cmap=plt.cm.get_cmap('viridis',2)#'tab10'\n",
    ")\n",
    "ax.set_xlabel('pca-one')\n",
    "ax.set_ylabel('pca-twp')\n",
    "ax.set_zlabel('pca-three')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBu4s8JGuA4I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ffkEEAsbC4EW",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Features Engineer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2f1ea0EPez4"
   },
   "source": [
    "[Simple FE](https://machinelearningcoban.com/general/2017/02/06/featureengineering/)\n",
    "-    Trực tiếp lấy raw data\n",
    "-    Bag-of-words\n",
    "  -    Bag-of-Words trong Computer Vision\n",
    "-    Feature Scaling and Normalization\n",
    "  -        Rescaling\n",
    "  -        Standardization\n",
    "  -        Scaling to unit length\n",
    "\n",
    "  - Feature Generation: ở bước này tập trung các kĩ thuật để tạo ra feature: xử lí nlp, image, binning, scaling, grouping, aggregate.\n",
    "  - Kết hợp với Modelling để tìm ra nhóm feature tốt và tập trung mạnh vào đó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "exdPs4WjOIXu"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "một số FE cho prices và các biến numeric quan trọng\n",
    "PRICES\n",
    "'''\n",
    "\n",
    "print('Prices')\n",
    "\n",
    "# We can do some basic aggregations\n",
    "prices_df['price_max'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "prices_df['price_min'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "prices_df['price_std'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "prices_df['price_mean'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "\n",
    "# and do price normalization (min/max scaling)\n",
    "prices_df['price_norm'] = prices_df['sell_price']/prices_df['price_max']\n",
    "\n",
    "# Some items are can be inflation dependent\n",
    "# and some items are very \"stable\"\n",
    "prices_df['price_nunique'] = prices_df.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "prices_df['item_nunique'] = prices_df.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "# I would like some \"rolling\" aggregations\n",
    "# but would like months and years as \"window\"\n",
    "calendar_prices = calendar_df[['wm_yr_wk','month','year']]\n",
    "calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "prices_df = prices_df.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')\n",
    "del calendar_prices\n",
    "\n",
    "# Now we can add price \"momentum\" (some sort of)\n",
    "# Shifted by week # by month mean # by year mean\n",
    "prices_df['price_momentum'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))\n",
    "prices_df['price_momentum_7'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(7))\n",
    "prices_df['price_momentum_14'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(14))\n",
    "prices_df['price_momentum_m'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "prices_df['price_momentum_y'] = prices_df['sell_price']/prices_df.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "del prices_df['month'], prices_df['year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## data-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52WqVf6eOIO1"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "TÌM HOLIDAYS\n",
    "https://github.com/dr-prodigy/python-holidays\n",
    "'''\n",
    "calendar_prices = calendar_df[['wm_yr_wk','date','month','year']]\n",
    "#calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "## get holidays\n",
    "north_america = holidays.US() + holidays.MX() \n",
    "calendar_prices['holiday'] = calendar_prices.date.apply(lambda x : north_america.get(x))\n",
    "\n",
    "calendar_prices['nearest_holiday'] = pd.to_datetime(pd.Series(np.where(calendar_prices.holiday.isnull()==False, pd.to_numeric(pd.to_datetime(calendar_prices.date)), calendar_prices.holiday)).astype(float).interpolate(method ='nearest'))\n",
    "\n",
    "calendar_prices.iloc[:7,].nearest_holiday.fillna(calendar_prices.nearest_holiday[8], inplace=True)\n",
    "\n",
    "calendar_prices.iloc[1949:,].nearest_holiday.fillna(calendar_prices.nearest_holiday[1948], inplace=True)\n",
    "#calendar_prices['tw_holiday'] = pd.to_datetime(pd.Series(np.where(calendar_prices.holiday.isnull()==False, pd.to_numeric(pd.to_datetime(calendar_prices.date)), calendar_prices.holiday)).astype(float).interpolate(method ='pad', limit_direction ='forward', limit = 60))\n",
    "\n",
    "holiday = calendar_prices[~calendar_prices.holiday.fillna('none').str.contains(r'(Observed)|none')].holiday.unique().tolist()#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ahyBAMSPhKW"
   },
   "outputs": [],
   "source": [
    "date_cols = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 35, 40]]\n",
    "datetime_cols = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\n",
    "correct_dt_cols = ['Field_34', 'ngaySinh']\n",
    "cat_cols = date_cols + datetime_cols + correct_dt_cols\n",
    "\n",
    "# Normalize Field_34, ngaySinh\n",
    "def ngaysinh_34_normalize(s):\n",
    "    if s != s: return np.nan\n",
    "    try: s = int(s)\n",
    "    except ValueError: s = s.split(\" \")[0]\n",
    "    return datetime.strptime(str(s)[:6], \"%Y%m\")\n",
    "\n",
    "# Normalize datetime data\n",
    "def datetime_normalize(s):\n",
    "    if s != s: return np.nan\n",
    "    s = s.split(\".\")[0]\n",
    "    if s[-1] == \"Z\": s = s[:-1]\n",
    "    return datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n",
    "\n",
    "# Normalize date data\n",
    "def date_normalize(s):\n",
    "    if s != s: return np.nan\n",
    "    try: t = datetime.strptime(s, \"%m/%d/%Y\")\n",
    "    except: t = datetime.strptime(s, \"%Y-%m-%d\")\n",
    "    return t\n",
    "\n",
    "def process_datetime_cols(df):\n",
    "    df[datetime_cols] = df[datetime_cols].applymap(datetime_normalize)  \n",
    "    df[date_cols] = df[date_cols].applymap(date_normalize)\n",
    "    df[correct_dt_cols] = df[correct_dt_cols].applymap(ngaysinh_34_normalize)\n",
    "\n",
    "    # Some delta columns\n",
    "    for i, j in zip('43 1 2'.split(), '1 2 44'.split()): df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.seconds\n",
    "    for i, j in zip('5 6 7 33 8 11 9 15 25 6 7 8 9 15 25 2'.split(), '6 34 33 40 11 35 15 25 32 7 8 9 15 25 32 8'.split()): \n",
    "        df[f'DT_{j}_{i}'] = (df[f'Field_{j}'] - df[f'Field_{i}']).dt.days\n",
    "    \n",
    "    # Age, month\n",
    "    df['age'] = 2020 - pd.DatetimeIndex(df['ngaySinh']).year\n",
    "    df['birth_month'] = pd.DatetimeIndex(df['ngaySinh']).month\n",
    "    \n",
    "    # Days from current time & isWeekday\n",
    "    for col in cat_cols:\n",
    "        name = col.split('_')[-1]\n",
    "        df[f'is_WD_{name}'] = df[col].dt.dayofweek.isin(range(5))\n",
    "        df[f'days_from_now_{name}'] = (datetime.now() - pd.DatetimeIndex(df[col])).days\n",
    "        df[col] = df[col].dt.strftime('%m-%Y')\n",
    "    \n",
    "    # Delta for x_startDate and x_endDate\n",
    "    for cat in ['F', 'E', 'C', 'G', 'A']:\n",
    "        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n",
    "        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n",
    "        \n",
    "        df[f'{cat}_start_end'] = (df[f'{cat}_endDate'] - df[f'{cat}_startDate']).dt.days\n",
    "        \n",
    "    for i, j in zip('F E C G'.split(), 'E C G A'.split()):\n",
    "        df[f'{j}_{i}_startDate'] = (df[f'{j}_startDate'] - df[f'{i}_startDate']).dt.days\n",
    "        df[f'{j}_{i}_endDate'] = (df[f'{j}_endDate'] - df[f'{i}_endDate']).dt.days\n",
    "    \n",
    "    temp_date = [f'{i}_startDate' for i in 'ACEFG'] + [f'{i}_endDate' for i in 'ACEFG']\n",
    "    \n",
    "    for col in temp_date:\n",
    "        df[col] = df[col].dt.strftime('%m-%Y')\n",
    "        \n",
    "    for col in cat_cols + temp_date:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zErroEQIPhBO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfdUhklbDAQa"
   },
   "source": [
    "# Feature Selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JiEiWrFdiZp"
   },
   "source": [
    "Research:\n",
    "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)\n",
    "- [2](https://www.kaggle.com/sz8416/6-ways-for-feature-selection) example selectkbes-RFE in kaggle\n",
    "- [3](https://www.kaggle.com/dkim1992/feature-selection-ranking) chua doc\n",
    "- [4](https://machinelearningmastery.com/feature-selection-machine-learning-python/) tong quat\n",
    "- [sklearn.feature_selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
    "- [Cách tìm feature bằng cách chạy các linear reg](https://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/) chua doc\n",
    "- [6. Blog tổng hợp các method](https://mlwhiz.com/blog/2019/08/07/feature_selection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fb6TIKC408SK"
   },
   "source": [
    "- Feature Selection: ở bước này tập trung các kĩ thuật chọn feature để tốt ưu mô hình\n",
    "  - Chọn được feature tốt và tối ưu được bộ nhớ\n",
    "  - Chọn được model parameters tốt nhất phù hợp với bộ features tốt nhất\n",
    "  - Kết hợp với các kĩ thuật chia dataset (CV, leave-one)\n",
    "\n",
    "- Tiêu chí đánh giá **metric of feature**\n",
    "  - AUC: thể hiện khả năng dự đoán\n",
    "  - Correlation: kiểm tra độ tương quan giữa feature với target hoặc với các feature quan trọng khác\n",
    "  - Converage: kiểm tra null, null nhiều thì ít thông tin\n",
    "  - Weighted of evidence, and information value:\n",
    "    - $\\ln \\left(\\frac{P(\\text {Good})}{P(\\text {Bad})}\\right)$\n",
    "    - \\begin{array}{l}{\\text { Information value }} \\\\ {\\qquad \\sum(P(G o o d)-P(B a d)) * \\ln \\left(\\frac{P(G o d)}{P(B a d)}\\right)}\\end{array}\n",
    "\n",
    "\n",
    "\n",
    "[Brute Force Approach](https://www.kdnuggets.com/2017/11/rapidminer-basic-concepts-feature-selection.html)\n",
    "- Cách tiếp cận bằng các chạy thử thất cả các feature combination và so sánh trên các metrics of feature\n",
    "\n",
    "[Feature Selection]\n",
    "- Forwarding: ta bắt đầu với tập feature rỗng, sau đó lần lượt add thêm feature vào tập này. Nếu thấy performance của model tăng ta sẽ tiếp tục quá trình này, ngược lại sẽ dừng lại.\n",
    "- Backwarding: ta bắt đầu với toàn bộ tập feature, sau đó lần lượt remove từng feature khỏi tập này. Nếu thấy performance của model tăng hoặc giảm không quá nhiều, ta sẽ tiếp tục quá trình này, ngược lại nếu performance bị drop quá mạnh sẽ dừng lại.\n",
    "- Hybridge: kết hợp cả 2 hướng trên\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FS with Null Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection process using target permutation tests actual importance significance against the distribution of feature importances when fitted to noise (shuffled target).\n",
    "\n",
    "The notebook implements the following steps :\n",
    "- Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target.\n",
    "- Fit the model on the original target and gather the feature importances. This gives us a benchmark whose significance can be tested against the Null Importances Distribution\n",
    "- For each feature test the actual importance:\n",
    "    - Compute the probabability of the actual importance wrt the null distribution. I will use a very simple estimation using occurences while the article proposes to fit known distribution to the gathered data. In fact here I'll compute 1 - the proba so that things are in the right order.\n",
    "    - Simply compare the actual importance to the mean and max of the null importances. This will give sort of a feature importance that allows to see major features in the dataset. Indeed the previous method may give us lots of ones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/sonannguyenngoc/tech-feature-selection-with-null-importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k9prSardKWXm"
   },
   "source": [
    "### High correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IAcrST9rMts"
   },
   "source": [
    "**High correlation features**\n",
    "Có thể xóa bỏ bớt các cột có high correlate với nhau vì: \n",
    "- các cột có ý nghĩa như nhau với model\n",
    "- có cơ hội học được từ các cột khác\n",
    "- **quan trọng trong việc train NN** vì giảm được khối lượng dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DLNYZQmereKS"
   },
   "outputs": [],
   "source": [
    "# drop columns with correlation greater than 0.9\n",
    "corr_matrix = df.drop(columns=\"isMale\").corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "print(\"high correlation features\", to_drop)\n",
    "df.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DuorNYw7KV4j"
   },
   "outputs": [],
   "source": [
    "#TESTING FOR MULTICOLLINEARITY:\n",
    "from patsy import dmatrices\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "#RUNNING FOR MULTIPLE REGRESSION:\n",
    "# %%capture\n",
    "#gather features\n",
    "vif_test = df_train\n",
    "vif_test = vif_test.drop('new_Active_Net.1', axis = 1)\n",
    "to_drop = ['new_Leader_ActNet_ratio_6m','new_Leader_RegionCode','new_Leader_APE_ratio_3m','new_NewRe_APE_ratio_3m','new_CaseNet_SumLast3m','new_Leader_APE_ratio_6m','new_NewRe_Manpower_SumLas3p','new_Leader_Case_ratio_3m','new_AG_Case_ratio_3m','new_NewRe_Case_Net_SumLas3p']\n",
    "vif_test = vif_test.drop(to_drop,axis =1)\n",
    "features = \"+\".join(vif_test.drop('label_1',axis=1).columns)\n",
    "# get y and X dataframes based on this regression:\n",
    "y, X = dmatrices('label_1 ~' + features, df_train, return_type='dataframe')\n",
    "vif = pd.DataFrame()\n",
    "range = np.arange(0,X.shape[1],1)\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range]\n",
    "vif[\"features\"] = X.columns\n",
    "\n",
    "col_vif = vif.drop(0).features\n",
    "df_train = df_train.drop(to_drop,axis =1)\n",
    "df_test = df_test.drop(to_drop,axis =1)\n",
    "vif.sort_values('VIF Factor',ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dFSouYDtKN9J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-8cO9PJ3ryk"
   },
   "source": [
    "### WOE & IV\n",
    "- https://www.one-tab.com/page/0V6bYIX4ShSQmuW4bpD7Zg\n",
    "\n",
    "```vi du```:\n",
    "- [woe - kalapa creditscr2nd](http://localhost:8888/lab/tree/DS/Project/Kalapa-CreditScor1st/readme.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "avH4UCL53u9Q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas.core.algorithms as algos\n",
    "from pandas import Series\n",
    "import scipy.stats.stats as stats\n",
    "import re\n",
    "import traceback\n",
    "import string\n",
    "\n",
    "max_bin = 20\n",
    "force_bin = 3\n",
    "\n",
    "# define a binning function\n",
    "def mono_bin(Y, X, n = max_bin):\n",
    "    \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]\n",
    "    r = 0\n",
    "    while np.abs(r) < 1:\n",
    "        try:\n",
    "            d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.qcut(notmiss.X, n)})\n",
    "            d2 = d1.groupby('Bucket', as_index=True)\n",
    "            r, p = stats.spearmanr(d2.mean().X, d2.mean().Y)\n",
    "            n = n - 1 \n",
    "        except Exception as e:\n",
    "            n = n - 1\n",
    "\n",
    "    if len(d2) == 1:\n",
    "        n = force_bin         \n",
    "        bins = algos.quantile(notmiss.X, np.linspace(0, 1, n))\n",
    "        if len(np.unique(bins)) == 2:\n",
    "            bins = np.insert(bins, 0, 1)\n",
    "            bins[1] = bins[1]-(bins[1]/2)\n",
    "        d1 = pd.DataFrame({\"X\": notmiss.X, \"Y\": notmiss.Y, \"Bucket\": pd.cut(notmiss.X, np.unique(bins),include_lowest=True)}) \n",
    "        d2 = d1.groupby('Bucket', as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"MIN_VALUE\"] = d2.min().X\n",
    "    d3[\"MAX_VALUE\"] = d2.max().X\n",
    "    d3[\"COUNT\"] = d2.count().Y\n",
    "    d3[\"EVENT\"] = d2.sum().Y\n",
    "    d3[\"NONEVENT\"] = d2.count().Y - d2.sum().Y\n",
    "    d3=d3.reset_index(drop=True)\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]       \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def char_bin(Y, X):\n",
    "        \n",
    "    df1 = pd.DataFrame({\"X\": X, \"Y\": Y})\n",
    "    justmiss = df1[['X','Y']][df1.X.isnull()]\n",
    "    notmiss = df1[['X','Y']][df1.X.notnull()]    \n",
    "    df2 = notmiss.groupby('X',as_index=True)\n",
    "    \n",
    "    d3 = pd.DataFrame({},index=[])\n",
    "    d3[\"COUNT\"] = df2.count().Y\n",
    "    d3[\"MIN_VALUE\"] = df2.sum().Y.index\n",
    "    d3[\"MAX_VALUE\"] = d3[\"MIN_VALUE\"]\n",
    "    d3[\"EVENT\"] = df2.sum().Y\n",
    "    d3[\"NONEVENT\"] = df2.count().Y - df2.sum().Y\n",
    "    \n",
    "    if len(justmiss.index) > 0:\n",
    "        d4 = pd.DataFrame({'MIN_VALUE':np.nan},index=[0])\n",
    "        d4[\"MAX_VALUE\"] = np.nan\n",
    "        d4[\"COUNT\"] = justmiss.count().Y\n",
    "        d4[\"EVENT\"] = justmiss.sum().Y\n",
    "        d4[\"NONEVENT\"] = justmiss.count().Y - justmiss.sum().Y\n",
    "        d3 = d3.append(d4,ignore_index=True)\n",
    "    \n",
    "    d3[\"EVENT_RATE\"] = d3.EVENT/d3.COUNT\n",
    "    d3[\"NON_EVENT_RATE\"] = d3.NONEVENT/d3.COUNT\n",
    "    d3[\"DIST_EVENT\"] = d3.EVENT/d3.sum().EVENT\n",
    "    d3[\"DIST_NON_EVENT\"] = d3.NONEVENT/d3.sum().NONEVENT\n",
    "    d3[\"WOE\"] = np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"IV\"] = (d3.DIST_EVENT-d3.DIST_NON_EVENT)*np.log(d3.DIST_EVENT/d3.DIST_NON_EVENT)\n",
    "    d3[\"VAR_NAME\"] = \"VAR\"\n",
    "    d3 = d3[['VAR_NAME','MIN_VALUE', 'MAX_VALUE', 'COUNT', 'EVENT', 'EVENT_RATE', 'NONEVENT', 'NON_EVENT_RATE', 'DIST_EVENT','DIST_NON_EVENT','WOE', 'IV']]      \n",
    "    d3 = d3.replace([np.inf, -np.inf], 0)\n",
    "    d3.IV = d3.IV.sum()\n",
    "    d3 = d3.reset_index(drop=True)\n",
    "    \n",
    "    return(d3)\n",
    "\n",
    "def data_vars(df1, target):\n",
    "    \n",
    "    stack = traceback.extract_stack()\n",
    "    filename, lineno, function_name, code = stack[-2]\n",
    "    vars_name = re.compile(r'\\((.*?)\\).*$').search(code).groups()[0]\n",
    "    final = (re.findall(r\"[\\w']+\", vars_name))[-1]\n",
    "    \n",
    "    x = df1.dtypes.index\n",
    "    count = -1\n",
    "    \n",
    "    for i in x:\n",
    "        if i.upper() not in (final.upper()):\n",
    "            if np.issubdtype(df1[i], np.number) and len(Series.unique(df1[i])) > 2:\n",
    "                conv = mono_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i\n",
    "                count = count + 1\n",
    "            else:\n",
    "                conv = char_bin(target, df1[i])\n",
    "                conv[\"VAR_NAME\"] = i            \n",
    "                count = count + 1\n",
    "                \n",
    "            if count == 0:\n",
    "                iv_df = conv\n",
    "            else:\n",
    "                iv_df = iv_df.append(conv,ignore_index=True)\n",
    "    \n",
    "    iv = pd.DataFrame({'IV':iv_df.groupby('VAR_NAME').IV.max()})\n",
    "    iv = iv.reset_index()\n",
    "    return(iv_df,iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fM9y6pscMafi"
   },
   "outputs": [],
   "source": [
    "## Kiem tra IV cua du lieu\n",
    "cols = [x for x in df_model.columns.tolist() if x not in id_col + ['label','ID']]\n",
    "final_iv, IV = data_vars(df_model[cols],df_model.label)\n",
    "IV.sort_values(by=['IV'])\n",
    "\n",
    "# Loc du lieu bang IV\n",
    "thress_hold = 0.01\n",
    "cols = IV[IV.IV > thress_hold].VAR_NAME.tolist()\n",
    "df_model = df_model[ id_col + ['label'] + [x for x in df_model.columns.tolist() if x in cols]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8bb__3HMaWD"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Chọn giá trị one hot bằng WOE\n",
    "'''\n",
    "cat_cols = df_model.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "def onehot_encode(df, onehot_col, prefix = True):\n",
    "    dummies_col = list()\n",
    "    for c in onehot_col:\n",
    "        print('> Get dummies with prefix {}'.format(c+'_'))\n",
    "        if prefix:\n",
    "            df_dummies = pd.get_dummies(df[c], prefix = c ) \n",
    "        else: \n",
    "            df_dummies = pd.get_dummies(df[c]) \n",
    "        dummies_col = dummies_col +(df_dummies.columns.to_list())\n",
    "        df = pd.concat( [df, df_dummies] , axis = 1)\n",
    "    #print('dummies col: ',dummies_col)\n",
    "    return df, dummies_col\n",
    "\n",
    "# điều chính IV_thresshold để chọn những cột sau khi one-hot có IV > IV_thresshold\n",
    "cols = cat_cols\n",
    "IV_thresshold = 0.01\n",
    "\n",
    "for col in cols:\n",
    "    tmp = df_model[[col] + ['label']]\n",
    "    tmp, dummies_col = onehot_encode(tmp, [col], prefix = False)\n",
    "\n",
    "    final_iv, IV = data_vars(tmp[dummies_col],df_model.label)\n",
    "    #IV.sort_values(by=['IV'])\n",
    "    highwoe_list = IV[IV.IV > IV_thresshold].VAR_NAME.tolist()\n",
    "    if len(highwoe_list) > 0 :\n",
    "        tmp[col] = np.where(tmp[col].isin(highwoe_list),tmp[col],'others')\n",
    "        print(\"     > Number of categorical in {}: {}\" .format(col,len(tmp[col].unique())))\n",
    "        df_model[col] = tmp[col]\n",
    "    else:\n",
    "        print(\"     > All sub-categories less than thresshold: \", col)\n",
    "        next\n",
    "    #df_model[col] = tmp[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8KQqwlOh02BZ"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Bien doi gia tri cua CAT feature thanh gia tri tu WOE\n",
    "'''\n",
    "\n",
    "WOE_cat_col = ['a', 'b', 'c']\n",
    "\n",
    "def woe_catfeat(df, WOE_cat_col, label):\n",
    "    final_iv, IV = data_vars(df[WOE_cat_col], df[label])\n",
    "    woe = pd.DataFrame()\n",
    "    new_woe_col = list()\n",
    "    \n",
    "    for col in WOE_cat_col:\n",
    "        l = []\n",
    "        t_final_iv = final_iv[final_iv.VAR_NAME == col]\n",
    "        print('> ',col)\n",
    "        print(' no of bins: ',t_final_iv.shape[0])\n",
    "        if t_final_iv.shape[0] <= 10:\n",
    "            print(t_final_iv[['VAR_NAME','MIN_VALUE']])\n",
    "        else:\n",
    "            print(t_final_iv.sample(5)[['VAR_NAME','MIN_VALUE']])\n",
    "        print('\\n')\n",
    "        for x in df[col]:\n",
    "            l.append(t_final_iv[t_final_iv.MIN_VALUE == x].WOE.values[0])\n",
    "        woe[col+'_woe'] = l\n",
    "        new_woe_col.append(col+'_woe')\n",
    "        \n",
    "    return woe, new_woe_col\n",
    "\n",
    "df, new_woe_cat = woe_catfeat(df, WOE_cat_col, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yQp7xf0C5Rp_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Bien doi gia tri cua NUMERIC feature thanh gia tri tu WOE\n",
    "'''\n",
    "\n",
    "WOE_numeric_col = ['a', 'b', 'c']\n",
    "\n",
    "def woe_numericfeat(df, WOE_numeric_col, label):\n",
    "    final_iv, IV = data_vars(df[WOE_numeric_col], df[label])\n",
    "    woe = pd.DataFrame()\n",
    "    new_woe_col = list()\n",
    "    \n",
    "    for col in WOE_numeric_col:\n",
    "        l = []\n",
    "        t_final_iv = final_iv[final_iv.VAR_NAME == col]\n",
    "        print('> ',col)\n",
    "        print(' no of bins: ',t_final_iv.shape[0])\n",
    "        if t_final_iv.shape[0] <= 10:\n",
    "            print(t_final_iv[['VAR_NAME','MIN_VALUE','MAX_VALUE']])\n",
    "        else:\n",
    "            print(t_final_iv.sample(5)[['VAR_NAME','MIN_VALUE','MAX_VALUE']])\n",
    "        print('\\n')\n",
    "        for x in df[col]:\n",
    "            l.append(t_final_iv[(t_final_iv.MIN_VALUE <= x)& (t_final_iv.MAX_VALUE >= x)].WOE.values[0])\n",
    "        woe[col+'_woe'] = l\n",
    "        new_woe_col.append(col+'_woe')\n",
    "        \n",
    "    return woe, new_woe_col\n",
    "\n",
    "df, new_woe_cat = woe_catfeat(df, WOE_cat_col, 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oGlijJm5RxT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cmnU0NRff_ck"
   },
   "source": [
    "### **RFE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I7hYFfmv7cbk"
   },
   "source": [
    "\n",
    "[theory](https://www.kaggle.com/nroman/recursive-feature-elimination)\n",
    "\n",
    "Trong mini course này, tôi sẽ áp dụng hướng “backwarding”. Các bước thực hiện như sau:\n",
    "\n",
    "- Đặt: n là số lần lặp feature selection, k là số feature sẽ drop ở mỗi lần lặp, p là AUC sau mỗi lần train\n",
    "Train model với XGboost\n",
    "- Lấy kết quả feature important sắp xếp giảm dần và loại ra k feature có giá trị thấp nhất\n",
    "- Lưu lại performance hiện tại để so sánh với performance tiếp theo.\n",
    "- Nếu thấp hơn ngưỡng p sẽ dừng\n",
    "- Tiếp tục quá trình selection\n",
    "\n",
    "Tuỳ theo số lượng feature và cài đặt hyper-parameter của model thì thời gian sẽ nhanh chậm khác nhau.\n",
    "\n",
    "Additional:\n",
    "- Genetic algorithm for feature selection\n",
    "\n",
    "Source:\n",
    "- [1](https://ongxuanhong.wordpress.com/2019/04/17/data-science-mini-course/#more-15645)\n",
    "- [2](https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15) doc them\n",
    "- https://www.kaggle.com/arthurtok/feature-ranking-rfe-random-forest-linear-models\n",
    "- https://www.kaggle.com/sz8416/6-ways-for-feature-selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UIowwhg2A3s"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "#lr_best = LogisticRegressionCV\n",
    "#lr_best = LogisticRegression(C=7.7, penalty='l2' ,class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2roeUhZt6FzV"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv = RFECV(estimator=lr_best, step=1, cv=10, scoring='roc_auc')\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "best_features = X_train.columns[rfecv.support_].tolist()\n",
    "\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "print('Selected features: %s' % best_features)\n",
    "\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(np.arange(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqatZvAQcNBw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUr63QqacNFN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfMtLkkO7fxr",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Data Type Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5mzGc0BE7xWq"
   },
   "source": [
    "### Under-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeblIals7lTa"
   },
   "outputs": [],
   "source": [
    "# under-sampling 7:3\n",
    "sample = train[train.label_2==1].shape[0]/3*7\n",
    "train_US73=pd.concat([train[train.label_2!=1].sample(int(sample)),train[train.label_2==1]], axis = 0, ignore_index=True)\n",
    "print('sample shape',train_US73.shape)\n",
    "\n",
    "# under-sampling 5:5\n",
    "# sample = train[train.label_2==1].shape[0]/5*5\n",
    "# train_US55=pd.concat([train[train.label_2!=1].sample(int(sample)),train[train.label_2==1]], axis = 0, ignore_index=True\n",
    "# print('sample shape',train_US73.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHICQmTi7nqn"
   },
   "outputs": [],
   "source": [
    "#X = train[dummies_col+numeric_col]\n",
    "X_train = train_US73[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
    "y_train = train_US73['label_2']\n",
    "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
    "y_test = test['label_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4il7y5CI8O8D",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Over-sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ioqMRx98Syz"
   },
   "source": [
    "#### RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49htNR8a7n1M"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.base import BaseSampler\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WdEAQb4H8XfJ"
   },
   "outputs": [],
   "source": [
    "#X = train[dummies_col+numeric_col]\n",
    "X_train = train[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
    "y_train = train['label_2']\n",
    "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
    "y_test = test['label_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EI2Up4po8Xit"
   },
   "outputs": [],
   "source": [
    "model_smote = LogisticRegression()\n",
    "\n",
    "pipe = make_pipeline(RandomOverSampler(sampling_strategy=1, random_state=0), model_smote)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwL1Z1yO8Xlo"
   },
   "outputs": [],
   "source": [
    "model_smote_predictions = pipe.predict_proba(X_test)\n",
    "model_smote_pred_label = pipe.predict(X_test) \n",
    "model_smote_roc_score = roc_auc_score( y_test, model_smote_predictions[:,1])\n",
    "model_smote_f1_score = f1_score(y_test, model_smote_pred_label)\n",
    "print('random forest roc score on test: ', model_smote_roc_score)\n",
    "print('random forest f1 score on test: ', model_smote_f1_score)\n",
    "\n",
    "confu_matrix = confusion_matrix(y_test, model_smote_pred_label) \n",
    "sns.heatmap(confu_matrix , annot=True, fmt='d')\n",
    "print(classification_report(y_test, model_smote_pred_label) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8A8WLw5B8dnc"
   },
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSansvBy8fIq"
   },
   "outputs": [],
   "source": [
    "#X = train[dummies_col+numeric_col]\n",
    "X_train = train[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
    "y_train = train['label_2']\n",
    "X_test = test[[x for x in train.columns if x not in ['label_2','First_month']]]\n",
    "y_test = test['label_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O1aaJroW8fQx"
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.base import BaseSampler\n",
    "\n",
    "rf_clf = LogisticRegression()\n",
    "\n",
    "smotes = {0 : 'SMOTE',\n",
    "          1 : 'BorderlineSMOTE',\n",
    "          2 : 'SVMSMOTE',\n",
    "          3 : 'ADASYN'}\n",
    "\n",
    "\n",
    "for i, sampler in enumerate((SMOTE(sampling_strategy = 1, random_state=0),\n",
    "                BorderlineSMOTE(sampling_strategy = 1, random_state=0, kind='borderline-1'),\n",
    "                SVMSMOTE(sampling_strategy = 1, random_state=0),\n",
    "                ADASYN(sampling_strategy = 1, random_state=0))):\n",
    "    pipe_line = make_pipeline(sampler, rf_clf)\n",
    "    pipe_line.fit(X_train, y_train)\n",
    "    rf_predictions = pipe_line.predict_proba(X_test)\n",
    "    rf_pred_label = pipe_line.predict(X_test) \n",
    "    rf_roc_score = roc_auc_score(y_test, rf_predictions[:,1])\n",
    "    rf_f1_score = f1_score(y_test, rf_pred_label)\n",
    "    print('------------------------------------------------')\n",
    "    print('SMOTE method: ', smotes[i])\n",
    "    print('random forest roc score on test: ', rf_roc_score)\n",
    "    print('random forest f1 score on test: ', rf_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzF3WgLN8fTt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cJ7_EPQZIm0I"
   },
   "source": [
    "# Model Selection & Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAHmcQqOn4yl"
   },
   "source": [
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chiến thuật 1 - giảm variance trên k-fold\n",
    "```Ví dụ```\n",
    "- [kalapa scoring 1st - 2nd solution modelling](http://localhost:8888/lab/tree/DS/Project/Kalapa-CreditScor1st/2nd%20solution.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Tạo Baseline model```\n",
    "\n",
    "Mình xin nói phần này trước phần Feature Engineering. Vì nó ảnh hưởng đến những việc mình làm ở các bước tiếp theo.\n",
    "sau khi clean data, mình không làm feature gì thêm cả, đưa hết data vào model LGBM, chạy 5-folds sau đó submit. Sau đây là một số kết luận của mình từ baseline model.\n",
    "- Không có correlation giữa validation và public LB.\n",
    "- Validation rất quan trọng. Bởi vì nó ảnh hưởng đến việc bạn chọn hyper-parameters/feature engineering. Chỉ khi nào bạn setup đc một solid validation scheme, thì mới đến bước tiếp theo (Feature engineering, tunning, etc).\n",
    "- Variance giữa các fold cực cao\n",
    "- Mình thay đổi seed và kêt quả public LB thay đổi theo (nhưng k có variance). Thống kê trong 20 lần chạy với các seed khác nhau, variance giữa các seed cao.\n",
    "\n",
    "**Tại sao lại variance cao?** - Theo quan điểm của mình, có thể:\n",
    "- Do data. Nope, bạn k thể đổ lỗi cho data được. Đây là cái bạn phải đương đầu trong thực tiễn.\n",
    "- Do model overfit.\n",
    "\n",
    "**Variance cao thể hiện điều gì?**\n",
    "Variance thể hiện tính ổn định của model. Variance cao tức là model không ổn định. Model không ổn định sẽ dẫn tới shakeup. Nó có thể tốt trên tập public test set, nhưng chưa chắc đã tốt trên tập test khác.\n",
    "- Variance có quan trọng không?. Rất quan trọng.\n",
    "- Tại sao?. Trong mô hình thực tế, data có thể thay đổi liên tục theo từng thời điểm. Tháng này Kalapa có 5triệu users, tháng sau có 15 triệu users. Distribution có thể bị skew liên tục. Công ty không cần bạn performance tốt trên 5tr user họ đang có (vì chính xác họ đã gán nhãn rồi), họ muốn kết quả tốt trên 10tr users sắp tới. Việc search seed minh nghĩ không phải một ý tưởng tốt trong bài này. Không thể để kết quả phụ thuộc quá nhiều vào con số hên xui. Vậy nên việc giữ cho variance càng thấp càng tốt sẽ giúp bạn tránh rủi ro hơn trọng kết quả cuối cùng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Feature engineering```\n",
    "\n",
    "Không có gì đặc biệt trong phần này cả. Mình chỉ thêm một feature mới là mean target encoding.\n",
    "\n",
    "```Feature selections```\n",
    "Đây là phần quan trọng để giảm variance cũng như overfit.\n",
    "Xuất phát ban đầu từ baseline, mình có 96 features. Sau mỗi lần chạy feature, mình tính feature importants và loại bỏ đi những feature yếu. Mỗi lần như vậy mình tỉa đi khoảng 5-10 features. Sau đó chạy lại lần nữa, và kết quả thấy variance giảm. Vòng loop được lặp lại cho đến khi variance không giảm được nữa thì thôi.\n",
    "Cuối cùng mình có 31 features tất cả.\n",
    "\n",
    "```Final Modeling```\n",
    "light-gbm\n",
    "\n",
    "Model của mình submit gồm 31 features trên. Mình chạy trong 20 seeds và lấy trung bình giữa các seed.\n",
    "```\n",
    "Gini for OOF: 0.2285142769658592\n",
    "Average CV : 0.2343615754995229\n",
    "STD CV : 0.01116168868867106\n",
    "Gini for OOF: 0.23307624671948943\n",
    "```\n",
    "\n",
    "Cuối cùng mình giảm variance xuống được 0.01. Lúc này CV và LB có sự correlation và GAP không lớn. Có thể nói model cua mình ổn định và không overfit. public LB: 0.237, private LB: 0.257\n",
    "Conclusion.\n",
    "\n",
    "Mặc solution của đội mình không phải tốt nhất, nhưng mình tin nó sẽ giúp cho organizers build được model stable , ổn định nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxxetmcqn4Ug"
   },
   "source": [
    "### Validation bằng cách search probabily ratio trên cut-off range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q46qxVdk8fWQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,roc_auc_score,roc_curve,auc,recall_score,f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def search_cutoff(model,x_test,y_test,search_range):\n",
    "    #Logistics Regression:\n",
    "    pred_prob= model.predict_proba(x_test)\n",
    "#     search_range = np.arange(0,1,0.025)\n",
    "    result = pd.DataFrame(columns = ['Thress_hold','accuracy','precision','recall','roc_auc','f1_score_poslabel','f1_score_neglabel'\n",
    "                        ,'TP','TN','FN','FP'\n",
    "                        ,'case','reduce','per_reduce','wrong','per_correct','send','per_send_correct'])\n",
    "\n",
    "    for k in search_range:\n",
    "        pred_var = np.where(pred_prob[:,1]>=k,1,0)#[1 if i > k else 0 for i in pred_prob[:,1]]\n",
    "        confu_matrix = confusion_matrix(pred_var,y_test, labels=[1,0])\n",
    "        result = pd.concat([result,pd.DataFrame([[\n",
    "            k, accuracy_score(pred_var,y_test)\n",
    "            ,precision_score(pred_var,y_test),recall_score(pred_var,y_test)\n",
    "            ,accuracy_score(pred_var,y_test),f1_score(pred_var,y_test)\n",
    "            ,f1_score(pred_var,y_test, pos_label=0)\n",
    "            ,confu_matrix[0,0],confu_matrix[0,1],confu_matrix[1,0],confu_matrix[1,1]  \n",
    "            ,len(pred_var),len(pred_var)-(confu_matrix[0,0]+confu_matrix[0,1]),1-round((confu_matrix[0,0]+confu_matrix[0,1])/len(pred_var),3)\n",
    "            ,confu_matrix[1,0],round(confu_matrix[1,1]/(confu_matrix[1,0]+confu_matrix[1,1]),3)\n",
    "            ,(confu_matrix[0,0]+confu_matrix[0,1]),round(confu_matrix[0,0]/(confu_matrix[0,0]+confu_matrix[0,1]),3)\n",
    "        ]]\n",
    "            ,columns = ['Thress_hold','accuracy','precision','recall','roc_auc','f1_score_poslabel','f1_score_neglabel'\n",
    "                         ,'TP','TN','FN','FP'\n",
    "                        ,'case','reduce','per_reduce','wrong','per_correct','send','per_send_correct'\n",
    "                       ])])\n",
    "\n",
    "    return result\n",
    "\n",
    "'''\n",
    "SHOW SOME IMPORTANT METRICS\n",
    "'''\n",
    "pred_prob = rf.predict_proba(x_test)\n",
    "# pred_var = np.where(pred_prob[:,1]>=0.3,1,0)\n",
    "pred_var = rf.predict(x_test)\n",
    "print('Real label counts \\n',y_test.value_counts())\n",
    "print('  > label percent = ',1 - round(y_test.value_counts()[1]/y_test.size,3))\n",
    "print('\\n')\n",
    "\n",
    "#precision_score(pred_var,y_test),recall_score(pred_var,y_test),\n",
    "print('> Accuracy: ', accuracy_score(pred_var,y_test))\n",
    "print('> Recall: ', recall_score(pred_var,y_test))\n",
    "print('> Precision: ', precision_score(pred_var,y_test))\n",
    "print('> F1 score: {}, F1 score pos: {} '.format(f1_score(pred_var,y_test),f1_score(pred_var,y_test, pos_label=0)))\n",
    "\n",
    "print('> Consufion matric: \\n',pd.DataFrame(confusion_matrix(pred_var,y_test, labels=[1,0]), index=['pred:1', 'pred:0'], columns=['true:1', 'true:0']))\n",
    "print('> classification report: \\n',classification_report(pred_var,y_test) )\n",
    "\n",
    "search_range = np.arange(0,1,0.05)\n",
    "search_cutoff(rf,x_test,y_test,search_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adversarial Validation\n",
    "Adversarial Validation là một phương pháp giúp kiểm tra tính giống nhau (similarity) của distribution, tính chất (property) giữa tập train và tập test.\n",
    "\n",
    "Việc thực hiện adversarial validation sẽ giúp ta biết nên trust Local cross-validation hay Leaderboard và có kế hoạch cụ thể cho việc validate model, tránh trường hợp bị overfit hay shakeup\n",
    "\n",
    "Các bước thực hiện gồm:\n",
    "- Drop các cột thông tin không cần thiết ở tập train và test (Ex: id). Cột target/label của tập train cũng được loại bỏ.\n",
    "- Gán một label mới cho toàn bộ tập train là '1', label cho tập test là '0'.\n",
    "- Dùng một model thông thường, không cần quá cồng kềnh (Randomforest, SVM, Xgboost, LightGBM,...) để phân loại đâu là sample của tập train (label=1) và đâu là data của tập test (label=0). Ở đây là bài toán binary classification nên hay dùng AUC để là metrics\n",
    "\n",
    "Dựa vào giá trị AUC đo được ở trên, các trường hợp có thể xảy ra:\n",
    "- AUC càng thấp (~0.5): Nghĩa là việc phân biệt sample của train và test là khó đối với model đó => Train và test giống nhau => Lúc này tỉ lệ shake-up sẽ thấp\n",
    "- AUC càng cao (0.9): Nghĩa là việc phân biệt sample của train và test là dễ đối với model đó => Train và test khác nhau => Tỉ lệ shake-up cao. Bạn cần chọn cách chia validation sao cho nó giống với test nhất. Cách đơn giản nhất đó là chạy cross-validation với model thông thường trên. Với mỗi fold xác định xem sample nào bị miss-classified và đưa nó vào tập validation cho bài toán gốc\n",
    "\n",
    "source:\n",
    "- [code](https://www.kaggle.com/sonannguyenngoc/kalapa-adversarial-validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlR2VrJocLM1",
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Genetic algorithm\n",
    "- [Explain 1](https://www.neuraldesigner.com/blog/genetic_algorithms_for_feature_selection)\n",
    "- [Explain 2](https://towardsdatascience.com/feature-reduction-using-genetic-algorithm-with-python-403a5f4ef0c1)\n",
    "- [Explain 3](http://dkopczyk.quantee.co.uk/genetic-algorithm/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G0vuN-k5cNip"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# load train and valid\n",
    "data_folder = 'competitionData'\n",
    "train = pd.read_csv(os.path.join(data_folder, 'trainingDataDeflated.csv'))\n",
    "valid = pd.read_csv(os.path.join(data_folder, 'validationDataDeflated.csv'))\n",
    "\n",
    "# get training columns\n",
    "training_columns = list(train.columns.values)\n",
    "for column in ['ind', 'deck', 'nofGames', 'nOfPlayers', 'winRate']:\n",
    "    training_columns.remove(column)\n",
    "print(\"Total number of training columns: {}\".format(len(training_columns)))\n",
    "\n",
    "# set index for loc\n",
    "train.set_index('ind', inplace=True)\n",
    "valid.set_index('ind', inplace=True)\n",
    "\n",
    "def load_dna_from_file(filename):\n",
    "    dna = []\n",
    "    f = open(filename)\n",
    "    while True:\n",
    "        line = f.readline().strip()\n",
    "        if line == '':\n",
    "            break\n",
    "        # parse params\n",
    "        params = line.split(\";\")\n",
    "        indices = [int(item) for item in params[3].split(',') if item != '']\n",
    "        # add information\n",
    "        gene = dict()\n",
    "        gene[\"epsilon\"] = float(params[0])\n",
    "        gene[\"c\"] = float(params[1])\n",
    "        gene[\"gamma\"] = round(float(params[2]), 5)\n",
    "        gene[\"indices\"] = indices\n",
    "        dna.append(gene)\n",
    "    f.close()\n",
    "    return dna\n",
    "\n",
    "'''\n",
    "save dna to file\n",
    "'''\n",
    "def save_dna_to_file(item):\n",
    "    dna = item[0]\n",
    "    fitness_score = item[1]\n",
    "    filename = os.path.join(\"finalPopulation\", \"{}.txt\".format(fitness_score))\n",
    "\n",
    "    fout = open(filename, \"w\")\n",
    "    for gene in dna:\n",
    "        ind_text = ','.join(str(index) for index in gene[\"indices\"])\n",
    "        text = ';'.join([str(gene[\"epsilon\"]), str(gene[\"c\"]), str(gene[\"gamma\"]), ind_text])\n",
    "        fout.write(text + '\\n')\n",
    "\n",
    "    fout.close()\n",
    "\n",
    "'''\n",
    "initialize the first population, either randomly or from a file\n",
    "'''\n",
    "def init_population(population_size, from_files=False):\n",
    "    pop = []\n",
    "\n",
    "    # add good dna to the init population from previously tested good solutions\n",
    "    if from_files == True:\n",
    "        files = os.listdir(\"goodSolutions\")\n",
    "        for file in files:\n",
    "            fitness_score = float(file[:-4])\n",
    "            filename = os.path.join(\"goodSolutions\", file)\n",
    "            dna = load_dna_from_file(filename)\n",
    "            pop.append([dna, fitness_score])\n",
    "\n",
    "    num_of_random_dna = population_size - len(pop)\n",
    "    for i in range(num_of_random_dna):\n",
    "        dna = []\n",
    "        for num_of_data in range(600, 1501, 100):\n",
    "            gene = dict()\n",
    "            gene[\"c\"] = 5.0\n",
    "            gene[\"gamma\"] = 1.0/90\n",
    "            gene[\"epsilon\"] = 0.02\n",
    "            gene[\"indices\"] = sorted(random.sample(range(1, 100001), num_of_data))\n",
    "            dna.append(gene)\n",
    "        pop.append([dna, 0.0])\n",
    "\n",
    "    min_score = min([item[1] for item in pop])\n",
    "    return pop, min_score\n",
    "\n",
    "'''\n",
    "R2 predict evaluation\n",
    "'''\n",
    "def R2(x, y):\n",
    "    return 1 - np.sum(np.square(x - y)) / np.sum(np.square(y - np.mean(y)))\n",
    "\n",
    "'''\n",
    "evaluate scores of a dna, sequential processing\n",
    "'''\n",
    "def fitness_sequential(dna, weight_train = 0.5):\n",
    "\n",
    "    gene_valid_r2 = []\n",
    "    gene_train_r2 = []\n",
    "\n",
    "    for gene in dna:\n",
    "        c = gene[\"c\"]\n",
    "        gamma = gene[\"gamma\"]\n",
    "        epsilon = gene[\"epsilon\"]\n",
    "        indices = gene[\"indices\"]\n",
    "\n",
    "        svr = SVR(kernel='rbf', gamma=gamma, C=c, epsilon=epsilon, shrinking=False)\n",
    "        svr.fit(train.loc[indices][training_columns], train.loc[indices][\"winRate\"])\n",
    "        valid_pred = svr.predict(valid[training_columns])\n",
    "        valid_r2 = R2(valid_pred, valid[\"winRate\"])\n",
    "        gene_valid_r2.append(valid_r2)\n",
    "\n",
    "        unselected_indices = list(set(range(1, 100001)) - set(indices))\n",
    "        train_pred = svr.predict(train.loc[unselected_indices][training_columns])\n",
    "        train_r2 = R2(train_pred, train.loc[unselected_indices][\"winRate\"])\n",
    "        gene_train_r2.append(train_r2)\n",
    "\n",
    "    fitness_valid = np.mean(gene_valid_r2)\n",
    "    fitness_train = np.mean(gene_train_r2)\n",
    "    fitness_score = (fitness_valid + weight_train * fitness_train) / (weight_train + 1.0)\n",
    "    print(\"{},{},{}:{}\".format(fitness_score, fitness_valid, fitness_train, gene_valid_r2 + gene_train_r2))\n",
    "    return fitness_score\n",
    "\n",
    "def train_and_valid(gene, using_train = False):\n",
    "    c = gene[\"c\"]\n",
    "    gamma = gene[\"gamma\"]\n",
    "    epsilon = gene[\"epsilon\"]\n",
    "    indices = gene[\"indices\"]\n",
    "\n",
    "    svr = SVR(kernel='rbf', gamma=gamma, C=c, epsilon=epsilon, shrinking=False)\n",
    "    svr.fit(train.loc[indices][training_columns], train.loc[indices][\"winRate\"])\n",
    "    valid_pred = svr.predict(valid[training_columns])\n",
    "    valid_r2 = R2(valid_pred, valid['winRate'])\n",
    "\n",
    "    if using_train:\n",
    "        unselected_indices = list(set(range(1, 100001)) - set(indices))\n",
    "        train_pred = svr.predict(train.loc[unselected_indices][training_columns])\n",
    "        train_r2 = R2(train_pred, train.loc[unselected_indices][\"winRate\"])\n",
    "        return [valid_r2, train_r2]\n",
    "\n",
    "    else:\n",
    "        return valid_r2\n",
    "\n",
    "'''\n",
    "evaluate scores of a dna, parallel processing\n",
    "'''\n",
    "def fitness_parallel(dna, using_train = False, weight_train = 1.0 / 16, valid_threshold = 21.0):\n",
    "    gene_r2 = Parallel(n_jobs=5, verbose=1)(delayed(train_and_valid)(gene, using_train) for gene in dna)\n",
    "    if using_train:\n",
    "        gene_valid_r2 = [item[0] for item in gene_r2]\n",
    "        gene_train_r2 = [item[1] for item in gene_r2]\n",
    "        fitness_valid = np.mean(gene_valid_r2)\n",
    "        fitness_train = np.mean(gene_train_r2)\n",
    "        fitness_score = (fitness_valid + fitness_train * weight_train) / (weight_train + 1.0)\n",
    "        print(\"{},{},{}:{}\".format(fitness_score, fitness_valid, fitness_train, gene_valid_r2 + gene_train_r2))\n",
    "    else:\n",
    "        fitness_score = np.mean(gene_r2)\n",
    "        print(\"{}:{}\".format(fitness_score, gene_r2))\n",
    "    return fitness_score\n",
    "\n",
    "'''\n",
    "randomly replace a number of indices\n",
    "no action is taken on epsilon, gamma, or c\n",
    "'''\n",
    "def mutate_gene(gene, percent_indices = 10):\n",
    "    num_of_indices = len(gene[\"indices\"])\n",
    "    selected_mutation_part = random.randint(0, num_of_indices * 4)\n",
    "    if selected_mutation_part < num_of_indices:\n",
    "        # c varies from 0.00001 to 10.0\n",
    "        gene[\"c\"] = random.randint(1, 1000000) / 100000.0\n",
    "    elif selected_mutation_part < num_of_indices * 2:\n",
    "        # gamma varies from 0.01 to 10.0\n",
    "        gene[\"gamma\"] = round(random.randint(1, 9000000) / 900000.0, 5)\n",
    "    elif selected_mutation_part <= num_of_indices * 3:\n",
    "        # epsilon varies from 0.00001 to 1.0\n",
    "        gene[\"epsilon\"] = random.randint(1, 100000) / 100000.0\n",
    "    else:\n",
    "        indices = gene[\"indices\"]\n",
    "        num_of_removed_indices = int(percent_indices * num_of_indices / 100)\n",
    "        removed_indices = random.sample(indices, num_of_removed_indices)\n",
    "\n",
    "        remaining_indices = set(indices) - set(removed_indices)\n",
    "        pool_indices = list(set(range(1, 100001)) - remaining_indices)\n",
    "        new_indices = random.sample(pool_indices, num_of_removed_indices)\n",
    "\n",
    "        gene[\"indices\"] = list(remaining_indices) + new_indices\n",
    "        assert len(gene[\"indices\"]) == num_of_indices, \"Missing indices in mutation\"\n",
    "\n",
    "'''\n",
    "randomly select a gene for mutation\n",
    "'''\n",
    "def mutate_dna(dna, single_gene=True):\n",
    "    if single_gene:\n",
    "        selected_gene = random.randint(0, 9)\n",
    "        mutate_gene(dna[selected_gene])\n",
    "    else:\n",
    "        for selected_gene in range(10):\n",
    "            mutate_gene(dna[selected_gene])\n",
    "    return dna\n",
    "\n",
    "'''\n",
    "perform crossover between 2 genes\n",
    "'''\n",
    "def crossover_gene(gene_x, gene_y):\n",
    "    new_gene = dict()\n",
    "    new_gene[\"c\"] = random.sample([gene_x[\"c\"], gene_y[\"c\"]], 1)[0]\n",
    "    new_gene[\"epsilon\"] = random.sample([gene_x[\"epsilon\"], gene_y[\"epsilon\"]], 1)[0]\n",
    "    new_gene[\"gamma\"] = random.sample([gene_x[\"gamma\"], gene_y[\"gamma\"]], 1)[0]\n",
    "\n",
    "    x_indices = set(gene_x[\"indices\"])\n",
    "    y_indices = set(gene_y[\"indices\"])\n",
    "    overlapping_indices = x_indices.intersection(y_indices)\n",
    "\n",
    "    num_of_indices = len(x_indices)\n",
    "    num_of_remaining_x_indices = int((num_of_indices - len(overlapping_indices)) / 2)\n",
    "    num_of_remaining_y_indices = num_of_indices - len(overlapping_indices) - num_of_remaining_x_indices\n",
    "\n",
    "    remaining_x_indices = list(x_indices - overlapping_indices)\n",
    "    remaining_y_indices = list(y_indices - overlapping_indices)\n",
    "\n",
    "    selected_x_indices = random.sample(remaining_x_indices, num_of_remaining_x_indices)\n",
    "    selected_y_indices = random.sample(remaining_y_indices, num_of_remaining_y_indices)\n",
    "\n",
    "    new_gene[\"indices\"] = list(overlapping_indices) + selected_x_indices + selected_y_indices\n",
    "    assert len(new_gene[\"indices\"]) == num_of_indices, \"Missing indices in crossover\"\n",
    "\n",
    "    return new_gene\n",
    "\n",
    "'''\n",
    "randomly select a gene and perform crossover on that gene\n",
    "'''\n",
    "def crossover_dna(dna_x, dna_y):\n",
    "\n",
    "    # perform crossover on all genes\n",
    "    new_dna = []\n",
    "    for i in range(10):\n",
    "        gene_x = dna_x[i]\n",
    "        gene_y = dna_y[i]\n",
    "        new_dna.append(crossover_gene(gene_x, gene_y))\n",
    "\n",
    "    return new_dna\n",
    "\n",
    "'''\n",
    "select dna for new generation\n",
    "'''\n",
    "def select(population, population_size):\n",
    "    if len(population) < population_size:\n",
    "        return population\n",
    "\n",
    "    prob_distribution = [item[1] for item in population]\n",
    "    selected_indices = np.random.choice(range(len(population)), population_size, prob_distribution)\n",
    "    removed_indices = [index for index in range(len(population)) if index not in selected_indices]\n",
    "\n",
    "    # set new population based on selected indices\n",
    "    new_population = []\n",
    "    for index in selected_indices:\n",
    "        new_population.append(population[index])\n",
    "\n",
    "    # remove unselected_indices\n",
    "    for index in sorted(removed_indices, reverse=True):\n",
    "        del population[index]\n",
    "    del population\n",
    "\n",
    "    return new_population\n",
    "\n",
    "'''\n",
    "clone a gene\n",
    "'''\n",
    "def copy_gene(gene):\n",
    "    new_gene = dict()\n",
    "    new_gene[\"c\"] = gene[\"c\"]\n",
    "    new_gene[\"gamma\"] = gene[\"gamma\"]\n",
    "    new_gene[\"epsilon\"] = gene[\"epsilon\"]\n",
    "    new_gene[\"indices\"] = list(gene[\"indices\"])\n",
    "    return new_gene\n",
    "\n",
    "'''\n",
    "Clone a dna\n",
    "'''\n",
    "def copy_dna(dna):\n",
    "    new_dna = []\n",
    "    for gene in dna:\n",
    "        new_gene = copy_gene(gene)\n",
    "        new_dna.append(new_gene)\n",
    "    return new_dna\n",
    "\n",
    "'''\n",
    "Select and update files for initial generation\n",
    "'''\n",
    "def update_files(population_size):\n",
    "    new_solutions = os.listdir(\"finalPopulation\")\n",
    "    if len(new_solutions) == 0:\n",
    "        return\n",
    "\n",
    "    for file in new_solutions:\n",
    "        if not os.path.exists(os.path.join(\"goodSolutions\", file)):\n",
    "            shutil.copy2(os.path.join(\"finalPopulation\", file), \"goodSolutions\")\n",
    "        os.remove(os.path.join(\"finalPopulation\", file))\n",
    "\n",
    "    all_solutions = os.listdir(\"goodSolutions\")\n",
    "    all_scores = [float(item[:-4]) for item in all_solutions]\n",
    "    top_scores = sorted(all_scores, reverse=True)[:population_size]\n",
    "\n",
    "    for item in all_solutions:\n",
    "        if float(item[:-4]) not in top_scores:\n",
    "            os.remove(os.path.join(\"goodSolutions\", item))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    counter = 1\n",
    "    while True:\n",
    "        POPULATION_MAX = 21\n",
    "        GENERATION_SIZE = 5\n",
    "\n",
    "        # init population\n",
    "        # tao ra population ban dau\n",
    "        population, MIN_LOG_SCORE = init_population(POPULATION_MAX, from_files=True)\n",
    "        print(\"Running times {}, min log score {}\".format(counter, MIN_LOG_SCORE))\n",
    "\n",
    "        # evolve\n",
    "        for i in range(GENERATION_SIZE):\n",
    "            new_solutions = []\n",
    "            population_size = len(population)\n",
    "            num_of_selected_parents = 15 # 3 * (population_size // 6)\n",
    "            print(\"Generation {}, input size {}, num of selected parents {}\".format(i + 1, population_size, num_of_selected_parents))\n",
    "\n",
    "            # generate dna for mutation and crossover\n",
    "            selected_indices = random.sample(range(population_size), num_of_selected_parents)\n",
    "\n",
    "            for j in range(0, num_of_selected_parents, 3):\n",
    "                # generate new child from mutation\n",
    "                mutated_child = mutate_dna(copy_dna(population[selected_indices[j]][0]))\n",
    "                new_solutions.append(mutated_child)\n",
    "\n",
    "                mutated_child = mutate_dna(copy_dna(population[selected_indices[j + 1]][0]))\n",
    "                new_solutions.append(mutated_child)\n",
    "\n",
    "                mutated_child = mutate_dna(copy_dna(population[selected_indices[j + 2]][0]))\n",
    "                new_solutions.append(mutated_child)\n",
    "\n",
    "                # generate new children from crossover\n",
    "                crossover_child = crossover_dna(population[selected_indices[j+1]][0], population[selected_indices[j+2]][0])\n",
    "                new_solutions.append(crossover_child)\n",
    "\n",
    "            # evaluate new children\n",
    "            for dna in new_solutions:\n",
    "                #fitness_score = fitness_sequential(dna)\n",
    "                fitness_score = fitness_parallel(dna, using_train=True)\n",
    "                if fitness_score > MIN_LOG_SCORE:\n",
    "                    save_dna_to_file([dna, fitness_score])\n",
    "                population.append([dna, fitness_score])\n",
    "\n",
    "            # randomly select individuals from new population for the next generation\n",
    "            population = select(population, POPULATION_MAX)\n",
    "\n",
    "        # output final population\n",
    "        '''\n",
    "        for item in population:\n",
    "            save_dna_to_file(item)\n",
    "        '''\n",
    "\n",
    "        # update good solutions for a new round\n",
    "        update_files(POPULATION_MAX)\n",
    "        counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "1.PreProcessing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
